{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQFvGnrbogmq",
        "outputId": "04b53327-a9e0-4007-eb17-a20879ebef5e"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# zip_path = '/content/drive/MyDrive/apredizajeautomatico/QuickDraw-Animals.zip'  # Cambia esta ruta\n",
        "# zip_path_2 = '/content/drive/MyDrive/apredizajeautomatico/QuickDraw-10-Tarea2.zip'\n",
        "\n",
        "# extract_path = '/content/imagenes_descomprimidas'\n",
        "# os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# with zipfile.ZipFile(zip_path_2, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(f\"Archivos extraídos en: {extract_path}\")\n",
        "\n",
        "# print(\"Archivos extraídos:\")\n",
        "# print(os.listdir(extract_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_bnYsh3WtCDk"
      },
      "outputs": [],
      "source": [
        "# importacion de librerias\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ESQWEpOtIsJ"
      },
      "source": [
        "# Parte 1 - Lectura de Imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HycllJm_tKiG"
      },
      "outputs": [],
      "source": [
        "def load_image_paths_and_labels(file_path):\n",
        "    \"\"\"Carga las rutas de las imágenes y las etiquetas desde un archivo de texto.\"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file.readlines():\n",
        "            path, label = line.strip().split('\\t')\n",
        "            image_paths.append(path)\n",
        "            labels.append(int(label))\n",
        "    return image_paths, np.array(labels)\n",
        "\n",
        "def load_images(image_paths, folder_route):\n",
        "    \"\"\"Carga las imágenes y las aplana a vectores.\"\"\"\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        with Image.open(folder_route + path) as img:\n",
        "            img_array = np.array(img).reshape(-1)\n",
        "            images.append(img_array)\n",
        "    return np.array(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13v2Ol-ut_dJ"
      },
      "source": [
        "QuickDraw-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-dsSiI0ytMYW"
      },
      "outputs": [],
      "source": [
        "# # Cargar las rutas de entrenamiento y prueba junto con las etiquetas\n",
        "# train_image_paths_10, train_labels_10 = load_image_paths_and_labels(\"/content/imagenes_descomprimidas/QuickDraw-10/train.txt\")\n",
        "# test_image_paths_10, test_labels_10 = load_image_paths_and_labels(\"/content/imagenes_descomprimidas/QuickDraw-10/test.txt\")\n",
        "\n",
        "# # Cargar y procesar las imágenes\n",
        "# train_images_10 = load_images(train_image_paths_10, \"/content/imagenes_descomprimidas/QuickDraw-10/\")\n",
        "# test_images_10 = load_images(test_image_paths_10, \"/content/imagenes_descomprimidas/QuickDraw-10/\")\n",
        "\n",
        "\n",
        "# Cargar las rutas de entrenamiento y prueba junto con las etiquetas\n",
        "train_image_paths_10, train_labels_10 = load_image_paths_and_labels(\"./QuickDraw-10/train.txt\")\n",
        "test_image_paths_10, test_labels_10 = load_image_paths_and_labels(\"./QuickDraw-10/test.txt\")\n",
        "\n",
        "# Cargar y procesar las imágenes\n",
        "train_images_10 = load_images(train_image_paths_10, \"./QuickDraw-10/\")\n",
        "test_images_10 = load_images(test_image_paths_10, \"./QuickDraw-10/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lWI6a-Ddt7Wn"
      },
      "outputs": [],
      "source": [
        "# Preparacion datos de validacion\n",
        "train_images_10, val_image_10, train_labels_10, val_labels_10 = train_test_split(\n",
        "    train_images_10, train_labels_10, test_size=0.15, random_state=42, stratify=train_labels_10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yY1zYbFDHEbZ"
      },
      "outputs": [],
      "source": [
        "# normalizacion\n",
        "train_images_10 = train_images_10.astype('float32') / 255.0\n",
        "test_images_10 = test_images_10.astype('float32') / 255.0\n",
        "val_image_10 = val_image_10.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kQjCrgw8NJKO"
      },
      "outputs": [],
      "source": [
        "# one hot encoding\n",
        "n_classes=10\n",
        "train_labels_10 = to_categorical(train_labels_10, num_classes=n_classes)\n",
        "test_labels_10 = to_categorical(test_labels_10, num_classes=n_classes)\n",
        "val_labels_10 = to_categorical(val_labels_10, num_classes=n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALUYnf1iuAMD"
      },
      "source": [
        "QuickDraw-Animals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wznfNYHnuClR"
      },
      "outputs": [],
      "source": [
        "def load_images_and_labels(dataset):\n",
        "    \"\"\"\n",
        "    Carga imágenes y etiquetas para train y test del dataset QuickDraw-Animals.\n",
        "\n",
        "    Retorna:\n",
        "      X_test, y_test, X_train, y_train (numpy arrays)\n",
        "    \"\"\"\n",
        "    if dataset != \"Animals\":\n",
        "        raise ValueError(\"Sólo soporta dataset 'Animals'.\")\n",
        "\n",
        "    # base_path = '/content/imagenes_descomprimidas/QuickDraw-Animals/'\n",
        "    base_path = './QuickDraw-Animals'\n",
        "    mapping_file = os.path.join(base_path, 'mapping.txt')\n",
        "\n",
        "    # Leer mapping.txt y crear diccionario etiqueta->número\n",
        "    label_map = {}\n",
        "    with open(mapping_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                label = parts[0]\n",
        "                idx = int(parts[1])\n",
        "                label_map[label] = idx\n",
        "\n",
        "    def load_images_from_folder(folder_path):\n",
        "        images = []\n",
        "        labels = []\n",
        "        # Las subcarpetas son las clases\n",
        "        for label_name in sorted(os.listdir(folder_path)):\n",
        "            label_folder = os.path.join(folder_path, label_name)\n",
        "            if os.path.isdir(label_folder) and label_name in label_map:\n",
        "                for img_file in sorted(os.listdir(label_folder)):\n",
        "                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        img_path = os.path.join(label_folder, img_file)\n",
        "                        with Image.open(img_path) as img:\n",
        "                            img_array = np.array(img).reshape(-1)\n",
        "                            images.append(img_array)\n",
        "                            labels.append(label_map[label_name])\n",
        "        return np.array(images), np.array(labels)\n",
        "\n",
        "    # Cargar test\n",
        "    # X_test, y_test = load_images_from_folder('/content/imagenes_descomprimidas/QuickDraw-Animals/test_images/test_images/')\n",
        "    X_test, y_test = load_images_from_folder('./QuickDraw-Animals/test_images/test_images')\n",
        "\n",
        "    # Cargar train\n",
        "    # X_train, y_train = load_images_from_folder('/content/imagenes_descomprimidas/QuickDraw-Animals/train_images/train_images/')\n",
        "    X_train, y_train = load_images_from_folder('./QuickDraw-Animals/train_images/train_images')\n",
        "\n",
        "\n",
        "    return X_test, y_test, X_train, y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rGJkQEGNuLgQ"
      },
      "outputs": [],
      "source": [
        "test_images_animals, test_labels_animals, train_images_animals, train_labels_animals = load_images_and_labels(\"Animals\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I7OuAJoQuXNZ"
      },
      "outputs": [],
      "source": [
        "# Preparacion datos de validacion\n",
        "train_images_animals, val_images_animals, train_labels_animals, val_labels_animals = train_test_split(\n",
        "    train_images_animals, train_labels_animals, test_size=0.15, random_state=42, stratify=train_labels_animals\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_pZQys-EH0SE"
      },
      "outputs": [],
      "source": [
        "# normalizacion\n",
        "train_images_animals = train_images_animals.astype('float32') / 255.0\n",
        "test_images_animals = test_images_animals.astype('float32') / 255.0\n",
        "val_images_animals = val_images_animals.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w5LhfPwPH-th"
      },
      "outputs": [],
      "source": [
        "# one hot encoding\n",
        "n_classes=12\n",
        "train_labels_animals = to_categorical(train_labels_animals, num_classes=n_classes)\n",
        "test_labels_animals = to_categorical(test_labels_animals, num_classes=n_classes)\n",
        "val_labels_animals = to_categorical(val_labels_animals, num_classes=n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEtptvDwuZGK"
      },
      "source": [
        "## Parte 2 - Construcción de modelos\n",
        "\n",
        "Clase MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9-IjfB__ueYv"
      },
      "outputs": [],
      "source": [
        "class MLP(tf.keras.Model):\n",
        "    # defining components\n",
        "    def __init__(self, layers_size, n_classes, activation='sigmoid'):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_list = []\n",
        "        for lsize in layers_size:\n",
        "            self.layer_list.append(tf.keras.layers.Dense(lsize))\n",
        "        self.classifier = tf.keras.layers.Dense(n_classes)\n",
        "        self.activation = activation\n",
        "\n",
        "\n",
        "    # defining architecture\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for mlp_layer in self.layer_list:\n",
        "            x = mlp_layer(x)\n",
        "            if self.activation == 'sigmoid':\n",
        "                x = tf.keras.activations.sigmoid(x)\n",
        "            elif self.activation == 'relu':\n",
        "                x = tf.keras.activations.relu(x)\n",
        "            elif self.activation == 'tanh':\n",
        "                x = tf.keras.activations.tanh(x)\n",
        "            else:\n",
        "                raise ValueError(\"Activación no soportada\")\n",
        "        x = self.classifier(x)\n",
        "        return tf.keras.activations.softmax(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA1PFhonukLZ"
      },
      "source": [
        "Funciones para entrenar, evaluar y realizar los experimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Cr-CDYlmuk-6"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, loss_fn, epochs=10, batch_size=32):\n",
        "\n",
        "    # Entrenamiento\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(),\n",
        "        loss=loss_fn,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Detiene el proceso tras 5 epocas sin encontrar una mejora. Evalúa la pérdida en el\n",
        "    # conjunto de validación.\n",
        "    early_stopping = EarlyStopping(\n",
        "      monitor='val_loss',\n",
        "      patience=5,\n",
        "      restore_best_weights=True,\n",
        "      verbose=1\n",
        "    )\n",
        "\n",
        "    final_model = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        # verbose=0\n",
        "    )\n",
        "    return final_model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, class_names):\n",
        "    y_pred_probs = model(X_test, training=False).numpy()\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Si y_test es one-hot, convierte a etiquetas enteras\n",
        "    if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
        "        y_test_int = np.argmax(y_test, axis=1)\n",
        "    else:\n",
        "        y_test_int = y_test\n",
        "\n",
        "    acc_total = accuracy_score(y_test_int, y_pred)\n",
        "\n",
        "    acc_per_class = {}\n",
        "    for cls in np.unique(y_test_int):\n",
        "        idx = y_test_int == cls\n",
        "        acc = accuracy_score(y_test_int[idx], y_pred[idx])\n",
        "        acc_per_class[class_names[cls]] = acc\n",
        "\n",
        "    cm = confusion_matrix(y_test_int, y_pred)\n",
        "\n",
        "    return acc_total, acc_per_class, cm\n",
        "\n",
        "\n",
        "def experiment(X_train, y_train, X_val, y_val, X_test, y_test, layers_size, n_classes, activation, loss_fn, class_names, epochs=10, batch_size=None, n_experiments=5):\n",
        "    acc_totals = []\n",
        "    acc_classes_list = []\n",
        "    cm_list = []\n",
        "\n",
        "    for i in range(n_experiments):\n",
        "        print(f\"\\nEntrenamiento número {i+1}\")\n",
        "        # Crear modelo nuevo para reinicializar pesos\n",
        "        model = MLP(layers_size, n_classes, activation)\n",
        "        model.build(input_shape=(None, X_train.shape[1]))\n",
        "\n",
        "        # Entrenar\n",
        "        train_model(model, X_train, y_train, X_val, y_val, loss_fn, epochs, batch_size)\n",
        "\n",
        "        # Evaluar\n",
        "        acc_total, acc_per_class, cm = evaluate_model(model, X_test, y_test, class_names)\n",
        "        acc_totals.append(acc_total)\n",
        "        acc_classes_list.append(acc_per_class)\n",
        "        cm_list.append(cm)\n",
        "\n",
        "    return acc_totals, acc_classes_list, cm_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOUMjhDBEN3"
      },
      "source": [
        "Experimentos para QuickDraw-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8ZQEHnrSBJMV"
      },
      "outputs": [],
      "source": [
        "clases = ['bandage', 'blackberry', 'castle', 'flashlight', 'lion', 'remote-control', 'sink', 'spreadsheet', 'teapot', 'trombone']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MJNUkmE4unJW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenamiento número 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 196ms/step - accuracy: 0.0941 - loss: 2.4282 - val_accuracy: 0.1172 - val_loss: 2.3005\n",
            "Epoch 2/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.1131 - loss: 2.3006 - val_accuracy: 0.1091 - val_loss: 2.2977\n",
            "Epoch 3/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.1223 - loss: 2.2976 - val_accuracy: 0.1551 - val_loss: 2.2945\n",
            "Epoch 4/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.1426 - loss: 2.2940 - val_accuracy: 0.1369 - val_loss: 2.2912\n",
            "Epoch 5/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.1563 - loss: 2.2898 - val_accuracy: 0.1782 - val_loss: 2.2876\n",
            "Epoch 6/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.1643 - loss: 2.2857 - val_accuracy: 0.2751 - val_loss: 2.2819\n",
            "Epoch 7/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.2041 - loss: 2.2806 - val_accuracy: 0.2270 - val_loss: 2.2778\n",
            "Epoch 8/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.2076 - loss: 2.2747 - val_accuracy: 0.1999 - val_loss: 2.2717\n",
            "Epoch 9/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 171ms/step - accuracy: 0.2337 - loss: 2.2685 - val_accuracy: 0.2663 - val_loss: 2.2655\n",
            "Epoch 10/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.2584 - loss: 2.2634 - val_accuracy: 0.2527 - val_loss: 2.2601\n",
            "Epoch 11/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.2895 - loss: 2.2554 - val_accuracy: 0.2364 - val_loss: 2.2540\n",
            "Epoch 12/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.2666 - loss: 2.2499 - val_accuracy: 0.2093 - val_loss: 2.2474\n",
            "Epoch 13/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.2869 - loss: 2.2416 - val_accuracy: 0.2764 - val_loss: 2.2382\n",
            "Epoch 14/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3082 - loss: 2.2338 - val_accuracy: 0.3056 - val_loss: 2.2289\n",
            "Epoch 15/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.3342 - loss: 2.2239 - val_accuracy: 0.3455 - val_loss: 2.2192\n",
            "Epoch 16/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.3364 - loss: 2.2142 - val_accuracy: 0.3774 - val_loss: 2.2090\n",
            "Epoch 17/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3531 - loss: 2.2006 - val_accuracy: 0.3889 - val_loss: 2.1983\n",
            "Epoch 18/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3497 - loss: 2.1931 - val_accuracy: 0.2358 - val_loss: 2.1914\n",
            "Epoch 19/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.3540 - loss: 2.1813 - val_accuracy: 0.3388 - val_loss: 2.1750\n",
            "Epoch 20/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3711 - loss: 2.1661 - val_accuracy: 0.4153 - val_loss: 2.1597\n",
            "Epoch 21/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.3848 - loss: 2.1516 - val_accuracy: 0.3855 - val_loss: 2.1453\n",
            "Epoch 22/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.3692 - loss: 2.1363 - val_accuracy: 0.4438 - val_loss: 2.1292\n",
            "Epoch 23/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3971 - loss: 2.1185 - val_accuracy: 0.3530 - val_loss: 2.1138\n",
            "Epoch 24/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.3879 - loss: 2.1026 - val_accuracy: 0.3862 - val_loss: 2.0967\n",
            "Epoch 25/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.4012 - loss: 2.0832 - val_accuracy: 0.3997 - val_loss: 2.0763\n",
            "Epoch 26/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4034 - loss: 2.0598 - val_accuracy: 0.3997 - val_loss: 2.0575\n",
            "Epoch 27/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3989 - loss: 2.0403 - val_accuracy: 0.4058 - val_loss: 2.0415\n",
            "Epoch 28/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4230 - loss: 2.0230 - val_accuracy: 0.3957 - val_loss: 2.0171\n",
            "Epoch 29/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4147 - loss: 2.0114 - val_accuracy: 0.4322 - val_loss: 1.9975\n",
            "Epoch 30/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4152 - loss: 1.9767 - val_accuracy: 0.4289 - val_loss: 1.9771\n",
            "Epoch 31/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4357 - loss: 1.9577 - val_accuracy: 0.4377 - val_loss: 1.9555\n",
            "Epoch 32/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4418 - loss: 1.9330 - val_accuracy: 0.4688 - val_loss: 1.9314\n",
            "Epoch 33/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4397 - loss: 1.9167 - val_accuracy: 0.4289 - val_loss: 1.9133\n",
            "Epoch 34/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4442 - loss: 1.8940 - val_accuracy: 0.4275 - val_loss: 1.8990\n",
            "Epoch 35/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4513 - loss: 1.8664 - val_accuracy: 0.4404 - val_loss: 1.8681\n",
            "Epoch 36/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4489 - loss: 1.8512 - val_accuracy: 0.4533 - val_loss: 1.8425\n",
            "Epoch 37/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4617 - loss: 1.8254 - val_accuracy: 0.4214 - val_loss: 1.8245\n",
            "Epoch 38/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4490 - loss: 1.8028 - val_accuracy: 0.3970 - val_loss: 1.8188\n",
            "Epoch 39/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4456 - loss: 1.7823 - val_accuracy: 0.4661 - val_loss: 1.7790\n",
            "Epoch 40/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4624 - loss: 1.7636 - val_accuracy: 0.4661 - val_loss: 1.7559\n",
            "Epoch 41/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4553 - loss: 1.7411 - val_accuracy: 0.4661 - val_loss: 1.7417\n",
            "Epoch 42/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4637 - loss: 1.7297 - val_accuracy: 0.4614 - val_loss: 1.7208\n",
            "Epoch 43/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4690 - loss: 1.7161 - val_accuracy: 0.4851 - val_loss: 1.6927\n",
            "Epoch 44/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.4725 - loss: 1.6814 - val_accuracy: 0.4743 - val_loss: 1.6774\n",
            "Epoch 45/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4803 - loss: 1.6757 - val_accuracy: 0.4729 - val_loss: 1.6644\n",
            "Epoch 46/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4722 - loss: 1.6577 - val_accuracy: 0.4648 - val_loss: 1.6631\n",
            "Epoch 47/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4676 - loss: 1.6465 - val_accuracy: 0.4810 - val_loss: 1.6520\n",
            "Epoch 48/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4892 - loss: 1.6137 - val_accuracy: 0.4478 - val_loss: 1.6336\n",
            "Epoch 49/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4863 - loss: 1.6021 - val_accuracy: 0.4607 - val_loss: 1.6338\n",
            "Epoch 50/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4813 - loss: 1.6031 - val_accuracy: 0.4499 - val_loss: 1.6282\n",
            "Epoch 51/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4903 - loss: 1.5764 - val_accuracy: 0.4675 - val_loss: 1.5858\n",
            "Epoch 52/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4842 - loss: 1.5845 - val_accuracy: 0.4404 - val_loss: 1.6263\n",
            "Epoch 53/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4856 - loss: 1.5858 - val_accuracy: 0.4593 - val_loss: 1.6019\n",
            "Epoch 54/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4838 - loss: 1.5533 - val_accuracy: 0.5156 - val_loss: 1.5572\n",
            "Epoch 55/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5065 - loss: 1.5282 - val_accuracy: 0.5054 - val_loss: 1.5297\n",
            "Epoch 56/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5055 - loss: 1.5290 - val_accuracy: 0.4153 - val_loss: 1.6596\n",
            "Epoch 57/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5002 - loss: 1.5144 - val_accuracy: 0.4573 - val_loss: 1.5964\n",
            "Epoch 58/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5030 - loss: 1.5248 - val_accuracy: 0.5061 - val_loss: 1.5158\n",
            "Epoch 59/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5037 - loss: 1.5031 - val_accuracy: 0.4492 - val_loss: 1.5929\n",
            "Epoch 60/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5064 - loss: 1.5110 - val_accuracy: 0.5102 - val_loss: 1.5212\n",
            "Epoch 61/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5315 - loss: 1.4698 - val_accuracy: 0.4776 - val_loss: 1.5231\n",
            "Epoch 62/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5293 - loss: 1.4537 - val_accuracy: 0.5203 - val_loss: 1.4886\n",
            "Epoch 63/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5269 - loss: 1.4689 - val_accuracy: 0.5251 - val_loss: 1.4834\n",
            "Epoch 64/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5380 - loss: 1.4639 - val_accuracy: 0.5413 - val_loss: 1.4614\n",
            "Epoch 65/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5340 - loss: 1.4663 - val_accuracy: 0.5203 - val_loss: 1.4734\n",
            "Epoch 66/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5418 - loss: 1.4435 - val_accuracy: 0.5047 - val_loss: 1.5016\n",
            "Epoch 67/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5406 - loss: 1.4417 - val_accuracy: 0.5373 - val_loss: 1.4598\n",
            "Epoch 68/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5392 - loss: 1.4348 - val_accuracy: 0.5359 - val_loss: 1.4624\n",
            "Epoch 69/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5360 - loss: 1.4501 - val_accuracy: 0.5183 - val_loss: 1.4824\n",
            "Epoch 70/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5476 - loss: 1.4092 - val_accuracy: 0.5230 - val_loss: 1.4618\n",
            "Epoch 71/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5487 - loss: 1.4179 - val_accuracy: 0.5427 - val_loss: 1.4370\n",
            "Epoch 72/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5470 - loss: 1.4189 - val_accuracy: 0.5427 - val_loss: 1.4334\n",
            "Epoch 73/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5594 - loss: 1.3938 - val_accuracy: 0.5461 - val_loss: 1.4440\n",
            "Epoch 74/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5619 - loss: 1.3753 - val_accuracy: 0.5115 - val_loss: 1.4568\n",
            "Epoch 75/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5567 - loss: 1.3941 - val_accuracy: 0.5447 - val_loss: 1.4148\n",
            "Epoch 76/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.5517 - loss: 1.3974 - val_accuracy: 0.5325 - val_loss: 1.4610\n",
            "Epoch 77/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 168ms/step - accuracy: 0.5673 - loss: 1.3669 - val_accuracy: 0.5501 - val_loss: 1.4020\n",
            "Epoch 78/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5791 - loss: 1.3475 - val_accuracy: 0.5542 - val_loss: 1.4030\n",
            "Epoch 79/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5667 - loss: 1.3725 - val_accuracy: 0.5508 - val_loss: 1.4247\n",
            "Epoch 80/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5731 - loss: 1.3648 - val_accuracy: 0.5501 - val_loss: 1.3867\n",
            "Epoch 81/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5675 - loss: 1.3651 - val_accuracy: 0.5589 - val_loss: 1.4055\n",
            "Epoch 82/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5605 - loss: 1.3613 - val_accuracy: 0.4919 - val_loss: 1.5553\n",
            "Epoch 83/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.5742 - loss: 1.3471 - val_accuracy: 0.5427 - val_loss: 1.4392\n",
            "Epoch 84/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5806 - loss: 1.3441 - val_accuracy: 0.4973 - val_loss: 1.5245\n",
            "Epoch 85/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5936 - loss: 1.3201 - val_accuracy: 0.5657 - val_loss: 1.3791\n",
            "Epoch 86/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5955 - loss: 1.3252 - val_accuracy: 0.5732 - val_loss: 1.3643\n",
            "Epoch 87/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5917 - loss: 1.3070 - val_accuracy: 0.5623 - val_loss: 1.3686\n",
            "Epoch 88/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6090 - loss: 1.2815 - val_accuracy: 0.5095 - val_loss: 1.4740\n",
            "Epoch 89/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5977 - loss: 1.2746 - val_accuracy: 0.5711 - val_loss: 1.3482\n",
            "Epoch 90/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.5959 - loss: 1.2996 - val_accuracy: 0.5379 - val_loss: 1.4278\n",
            "Epoch 91/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6037 - loss: 1.2993 - val_accuracy: 0.5854 - val_loss: 1.3458\n",
            "Epoch 92/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6134 - loss: 1.2870 - val_accuracy: 0.5657 - val_loss: 1.3999\n",
            "Epoch 93/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.6148 - loss: 1.2735 - val_accuracy: 0.5623 - val_loss: 1.3888\n",
            "Epoch 94/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6179 - loss: 1.2806 - val_accuracy: 0.5081 - val_loss: 1.4358\n",
            "Epoch 95/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6060 - loss: 1.2697 - val_accuracy: 0.5840 - val_loss: 1.3293\n",
            "Epoch 96/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6236 - loss: 1.2527 - val_accuracy: 0.5813 - val_loss: 1.3155\n",
            "Epoch 97/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6210 - loss: 1.2601 - val_accuracy: 0.5393 - val_loss: 1.4130\n",
            "Epoch 98/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6129 - loss: 1.2403 - val_accuracy: 0.5725 - val_loss: 1.3565\n",
            "Epoch 99/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.6196 - loss: 1.2447 - val_accuracy: 0.6091 - val_loss: 1.2865\n",
            "Epoch 100/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6382 - loss: 1.2033 - val_accuracy: 0.5420 - val_loss: 1.4002\n",
            "\n",
            "Entrenamiento número 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.1037 - loss: 2.3359 - val_accuracy: 0.1023 - val_loss: 2.2999\n",
            "Epoch 2/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.1129 - loss: 2.2998 - val_accuracy: 0.1111 - val_loss: 2.2968\n",
            "Epoch 3/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.1434 - loss: 2.2963 - val_accuracy: 0.1084 - val_loss: 2.2924\n",
            "Epoch 4/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.1297 - loss: 2.2910 - val_accuracy: 0.1897 - val_loss: 2.2873\n",
            "Epoch 5/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.1612 - loss: 2.2861 - val_accuracy: 0.2297 - val_loss: 2.2834\n",
            "Epoch 6/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.2093 - loss: 2.2821 - val_accuracy: 0.1714 - val_loss: 2.2793\n",
            "Epoch 7/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.2272 - loss: 2.2778 - val_accuracy: 0.1863 - val_loss: 2.2734\n",
            "Epoch 8/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.2469 - loss: 2.2710 - val_accuracy: 0.2893 - val_loss: 2.2675\n",
            "Epoch 9/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.2874 - loss: 2.2664 - val_accuracy: 0.2222 - val_loss: 2.2629\n",
            "Epoch 10/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.2669 - loss: 2.2601 - val_accuracy: 0.3022 - val_loss: 2.2550\n",
            "Epoch 11/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.3257 - loss: 2.2536 - val_accuracy: 0.3347 - val_loss: 2.2488\n",
            "Epoch 12/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.2885 - loss: 2.2459 - val_accuracy: 0.3706 - val_loss: 2.2409\n",
            "Epoch 13/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.3459 - loss: 2.2367 - val_accuracy: 0.3530 - val_loss: 2.2339\n",
            "Epoch 14/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.3340 - loss: 2.2285 - val_accuracy: 0.3618 - val_loss: 2.2246\n",
            "Epoch 15/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - accuracy: 0.3503 - loss: 2.2207 - val_accuracy: 0.3896 - val_loss: 2.2141\n",
            "Epoch 16/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 115ms/step - accuracy: 0.3635 - loss: 2.2093 - val_accuracy: 0.2676 - val_loss: 2.2052\n",
            "Epoch 17/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.3429 - loss: 2.1990 - val_accuracy: 0.3618 - val_loss: 2.1941\n",
            "Epoch 18/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.3848 - loss: 2.1860 - val_accuracy: 0.4133 - val_loss: 2.1803\n",
            "Epoch 19/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 115ms/step - accuracy: 0.4025 - loss: 2.1745 - val_accuracy: 0.4282 - val_loss: 2.1677\n",
            "Epoch 20/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.4115 - loss: 2.1607 - val_accuracy: 0.3855 - val_loss: 2.1545\n",
            "Epoch 21/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.4109 - loss: 2.1428 - val_accuracy: 0.3943 - val_loss: 2.1373\n",
            "Epoch 22/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.4142 - loss: 2.1271 - val_accuracy: 0.4465 - val_loss: 2.1184\n",
            "Epoch 23/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.4155 - loss: 2.1076 - val_accuracy: 0.4295 - val_loss: 2.1019\n",
            "Epoch 24/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.4274 - loss: 2.0901 - val_accuracy: 0.3753 - val_loss: 2.0836\n",
            "Epoch 25/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.4284 - loss: 2.0686 - val_accuracy: 0.4214 - val_loss: 2.0616\n",
            "Epoch 26/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.4128 - loss: 2.0420 - val_accuracy: 0.4221 - val_loss: 2.0416\n",
            "Epoch 27/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4347 - loss: 2.0189 - val_accuracy: 0.4438 - val_loss: 2.0177\n",
            "Epoch 28/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4253 - loss: 2.0023 - val_accuracy: 0.4492 - val_loss: 1.9958\n",
            "Epoch 29/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.4376 - loss: 1.9824 - val_accuracy: 0.4607 - val_loss: 1.9713\n",
            "Epoch 30/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.4535 - loss: 1.9557 - val_accuracy: 0.4383 - val_loss: 1.9487\n",
            "Epoch 31/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.4439 - loss: 1.9313 - val_accuracy: 0.4411 - val_loss: 1.9286\n",
            "Epoch 32/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 117ms/step - accuracy: 0.4545 - loss: 1.8995 - val_accuracy: 0.4553 - val_loss: 1.9008\n",
            "Epoch 33/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.4593 - loss: 1.8805 - val_accuracy: 0.4709 - val_loss: 1.8774\n",
            "Epoch 34/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.4676 - loss: 1.8578 - val_accuracy: 0.4526 - val_loss: 1.8623\n",
            "Epoch 35/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.4582 - loss: 1.8442 - val_accuracy: 0.4533 - val_loss: 1.8373\n",
            "Epoch 36/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4676 - loss: 1.8153 - val_accuracy: 0.4472 - val_loss: 1.8128\n",
            "Epoch 37/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4629 - loss: 1.7996 - val_accuracy: 0.4336 - val_loss: 1.8240\n",
            "Epoch 38/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4808 - loss: 1.7778 - val_accuracy: 0.4654 - val_loss: 1.7759\n",
            "Epoch 39/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.4734 - loss: 1.7518 - val_accuracy: 0.4824 - val_loss: 1.7527\n",
            "Epoch 40/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.4777 - loss: 1.7439 - val_accuracy: 0.4668 - val_loss: 1.7394\n",
            "Epoch 41/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.4762 - loss: 1.7220 - val_accuracy: 0.4336 - val_loss: 1.7606\n",
            "Epoch 42/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.4847 - loss: 1.6942 - val_accuracy: 0.4160 - val_loss: 1.7342\n",
            "Epoch 43/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4834 - loss: 1.6788 - val_accuracy: 0.4627 - val_loss: 1.7199\n",
            "Epoch 44/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.4874 - loss: 1.6605 - val_accuracy: 0.5075 - val_loss: 1.6655\n",
            "Epoch 45/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4939 - loss: 1.6576 - val_accuracy: 0.4939 - val_loss: 1.6594\n",
            "Epoch 46/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.5044 - loss: 1.6363 - val_accuracy: 0.4851 - val_loss: 1.6624\n",
            "Epoch 47/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5092 - loss: 1.6132 - val_accuracy: 0.4695 - val_loss: 1.6550\n",
            "Epoch 48/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.4971 - loss: 1.6161 - val_accuracy: 0.5169 - val_loss: 1.6116\n",
            "Epoch 49/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5109 - loss: 1.5838 - val_accuracy: 0.4993 - val_loss: 1.5954\n",
            "Epoch 50/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5101 - loss: 1.5864 - val_accuracy: 0.5373 - val_loss: 1.5836\n",
            "Epoch 51/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5285 - loss: 1.5715 - val_accuracy: 0.4499 - val_loss: 1.6439\n",
            "Epoch 52/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 116ms/step - accuracy: 0.5268 - loss: 1.5611 - val_accuracy: 0.4837 - val_loss: 1.5884\n",
            "Epoch 53/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5206 - loss: 1.5427 - val_accuracy: 0.5312 - val_loss: 1.5487\n",
            "Epoch 54/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5299 - loss: 1.5353 - val_accuracy: 0.4770 - val_loss: 1.5550\n",
            "Epoch 55/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.5246 - loss: 1.5211 - val_accuracy: 0.5088 - val_loss: 1.5357\n",
            "Epoch 56/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5244 - loss: 1.5331 - val_accuracy: 0.4966 - val_loss: 1.5425\n",
            "Epoch 57/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5310 - loss: 1.4999 - val_accuracy: 0.4797 - val_loss: 1.6108\n",
            "Epoch 58/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.5310 - loss: 1.4973 - val_accuracy: 0.5054 - val_loss: 1.5664\n",
            "Epoch 59/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.5462 - loss: 1.4716 - val_accuracy: 0.4763 - val_loss: 1.5647\n",
            "Epoch 60/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5470 - loss: 1.4584 - val_accuracy: 0.5346 - val_loss: 1.5116\n",
            "Epoch 61/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5493 - loss: 1.4425 - val_accuracy: 0.5407 - val_loss: 1.4865\n",
            "Epoch 62/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 115ms/step - accuracy: 0.5600 - loss: 1.4333 - val_accuracy: 0.5528 - val_loss: 1.4544\n",
            "Epoch 63/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.5642 - loss: 1.4246 - val_accuracy: 0.5020 - val_loss: 1.5186\n",
            "Epoch 64/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.5369 - loss: 1.4774 - val_accuracy: 0.5630 - val_loss: 1.4571\n",
            "Epoch 65/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 117ms/step - accuracy: 0.5555 - loss: 1.4385 - val_accuracy: 0.5650 - val_loss: 1.4185\n",
            "Epoch 66/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.5619 - loss: 1.4191 - val_accuracy: 0.5501 - val_loss: 1.4281\n",
            "Epoch 67/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5697 - loss: 1.3881 - val_accuracy: 0.5352 - val_loss: 1.4511\n",
            "Epoch 68/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5684 - loss: 1.3900 - val_accuracy: 0.5569 - val_loss: 1.4070\n",
            "Epoch 69/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5740 - loss: 1.3710 - val_accuracy: 0.5495 - val_loss: 1.4242\n",
            "Epoch 70/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5764 - loss: 1.3721 - val_accuracy: 0.5637 - val_loss: 1.3865\n",
            "Epoch 71/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5817 - loss: 1.3498 - val_accuracy: 0.5752 - val_loss: 1.4086\n",
            "Epoch 72/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.5893 - loss: 1.3370 - val_accuracy: 0.5556 - val_loss: 1.4377\n",
            "Epoch 73/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5843 - loss: 1.3497 - val_accuracy: 0.5671 - val_loss: 1.3804\n",
            "Epoch 74/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.5835 - loss: 1.3559 - val_accuracy: 0.5501 - val_loss: 1.4638\n",
            "Epoch 75/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5792 - loss: 1.3499 - val_accuracy: 0.5264 - val_loss: 1.4524\n",
            "Epoch 76/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.5872 - loss: 1.3254 - val_accuracy: 0.5772 - val_loss: 1.3494\n",
            "Epoch 77/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5973 - loss: 1.3191 - val_accuracy: 0.5745 - val_loss: 1.3662\n",
            "Epoch 78/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.5912 - loss: 1.3049 - val_accuracy: 0.5752 - val_loss: 1.3826\n",
            "Epoch 79/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.5834 - loss: 1.3305 - val_accuracy: 0.5569 - val_loss: 1.3849\n",
            "Epoch 80/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5965 - loss: 1.2948 - val_accuracy: 0.5996 - val_loss: 1.3341\n",
            "Epoch 81/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.6131 - loss: 1.2941 - val_accuracy: 0.5759 - val_loss: 1.3391\n",
            "Epoch 82/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.6093 - loss: 1.2739 - val_accuracy: 0.4472 - val_loss: 1.6619\n",
            "Epoch 83/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.5898 - loss: 1.2982 - val_accuracy: 0.5474 - val_loss: 1.3741\n",
            "Epoch 84/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.6081 - loss: 1.2785 - val_accuracy: 0.5955 - val_loss: 1.3134\n",
            "Epoch 85/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.6225 - loss: 1.2490 - val_accuracy: 0.5908 - val_loss: 1.3312\n",
            "Epoch 86/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 111ms/step - accuracy: 0.6126 - loss: 1.2550 - val_accuracy: 0.5488 - val_loss: 1.4149\n",
            "Epoch 87/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.6189 - loss: 1.2407 - val_accuracy: 0.5603 - val_loss: 1.3745\n",
            "Epoch 88/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.6097 - loss: 1.2511 - val_accuracy: 0.5569 - val_loss: 1.3770\n",
            "Epoch 89/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.6375 - loss: 1.2258 - val_accuracy: 0.5305 - val_loss: 1.4170\n",
            "Epoch 90/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.6147 - loss: 1.2232 - val_accuracy: 0.6104 - val_loss: 1.2874\n",
            "Epoch 91/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.6291 - loss: 1.2182 - val_accuracy: 0.5888 - val_loss: 1.3206\n",
            "Epoch 92/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.6299 - loss: 1.2124 - val_accuracy: 0.5434 - val_loss: 1.4088\n",
            "Epoch 93/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.6376 - loss: 1.2118 - val_accuracy: 0.5474 - val_loss: 1.4133\n",
            "Epoch 94/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.6403 - loss: 1.1876 - val_accuracy: 0.5196 - val_loss: 1.4306\n",
            "Epoch 95/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.6167 - loss: 1.2392 - val_accuracy: 0.5359 - val_loss: 1.4191\n",
            "Epoch 96/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.6424 - loss: 1.1719 - val_accuracy: 0.5779 - val_loss: 1.3321\n",
            "Epoch 97/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.6519 - loss: 1.1716 - val_accuracy: 0.6023 - val_loss: 1.2869\n",
            "Epoch 98/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.6470 - loss: 1.1674 - val_accuracy: 0.5847 - val_loss: 1.3319\n",
            "Epoch 99/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 0.6383 - loss: 1.1857 - val_accuracy: 0.5596 - val_loss: 1.3793\n",
            "Epoch 100/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.6281 - loss: 1.2085 - val_accuracy: 0.5732 - val_loss: 1.3567\n",
            "\n",
            "Entrenamiento número 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.0921 - loss: 2.3545 - val_accuracy: 0.1003 - val_loss: 2.2986\n",
            "Epoch 2/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.1177 - loss: 2.2979 - val_accuracy: 0.1077 - val_loss: 2.2952\n",
            "Epoch 3/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.1369 - loss: 2.2943 - val_accuracy: 0.1016 - val_loss: 2.2908\n",
            "Epoch 4/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.1368 - loss: 2.2892 - val_accuracy: 0.2127 - val_loss: 2.2861\n",
            "Epoch 5/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.1573 - loss: 2.2847 - val_accuracy: 0.1064 - val_loss: 2.2822\n",
            "Epoch 6/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.1864 - loss: 2.2797 - val_accuracy: 0.1579 - val_loss: 2.2745\n",
            "Epoch 7/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.1991 - loss: 2.2734 - val_accuracy: 0.3103 - val_loss: 2.2683\n",
            "Epoch 8/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.2592 - loss: 2.2663 - val_accuracy: 0.3950 - val_loss: 2.2612\n",
            "Epoch 9/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.3022 - loss: 2.2591 - val_accuracy: 0.3259 - val_loss: 2.2551\n",
            "Epoch 10/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.2737 - loss: 2.2521 - val_accuracy: 0.2358 - val_loss: 2.2473\n",
            "Epoch 11/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.2969 - loss: 2.2431 - val_accuracy: 0.2805 - val_loss: 2.2382\n",
            "Epoch 12/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.2956 - loss: 2.2332 - val_accuracy: 0.3855 - val_loss: 2.2282\n",
            "Epoch 13/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.3696 - loss: 2.2228 - val_accuracy: 0.3035 - val_loss: 2.2184\n",
            "Epoch 14/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.3468 - loss: 2.2123 - val_accuracy: 0.3692 - val_loss: 2.2070\n",
            "Epoch 15/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.3946 - loss: 2.2022 - val_accuracy: 0.3665 - val_loss: 2.1943\n",
            "Epoch 16/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.3825 - loss: 2.1895 - val_accuracy: 0.3638 - val_loss: 2.1823\n",
            "Epoch 17/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.3783 - loss: 2.1725 - val_accuracy: 0.3211 - val_loss: 2.1672\n",
            "Epoch 18/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.3885 - loss: 2.1584 - val_accuracy: 0.4885 - val_loss: 2.1498\n",
            "Epoch 19/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.4325 - loss: 2.1376 - val_accuracy: 0.3665 - val_loss: 2.1330\n",
            "Epoch 20/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.4079 - loss: 2.1256 - val_accuracy: 0.4397 - val_loss: 2.1164\n",
            "Epoch 21/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4165 - loss: 2.0988 - val_accuracy: 0.4289 - val_loss: 2.0986\n",
            "Epoch 22/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.4193 - loss: 2.0853 - val_accuracy: 0.4770 - val_loss: 2.0734\n",
            "Epoch 23/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4481 - loss: 2.0598 - val_accuracy: 0.3726 - val_loss: 2.0597\n",
            "Epoch 24/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4485 - loss: 2.0400 - val_accuracy: 0.4180 - val_loss: 2.0295\n",
            "Epoch 25/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.4380 - loss: 2.0210 - val_accuracy: 0.3747 - val_loss: 2.0151\n",
            "Epoch 26/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4440 - loss: 1.9970 - val_accuracy: 0.4533 - val_loss: 1.9838\n",
            "Epoch 27/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4433 - loss: 1.9632 - val_accuracy: 0.4573 - val_loss: 1.9642\n",
            "Epoch 28/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.4448 - loss: 1.9446 - val_accuracy: 0.4600 - val_loss: 1.9357\n",
            "Epoch 29/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.4638 - loss: 1.9202 - val_accuracy: 0.4512 - val_loss: 1.9090\n",
            "Epoch 30/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.4569 - loss: 1.8995 - val_accuracy: 0.4160 - val_loss: 1.8943\n",
            "Epoch 31/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4651 - loss: 1.8612 - val_accuracy: 0.4289 - val_loss: 1.8804\n",
            "Epoch 32/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.4821 - loss: 1.8563 - val_accuracy: 0.4695 - val_loss: 1.8408\n",
            "Epoch 33/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.4636 - loss: 1.8302 - val_accuracy: 0.4743 - val_loss: 1.8142\n",
            "Epoch 34/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4656 - loss: 1.8040 - val_accuracy: 0.4675 - val_loss: 1.7929\n",
            "Epoch 35/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.4659 - loss: 1.7915 - val_accuracy: 0.5020 - val_loss: 1.7741\n",
            "Epoch 36/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4808 - loss: 1.7506 - val_accuracy: 0.4851 - val_loss: 1.7526\n",
            "Epoch 37/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.4836 - loss: 1.7342 - val_accuracy: 0.5020 - val_loss: 1.7367\n",
            "Epoch 38/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.4991 - loss: 1.7196 - val_accuracy: 0.4953 - val_loss: 1.7188\n",
            "Epoch 39/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.4904 - loss: 1.7061 - val_accuracy: 0.4140 - val_loss: 1.7444\n",
            "Epoch 40/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.4931 - loss: 1.6860 - val_accuracy: 0.4919 - val_loss: 1.6817\n",
            "Epoch 41/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5102 - loss: 1.6591 - val_accuracy: 0.5102 - val_loss: 1.6652\n",
            "Epoch 42/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.5123 - loss: 1.6422 - val_accuracy: 0.5014 - val_loss: 1.6456\n",
            "Epoch 43/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5083 - loss: 1.6188 - val_accuracy: 0.5068 - val_loss: 1.6234\n",
            "Epoch 44/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5108 - loss: 1.6167 - val_accuracy: 0.5027 - val_loss: 1.6262\n",
            "Epoch 45/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.5150 - loss: 1.5957 - val_accuracy: 0.5176 - val_loss: 1.6087\n",
            "Epoch 46/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 147ms/step - accuracy: 0.5099 - loss: 1.5809 - val_accuracy: 0.4919 - val_loss: 1.6130\n",
            "Epoch 47/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 147ms/step - accuracy: 0.5247 - loss: 1.5570 - val_accuracy: 0.5156 - val_loss: 1.5955\n",
            "Epoch 48/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.5184 - loss: 1.5699 - val_accuracy: 0.4573 - val_loss: 1.5958\n",
            "Epoch 49/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.5182 - loss: 1.5516 - val_accuracy: 0.5142 - val_loss: 1.5518\n",
            "Epoch 50/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.5222 - loss: 1.5302 - val_accuracy: 0.5210 - val_loss: 1.5483\n",
            "Epoch 51/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.5267 - loss: 1.5322 - val_accuracy: 0.4173 - val_loss: 1.6954\n",
            "Epoch 52/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.5198 - loss: 1.5250 - val_accuracy: 0.5190 - val_loss: 1.5519\n",
            "Epoch 53/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5192 - loss: 1.5446 - val_accuracy: 0.5339 - val_loss: 1.5076\n",
            "Epoch 54/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5414 - loss: 1.4749 - val_accuracy: 0.4905 - val_loss: 1.5424\n",
            "Epoch 55/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5232 - loss: 1.5099 - val_accuracy: 0.5413 - val_loss: 1.4884\n",
            "Epoch 56/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5324 - loss: 1.4864 - val_accuracy: 0.5196 - val_loss: 1.4993\n",
            "Epoch 57/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5357 - loss: 1.4802 - val_accuracy: 0.5447 - val_loss: 1.4748\n",
            "Epoch 58/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5535 - loss: 1.4603 - val_accuracy: 0.4363 - val_loss: 1.6938\n",
            "Epoch 59/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.5398 - loss: 1.4717 - val_accuracy: 0.5359 - val_loss: 1.4866\n",
            "Epoch 60/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.5456 - loss: 1.4556 - val_accuracy: 0.5413 - val_loss: 1.4796\n",
            "Epoch 61/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.5496 - loss: 1.4462 - val_accuracy: 0.4797 - val_loss: 1.5621\n",
            "Epoch 62/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5536 - loss: 1.4475 - val_accuracy: 0.4458 - val_loss: 1.6454\n",
            "Epoch 63/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5441 - loss: 1.4932 - val_accuracy: 0.4329 - val_loss: 1.6891\n",
            "Epoch 64/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5483 - loss: 1.4411 - val_accuracy: 0.5684 - val_loss: 1.4233\n",
            "Epoch 65/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5610 - loss: 1.4240 - val_accuracy: 0.5251 - val_loss: 1.4891\n",
            "Epoch 66/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.5660 - loss: 1.4342 - val_accuracy: 0.4736 - val_loss: 1.5462\n",
            "Epoch 67/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.5672 - loss: 1.3993 - val_accuracy: 0.5136 - val_loss: 1.4832\n",
            "Epoch 68/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5680 - loss: 1.3932 - val_accuracy: 0.5366 - val_loss: 1.4429\n",
            "Epoch 69/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5660 - loss: 1.4056 - val_accuracy: 0.5718 - val_loss: 1.4074\n",
            "Epoch 70/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5907 - loss: 1.3768 - val_accuracy: 0.4973 - val_loss: 1.5140\n",
            "Epoch 71/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5763 - loss: 1.3749 - val_accuracy: 0.5474 - val_loss: 1.4371\n",
            "Epoch 72/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.5915 - loss: 1.3534 - val_accuracy: 0.5698 - val_loss: 1.3955\n",
            "Epoch 73/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5953 - loss: 1.3361 - val_accuracy: 0.5569 - val_loss: 1.4201\n",
            "Epoch 74/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5841 - loss: 1.3542 - val_accuracy: 0.5671 - val_loss: 1.3868\n",
            "Epoch 75/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6001 - loss: 1.3221 - val_accuracy: 0.5623 - val_loss: 1.4130\n",
            "Epoch 76/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5901 - loss: 1.3454 - val_accuracy: 0.5759 - val_loss: 1.3638\n",
            "Epoch 77/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6077 - loss: 1.3213 - val_accuracy: 0.5637 - val_loss: 1.4121\n",
            "Epoch 78/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5988 - loss: 1.3206 - val_accuracy: 0.5088 - val_loss: 1.5684\n",
            "Epoch 79/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6008 - loss: 1.3148 - val_accuracy: 0.5596 - val_loss: 1.4370\n",
            "Epoch 80/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6007 - loss: 1.3153 - val_accuracy: 0.5603 - val_loss: 1.4020\n",
            "Epoch 81/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5901 - loss: 1.3338 - val_accuracy: 0.5732 - val_loss: 1.3472\n",
            "Epoch 82/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6163 - loss: 1.2716 - val_accuracy: 0.5766 - val_loss: 1.3538\n",
            "Epoch 83/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6120 - loss: 1.2909 - val_accuracy: 0.5840 - val_loss: 1.3773\n",
            "Epoch 84/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.6242 - loss: 1.2578 - val_accuracy: 0.5772 - val_loss: 1.3490\n",
            "Epoch 85/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6162 - loss: 1.2694 - val_accuracy: 0.5393 - val_loss: 1.4091\n",
            "Epoch 86/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.6259 - loss: 1.2625 - val_accuracy: 0.4743 - val_loss: 1.5606\n",
            "Epoch 87/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6144 - loss: 1.2832 - val_accuracy: 0.5996 - val_loss: 1.3137\n",
            "Epoch 88/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6171 - loss: 1.2599 - val_accuracy: 0.5732 - val_loss: 1.3316\n",
            "Epoch 89/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6176 - loss: 1.2548 - val_accuracy: 0.6077 - val_loss: 1.3181\n",
            "Epoch 90/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.6194 - loss: 1.2704 - val_accuracy: 0.5996 - val_loss: 1.2905\n",
            "Epoch 91/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.6349 - loss: 1.2316 - val_accuracy: 0.5711 - val_loss: 1.3404\n",
            "Epoch 92/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.6215 - loss: 1.2357 - val_accuracy: 0.6064 - val_loss: 1.3001\n",
            "Epoch 93/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.6290 - loss: 1.2232 - val_accuracy: 0.5738 - val_loss: 1.3568\n",
            "Epoch 94/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6328 - loss: 1.2185 - val_accuracy: 0.5894 - val_loss: 1.3060\n",
            "Epoch 95/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6518 - loss: 1.1981 - val_accuracy: 0.5556 - val_loss: 1.4326\n",
            "Epoch 96/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.6412 - loss: 1.1969 - val_accuracy: 0.5711 - val_loss: 1.3702\n",
            "Epoch 97/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6458 - loss: 1.1862 - val_accuracy: 0.5196 - val_loss: 1.5476\n",
            "Epoch 98/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6272 - loss: 1.2288 - val_accuracy: 0.5786 - val_loss: 1.3276\n",
            "Epoch 99/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.6579 - loss: 1.1629 - val_accuracy: 0.6003 - val_loss: 1.3020\n",
            "Epoch 100/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.6417 - loss: 1.1994 - val_accuracy: 0.5684 - val_loss: 1.3666\n",
            "\n",
            "Entrenamiento número 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 164ms/step - accuracy: 0.1077 - loss: 2.3737 - val_accuracy: 0.1375 - val_loss: 2.2996\n",
            "Epoch 2/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.1346 - loss: 2.2987 - val_accuracy: 0.1287 - val_loss: 2.2954\n",
            "Epoch 3/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.1440 - loss: 2.2950 - val_accuracy: 0.1138 - val_loss: 2.2921\n",
            "Epoch 4/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.1496 - loss: 2.2915 - val_accuracy: 0.2690 - val_loss: 2.2873\n",
            "Epoch 5/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.1831 - loss: 2.2863 - val_accuracy: 0.1416 - val_loss: 2.2831\n",
            "Epoch 6/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.1869 - loss: 2.2815 - val_accuracy: 0.1999 - val_loss: 2.2786\n",
            "Epoch 7/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.1928 - loss: 2.2766 - val_accuracy: 0.2073 - val_loss: 2.2722\n",
            "Epoch 8/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.2294 - loss: 2.2703 - val_accuracy: 0.3625 - val_loss: 2.2658\n",
            "Epoch 9/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.2467 - loss: 2.2642 - val_accuracy: 0.2419 - val_loss: 2.2602\n",
            "Epoch 10/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.2353 - loss: 2.2577 - val_accuracy: 0.1653 - val_loss: 2.2558\n",
            "Epoch 11/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.2336 - loss: 2.2501 - val_accuracy: 0.2879 - val_loss: 2.2469\n",
            "Epoch 12/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.2940 - loss: 2.2417 - val_accuracy: 0.2371 - val_loss: 2.2388\n",
            "Epoch 13/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.3185 - loss: 2.2346 - val_accuracy: 0.3509 - val_loss: 2.2297\n",
            "Epoch 14/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3278 - loss: 2.2211 - val_accuracy: 0.4322 - val_loss: 2.2209\n",
            "Epoch 15/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.3383 - loss: 2.2187 - val_accuracy: 0.3598 - val_loss: 2.2113\n",
            "Epoch 16/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.3357 - loss: 2.2023 - val_accuracy: 0.3862 - val_loss: 2.2004\n",
            "Epoch 17/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.3563 - loss: 2.1972 - val_accuracy: 0.3198 - val_loss: 2.1884\n",
            "Epoch 18/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.3493 - loss: 2.1809 - val_accuracy: 0.4058 - val_loss: 2.1768\n",
            "Epoch 19/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.3804 - loss: 2.1704 - val_accuracy: 0.3191 - val_loss: 2.1649\n",
            "Epoch 20/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.3909 - loss: 2.1543 - val_accuracy: 0.3570 - val_loss: 2.1498\n",
            "Epoch 21/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 168ms/step - accuracy: 0.3824 - loss: 2.1378 - val_accuracy: 0.4140 - val_loss: 2.1339\n",
            "Epoch 22/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.4115 - loss: 2.1221 - val_accuracy: 0.3692 - val_loss: 2.1183\n",
            "Epoch 23/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.3915 - loss: 2.1045 - val_accuracy: 0.3252 - val_loss: 2.1038\n",
            "Epoch 24/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4050 - loss: 2.0882 - val_accuracy: 0.4282 - val_loss: 2.0852\n",
            "Epoch 25/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.4140 - loss: 2.0644 - val_accuracy: 0.4153 - val_loss: 2.0674\n",
            "Epoch 26/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.4402 - loss: 2.0480 - val_accuracy: 0.4533 - val_loss: 2.0492\n",
            "Epoch 27/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.4253 - loss: 2.0239 - val_accuracy: 0.4824 - val_loss: 2.0334\n",
            "Epoch 28/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4378 - loss: 2.0109 - val_accuracy: 0.4505 - val_loss: 2.0095\n",
            "Epoch 29/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.4405 - loss: 1.9870 - val_accuracy: 0.4722 - val_loss: 1.9889\n",
            "Epoch 30/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4478 - loss: 1.9743 - val_accuracy: 0.4214 - val_loss: 1.9763\n",
            "Epoch 31/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4496 - loss: 1.9474 - val_accuracy: 0.4566 - val_loss: 1.9558\n",
            "Epoch 32/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4623 - loss: 1.9260 - val_accuracy: 0.4201 - val_loss: 1.9269\n",
            "Epoch 33/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.4532 - loss: 1.9106 - val_accuracy: 0.4729 - val_loss: 1.9086\n",
            "Epoch 34/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4575 - loss: 1.9019 - val_accuracy: 0.4749 - val_loss: 1.8839\n",
            "Epoch 35/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.4786 - loss: 1.8658 - val_accuracy: 0.4444 - val_loss: 1.8789\n",
            "Epoch 36/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.4739 - loss: 1.8459 - val_accuracy: 0.4593 - val_loss: 1.8581\n",
            "Epoch 37/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4834 - loss: 1.8134 - val_accuracy: 0.4715 - val_loss: 1.8271\n",
            "Epoch 38/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4600 - loss: 1.8136 - val_accuracy: 0.5047 - val_loss: 1.7996\n",
            "Epoch 39/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.4835 - loss: 1.7829 - val_accuracy: 0.4858 - val_loss: 1.7765\n",
            "Epoch 40/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4878 - loss: 1.7602 - val_accuracy: 0.4729 - val_loss: 1.7727\n",
            "Epoch 41/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.4876 - loss: 1.7458 - val_accuracy: 0.4295 - val_loss: 1.7742\n",
            "Epoch 42/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4805 - loss: 1.7389 - val_accuracy: 0.4268 - val_loss: 1.7379\n",
            "Epoch 43/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4845 - loss: 1.6976 - val_accuracy: 0.4654 - val_loss: 1.7076\n",
            "Epoch 44/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4896 - loss: 1.6802 - val_accuracy: 0.4946 - val_loss: 1.6823\n",
            "Epoch 45/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5046 - loss: 1.6500 - val_accuracy: 0.4675 - val_loss: 1.6817\n",
            "Epoch 46/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5036 - loss: 1.6421 - val_accuracy: 0.5346 - val_loss: 1.6590\n",
            "Epoch 47/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5142 - loss: 1.6256 - val_accuracy: 0.4844 - val_loss: 1.6398\n",
            "Epoch 48/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 158ms/step - accuracy: 0.4935 - loss: 1.6210 - val_accuracy: 0.5014 - val_loss: 1.6068\n",
            "Epoch 49/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4869 - loss: 1.6178 - val_accuracy: 0.4770 - val_loss: 1.6051\n",
            "Epoch 50/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5088 - loss: 1.5984 - val_accuracy: 0.4959 - val_loss: 1.6074\n",
            "Epoch 51/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5196 - loss: 1.5685 - val_accuracy: 0.5224 - val_loss: 1.5721\n",
            "Epoch 52/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5168 - loss: 1.5533 - val_accuracy: 0.5298 - val_loss: 1.5668\n",
            "Epoch 53/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5192 - loss: 1.5525 - val_accuracy: 0.5142 - val_loss: 1.5564\n",
            "Epoch 54/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5227 - loss: 1.5406 - val_accuracy: 0.5169 - val_loss: 1.5550\n",
            "Epoch 55/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5259 - loss: 1.5265 - val_accuracy: 0.5298 - val_loss: 1.5240\n",
            "Epoch 56/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5300 - loss: 1.5144 - val_accuracy: 0.5169 - val_loss: 1.5247\n",
            "Epoch 57/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5174 - loss: 1.5162 - val_accuracy: 0.5447 - val_loss: 1.5061\n",
            "Epoch 58/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5248 - loss: 1.5140 - val_accuracy: 0.4973 - val_loss: 1.5104\n",
            "Epoch 59/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5299 - loss: 1.4896 - val_accuracy: 0.5440 - val_loss: 1.4861\n",
            "Epoch 60/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5249 - loss: 1.4951 - val_accuracy: 0.5501 - val_loss: 1.4985\n",
            "Epoch 61/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.5578 - loss: 1.4611 - val_accuracy: 0.5393 - val_loss: 1.5004\n",
            "Epoch 62/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5405 - loss: 1.4815 - val_accuracy: 0.4905 - val_loss: 1.5507\n",
            "Epoch 63/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5339 - loss: 1.4710 - val_accuracy: 0.5102 - val_loss: 1.5107\n",
            "Epoch 64/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5429 - loss: 1.4697 - val_accuracy: 0.5481 - val_loss: 1.4468\n",
            "Epoch 65/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5440 - loss: 1.4430 - val_accuracy: 0.4322 - val_loss: 1.6405\n",
            "Epoch 66/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5537 - loss: 1.4415 - val_accuracy: 0.5501 - val_loss: 1.4799\n",
            "Epoch 67/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5603 - loss: 1.4208 - val_accuracy: 0.5603 - val_loss: 1.4433\n",
            "Epoch 68/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5625 - loss: 1.4174 - val_accuracy: 0.5650 - val_loss: 1.4210\n",
            "Epoch 69/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5684 - loss: 1.4006 - val_accuracy: 0.5407 - val_loss: 1.4544\n",
            "Epoch 70/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5505 - loss: 1.4172 - val_accuracy: 0.5474 - val_loss: 1.4264\n",
            "Epoch 71/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5628 - loss: 1.4171 - val_accuracy: 0.5766 - val_loss: 1.4030\n",
            "Epoch 72/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5745 - loss: 1.3837 - val_accuracy: 0.5474 - val_loss: 1.4315\n",
            "Epoch 73/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5778 - loss: 1.3832 - val_accuracy: 0.5806 - val_loss: 1.3924\n",
            "Epoch 74/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5825 - loss: 1.3705 - val_accuracy: 0.5705 - val_loss: 1.3933\n",
            "Epoch 75/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5751 - loss: 1.3668 - val_accuracy: 0.4146 - val_loss: 1.6685\n",
            "Epoch 76/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5718 - loss: 1.3714 - val_accuracy: 0.5291 - val_loss: 1.4711\n",
            "Epoch 77/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5865 - loss: 1.3520 - val_accuracy: 0.4959 - val_loss: 1.5066\n",
            "Epoch 78/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5773 - loss: 1.3602 - val_accuracy: 0.5264 - val_loss: 1.4067\n",
            "Epoch 79/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5889 - loss: 1.3370 - val_accuracy: 0.5325 - val_loss: 1.4181\n",
            "Epoch 80/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5954 - loss: 1.3310 - val_accuracy: 0.5759 - val_loss: 1.3538\n",
            "Epoch 81/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5838 - loss: 1.3513 - val_accuracy: 0.5400 - val_loss: 1.4180\n",
            "Epoch 82/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5848 - loss: 1.3251 - val_accuracy: 0.4966 - val_loss: 1.5274\n",
            "Epoch 83/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5808 - loss: 1.3466 - val_accuracy: 0.5562 - val_loss: 1.3811\n",
            "Epoch 84/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5987 - loss: 1.2974 - val_accuracy: 0.5664 - val_loss: 1.3736\n",
            "Epoch 85/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5949 - loss: 1.3192 - val_accuracy: 0.5752 - val_loss: 1.3820\n",
            "Epoch 86/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6082 - loss: 1.2866 - val_accuracy: 0.5955 - val_loss: 1.3499\n",
            "Epoch 87/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6043 - loss: 1.2919 - val_accuracy: 0.5474 - val_loss: 1.3780\n",
            "Epoch 88/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6159 - loss: 1.2702 - val_accuracy: 0.5820 - val_loss: 1.3373\n",
            "Epoch 89/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6092 - loss: 1.2828 - val_accuracy: 0.5637 - val_loss: 1.3521\n",
            "Epoch 90/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.6236 - loss: 1.2554 - val_accuracy: 0.5359 - val_loss: 1.4062\n",
            "Epoch 91/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6099 - loss: 1.2698 - val_accuracy: 0.5440 - val_loss: 1.4118\n",
            "Epoch 92/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5912 - loss: 1.3106 - val_accuracy: 0.5549 - val_loss: 1.3627\n",
            "Epoch 93/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6101 - loss: 1.2523 - val_accuracy: 0.5278 - val_loss: 1.4133\n",
            "Epoch 94/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.6236 - loss: 1.2358 - val_accuracy: 0.5467 - val_loss: 1.4019\n",
            "Epoch 95/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6222 - loss: 1.2435 - val_accuracy: 0.6023 - val_loss: 1.2977\n",
            "Epoch 96/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6202 - loss: 1.2353 - val_accuracy: 0.5799 - val_loss: 1.3293\n",
            "Epoch 97/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6304 - loss: 1.2303 - val_accuracy: 0.5813 - val_loss: 1.3292\n",
            "Epoch 98/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.6276 - loss: 1.2135 - val_accuracy: 0.5949 - val_loss: 1.2965\n",
            "Epoch 99/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.6320 - loss: 1.2120 - val_accuracy: 0.5420 - val_loss: 1.4114\n",
            "Epoch 100/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6408 - loss: 1.2025 - val_accuracy: 0.6030 - val_loss: 1.2825\n",
            "\n",
            "Entrenamiento número 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 165ms/step - accuracy: 0.1026 - loss: 2.3571 - val_accuracy: 0.1009 - val_loss: 2.2985\n",
            "Epoch 2/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.1239 - loss: 2.2979 - val_accuracy: 0.1294 - val_loss: 2.2953\n",
            "Epoch 3/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.1175 - loss: 2.2951 - val_accuracy: 0.1341 - val_loss: 2.2916\n",
            "Epoch 4/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.1615 - loss: 2.2909 - val_accuracy: 0.2114 - val_loss: 2.2871\n",
            "Epoch 5/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.1777 - loss: 2.2869 - val_accuracy: 0.1511 - val_loss: 2.2818\n",
            "Epoch 6/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.1856 - loss: 2.2810 - val_accuracy: 0.2337 - val_loss: 2.2777\n",
            "Epoch 7/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.2141 - loss: 2.2762 - val_accuracy: 0.2873 - val_loss: 2.2728\n",
            "Epoch 8/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.2450 - loss: 2.2709 - val_accuracy: 0.1877 - val_loss: 2.2667\n",
            "Epoch 9/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.2557 - loss: 2.2638 - val_accuracy: 0.1579 - val_loss: 2.2616\n",
            "Epoch 10/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.2417 - loss: 2.2581 - val_accuracy: 0.3496 - val_loss: 2.2522\n",
            "Epoch 11/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.3155 - loss: 2.2503 - val_accuracy: 0.3171 - val_loss: 2.2447\n",
            "Epoch 12/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.3238 - loss: 2.2427 - val_accuracy: 0.3909 - val_loss: 2.2371\n",
            "Epoch 13/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.3428 - loss: 2.2329 - val_accuracy: 0.2493 - val_loss: 2.2288\n",
            "Epoch 14/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.3184 - loss: 2.2276 - val_accuracy: 0.3930 - val_loss: 2.2186\n",
            "Epoch 15/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.3745 - loss: 2.2131 - val_accuracy: 0.3225 - val_loss: 2.2096\n",
            "Epoch 16/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3750 - loss: 2.2041 - val_accuracy: 0.3347 - val_loss: 2.1978\n",
            "Epoch 17/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.3534 - loss: 2.1901 - val_accuracy: 0.3591 - val_loss: 2.1859\n",
            "Epoch 18/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.3870 - loss: 2.1812 - val_accuracy: 0.3950 - val_loss: 2.1711\n",
            "Epoch 19/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.3762 - loss: 2.1645 - val_accuracy: 0.4112 - val_loss: 2.1586\n",
            "Epoch 20/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4110 - loss: 2.1488 - val_accuracy: 0.3225 - val_loss: 2.1432\n",
            "Epoch 21/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.3733 - loss: 2.1369 - val_accuracy: 0.4533 - val_loss: 2.1234\n",
            "Epoch 22/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.4373 - loss: 2.1166 - val_accuracy: 0.3625 - val_loss: 2.1057\n",
            "Epoch 23/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4035 - loss: 2.0926 - val_accuracy: 0.3828 - val_loss: 2.0867\n",
            "Epoch 24/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4040 - loss: 2.0747 - val_accuracy: 0.3882 - val_loss: 2.0657\n",
            "Epoch 25/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4185 - loss: 2.0500 - val_accuracy: 0.4201 - val_loss: 2.0432\n",
            "Epoch 26/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4284 - loss: 2.0282 - val_accuracy: 0.4302 - val_loss: 2.0195\n",
            "Epoch 27/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4276 - loss: 2.0065 - val_accuracy: 0.4573 - val_loss: 1.9968\n",
            "Epoch 28/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4539 - loss: 1.9789 - val_accuracy: 0.4505 - val_loss: 1.9732\n",
            "Epoch 29/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4428 - loss: 1.9593 - val_accuracy: 0.4411 - val_loss: 1.9471\n",
            "Epoch 30/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4412 - loss: 1.9352 - val_accuracy: 0.4607 - val_loss: 1.9212\n",
            "Epoch 31/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4544 - loss: 1.9070 - val_accuracy: 0.4370 - val_loss: 1.9002\n",
            "Epoch 32/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4529 - loss: 1.8791 - val_accuracy: 0.4526 - val_loss: 1.8709\n",
            "Epoch 33/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4587 - loss: 1.8556 - val_accuracy: 0.4715 - val_loss: 1.8457\n",
            "Epoch 34/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4651 - loss: 1.8133 - val_accuracy: 0.4770 - val_loss: 1.8208\n",
            "Epoch 35/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4867 - loss: 1.8021 - val_accuracy: 0.4194 - val_loss: 1.8135\n",
            "Epoch 36/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4724 - loss: 1.7765 - val_accuracy: 0.4627 - val_loss: 1.7759\n",
            "Epoch 37/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.4629 - loss: 1.7637 - val_accuracy: 0.4709 - val_loss: 1.7534\n",
            "Epoch 38/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4794 - loss: 1.7363 - val_accuracy: 0.4797 - val_loss: 1.7465\n",
            "Epoch 39/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4884 - loss: 1.7223 - val_accuracy: 0.4729 - val_loss: 1.7061\n",
            "Epoch 40/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4806 - loss: 1.6874 - val_accuracy: 0.4573 - val_loss: 1.7063\n",
            "Epoch 41/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4840 - loss: 1.6794 - val_accuracy: 0.5102 - val_loss: 1.6710\n",
            "Epoch 42/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4879 - loss: 1.6570 - val_accuracy: 0.5061 - val_loss: 1.6513\n",
            "Epoch 43/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4866 - loss: 1.6371 - val_accuracy: 0.5102 - val_loss: 1.6343\n",
            "Epoch 44/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4961 - loss: 1.6220 - val_accuracy: 0.4966 - val_loss: 1.6320\n",
            "Epoch 45/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5035 - loss: 1.6004 - val_accuracy: 0.5034 - val_loss: 1.6002\n",
            "Epoch 46/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5102 - loss: 1.5878 - val_accuracy: 0.4844 - val_loss: 1.5977\n",
            "Epoch 47/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5022 - loss: 1.5683 - val_accuracy: 0.5176 - val_loss: 1.5797\n",
            "Epoch 48/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5140 - loss: 1.5694 - val_accuracy: 0.5047 - val_loss: 1.5678\n",
            "Epoch 49/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5217 - loss: 1.5448 - val_accuracy: 0.4539 - val_loss: 1.6176\n",
            "Epoch 50/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5126 - loss: 1.5354 - val_accuracy: 0.4424 - val_loss: 1.6293\n",
            "Epoch 51/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5116 - loss: 1.5495 - val_accuracy: 0.5257 - val_loss: 1.5290\n",
            "Epoch 52/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5154 - loss: 1.5267 - val_accuracy: 0.5075 - val_loss: 1.5305\n",
            "Epoch 53/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.5194 - loss: 1.4967 - val_accuracy: 0.5102 - val_loss: 1.5340\n",
            "Epoch 54/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.5257 - loss: 1.4983 - val_accuracy: 0.5346 - val_loss: 1.5203\n",
            "Epoch 55/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.5337 - loss: 1.4867 - val_accuracy: 0.5359 - val_loss: 1.4920\n",
            "Epoch 56/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 170ms/step - accuracy: 0.5257 - loss: 1.4925 - val_accuracy: 0.5163 - val_loss: 1.5090\n",
            "Epoch 57/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 171ms/step - accuracy: 0.5265 - loss: 1.4768 - val_accuracy: 0.5156 - val_loss: 1.5004\n",
            "Epoch 58/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5414 - loss: 1.4603 - val_accuracy: 0.5427 - val_loss: 1.4889\n",
            "Epoch 59/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5384 - loss: 1.4469 - val_accuracy: 0.5393 - val_loss: 1.4698\n",
            "Epoch 60/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5486 - loss: 1.4406 - val_accuracy: 0.5264 - val_loss: 1.4918\n",
            "Epoch 61/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5312 - loss: 1.4668 - val_accuracy: 0.5569 - val_loss: 1.4500\n",
            "Epoch 62/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5550 - loss: 1.4362 - val_accuracy: 0.5501 - val_loss: 1.4575\n",
            "Epoch 63/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5551 - loss: 1.4238 - val_accuracy: 0.5366 - val_loss: 1.4695\n",
            "Epoch 64/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5475 - loss: 1.4222 - val_accuracy: 0.5528 - val_loss: 1.4443\n",
            "Epoch 65/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5715 - loss: 1.3928 - val_accuracy: 0.5467 - val_loss: 1.4762\n",
            "Epoch 66/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5538 - loss: 1.4248 - val_accuracy: 0.5501 - val_loss: 1.4312\n",
            "Epoch 67/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5634 - loss: 1.4048 - val_accuracy: 0.5332 - val_loss: 1.4521\n",
            "Epoch 68/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5570 - loss: 1.4085 - val_accuracy: 0.5108 - val_loss: 1.4900\n",
            "Epoch 69/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5587 - loss: 1.3956 - val_accuracy: 0.5271 - val_loss: 1.5011\n",
            "Epoch 70/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5663 - loss: 1.3871 - val_accuracy: 0.5583 - val_loss: 1.4075\n",
            "Epoch 71/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.5777 - loss: 1.3475 - val_accuracy: 0.5738 - val_loss: 1.3888\n",
            "Epoch 72/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5635 - loss: 1.3894 - val_accuracy: 0.5528 - val_loss: 1.4271\n",
            "Epoch 73/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5713 - loss: 1.3735 - val_accuracy: 0.4241 - val_loss: 1.6370\n",
            "Epoch 74/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.5740 - loss: 1.3792 - val_accuracy: 0.4546 - val_loss: 1.6253\n",
            "Epoch 75/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5604 - loss: 1.3921 - val_accuracy: 0.5793 - val_loss: 1.3707\n",
            "Epoch 76/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5668 - loss: 1.3688 - val_accuracy: 0.5413 - val_loss: 1.4269\n",
            "Epoch 77/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5756 - loss: 1.3482 - val_accuracy: 0.5610 - val_loss: 1.3988\n",
            "Epoch 78/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5980 - loss: 1.3068 - val_accuracy: 0.5610 - val_loss: 1.4218\n",
            "Epoch 79/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5877 - loss: 1.3399 - val_accuracy: 0.5766 - val_loss: 1.3729\n",
            "Epoch 80/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.5798 - loss: 1.3449 - val_accuracy: 0.5691 - val_loss: 1.3771\n",
            "Epoch 81/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.6052 - loss: 1.3049 - val_accuracy: 0.5542 - val_loss: 1.4035\n",
            "Epoch 82/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.6070 - loss: 1.2861 - val_accuracy: 0.4973 - val_loss: 1.4862\n",
            "Epoch 83/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5691 - loss: 1.3546 - val_accuracy: 0.5542 - val_loss: 1.3881\n",
            "Epoch 84/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5926 - loss: 1.3135 - val_accuracy: 0.5955 - val_loss: 1.3276\n",
            "Epoch 85/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 158ms/step - accuracy: 0.5918 - loss: 1.3051 - val_accuracy: 0.5522 - val_loss: 1.4275\n",
            "Epoch 86/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5933 - loss: 1.3093 - val_accuracy: 0.5657 - val_loss: 1.3582\n",
            "Epoch 87/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.6100 - loss: 1.2728 - val_accuracy: 0.5955 - val_loss: 1.3480\n",
            "Epoch 88/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.6054 - loss: 1.2751 - val_accuracy: 0.5678 - val_loss: 1.3846\n",
            "Epoch 89/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 157ms/step - accuracy: 0.6140 - loss: 1.2706 - val_accuracy: 0.6016 - val_loss: 1.3061\n",
            "Epoch 90/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6110 - loss: 1.2715 - val_accuracy: 0.5759 - val_loss: 1.3694\n",
            "Epoch 91/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.6259 - loss: 1.2481 - val_accuracy: 0.5678 - val_loss: 1.3683\n",
            "Epoch 92/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6199 - loss: 1.2560 - val_accuracy: 0.5976 - val_loss: 1.3148\n",
            "Epoch 93/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6411 - loss: 1.2195 - val_accuracy: 0.5901 - val_loss: 1.3280\n",
            "Epoch 94/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step - accuracy: 0.6322 - loss: 1.2329 - val_accuracy: 0.5095 - val_loss: 1.5109\n",
            "Epoch 95/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.6202 - loss: 1.2455 - val_accuracy: 0.5257 - val_loss: 1.4297\n",
            "Epoch 96/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6261 - loss: 1.2212 - val_accuracy: 0.5847 - val_loss: 1.3521\n",
            "Epoch 97/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6226 - loss: 1.2328 - val_accuracy: 0.5847 - val_loss: 1.3566\n",
            "Epoch 98/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6304 - loss: 1.2204 - val_accuracy: 0.5786 - val_loss: 1.3406\n",
            "Epoch 99/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6175 - loss: 1.2402 - val_accuracy: 0.4878 - val_loss: 1.5053\n",
            "Epoch 100/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6326 - loss: 1.1981 - val_accuracy: 0.5894 - val_loss: 1.3179\n"
          ]
        }
      ],
      "source": [
        "# Modelo 1 - QuickDraw-10\n",
        "# Detalles:\n",
        "# Activacion: sigmoid\n",
        "# Perdida: CrossEntropy\n",
        "# Capas: 3\n",
        "# Epocas: 100\n",
        "# Batch size: 100\n",
        "\n",
        "acc_total_1, acc_clase_1, cm_1 = experiment(\n",
        "        train_images_10,\n",
        "        train_labels_10,\n",
        "        val_image_10,\n",
        "        val_labels_10,\n",
        "        test_images_10,\n",
        "        test_labels_10,\n",
        "        layers_size=[512, 256,128],\n",
        "        n_classes=10,\n",
        "        activation='sigmoid',\n",
        "        loss_fn='categorical_crossentropy',\n",
        "        class_names=clases,\n",
        "        epochs=100,\n",
        "        batch_size=100,\n",
        "        n_experiments=5\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cwUV5IR5EpvA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'bandage': 0.8,\n",
              "  'blackberry': 0.4918032786885246,\n",
              "  'castle': 0.31297709923664124,\n",
              "  'flashlight': 0.5086206896551724,\n",
              "  'lion': 0.5904761904761905,\n",
              "  'remote-control': 0.7281553398058253,\n",
              "  'sink': 0.4827586206896552,\n",
              "  'spreadsheet': 0.6198347107438017,\n",
              "  'teapot': 0.823076923076923,\n",
              "  'trombone': 0.2647058823529412},\n",
              " {'bandage': 0.5416666666666666,\n",
              "  'blackberry': 0.8114754098360656,\n",
              "  'castle': 0.366412213740458,\n",
              "  'flashlight': 0.8189655172413793,\n",
              "  'lion': 0.5904761904761905,\n",
              "  'remote-control': 0.5242718446601942,\n",
              "  'sink': 0.5344827586206896,\n",
              "  'spreadsheet': 0.512396694214876,\n",
              "  'teapot': 0.7307692307692307,\n",
              "  'trombone': 0.6078431372549019},\n",
              " {'bandage': 0.5666666666666667,\n",
              "  'blackberry': 0.6311475409836066,\n",
              "  'castle': 0.48854961832061067,\n",
              "  'flashlight': 0.75,\n",
              "  'lion': 0.44761904761904764,\n",
              "  'remote-control': 0.6310679611650486,\n",
              "  'sink': 0.4051724137931034,\n",
              "  'spreadsheet': 0.7933884297520661,\n",
              "  'teapot': 0.7307692307692307,\n",
              "  'trombone': 0.4215686274509804},\n",
              " {'bandage': 0.7416666666666667,\n",
              "  'blackberry': 0.5573770491803278,\n",
              "  'castle': 0.5190839694656488,\n",
              "  'flashlight': 0.7758620689655172,\n",
              "  'lion': 0.4,\n",
              "  'remote-control': 0.6990291262135923,\n",
              "  'sink': 0.603448275862069,\n",
              "  'spreadsheet': 0.71900826446281,\n",
              "  'teapot': 0.7307692307692307,\n",
              "  'trombone': 0.6666666666666666},\n",
              " {'bandage': 0.6416666666666667,\n",
              "  'blackberry': 0.6967213114754098,\n",
              "  'castle': 0.5648854961832062,\n",
              "  'flashlight': 0.7586206896551724,\n",
              "  'lion': 0.45714285714285713,\n",
              "  'remote-control': 0.5922330097087378,\n",
              "  'sink': 0.5948275862068966,\n",
              "  'spreadsheet': 0.6033057851239669,\n",
              "  'teapot': 0.6692307692307692,\n",
              "  'trombone': 0.7549019607843137}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_clase_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5643224699828473,\n",
              " 0.6037735849056604,\n",
              " 0.5909090909090909,\n",
              " 0.6423670668953688,\n",
              " 0.6337907375643225]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_total_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[ 96,   1,   2,   1,   7,   6,   0,   2,   5,   0],\n",
              "        [ 31,  60,   5,   5,   7,   1,   0,   3,   8,   2],\n",
              "        [ 43,   0,  41,   2,   7,  21,   6,   3,   3,   5],\n",
              "        [ 17,   4,   2,  59,  17,   0,   1,  10,   6,   0],\n",
              "        [  2,   0,   0,   0,  62,   0,   0,  20,  20,   1],\n",
              "        [  9,   0,   1,   0,   5,  75,   8,   1,   3,   1],\n",
              "        [ 12,   6,  12,   1,   1,  22,  56,   3,   1,   2],\n",
              "        [  1,   1,   0,   1,  28,   6,   2,  75,   2,   5],\n",
              "        [  6,   3,   0,   0,   9,   4,   0,   1, 107,   0],\n",
              "        [  3,   3,   2,   2,  30,   3,   3,  26,   3,  27]]),\n",
              " array([[65, 18,  1, 20,  2,  3,  3,  1,  1,  6],\n",
              "        [ 4, 99,  0,  9,  4,  0,  1,  1,  2,  2],\n",
              "        [ 3, 21, 48, 16,  3,  3, 13,  0,  4, 20],\n",
              "        [ 1, 12,  2, 95,  4,  0,  0,  1,  1,  0],\n",
              "        [ 1,  5,  0, 10, 62,  0,  0,  6,  9, 12],\n",
              "        [ 3,  4,  4,  1,  5, 54, 23,  0,  4,  5],\n",
              "        [ 1, 24, 11,  2,  2,  7, 62,  0,  0,  7],\n",
              "        [ 0,  3,  1, 13, 21,  3,  3, 62,  2, 13],\n",
              "        [ 5,  7,  1,  9,  9,  1,  1,  0, 95,  2],\n",
              "        [ 0,  8,  1, 15,  5,  2,  2,  5,  2, 62]]),\n",
              " array([[68,  5,  2, 17, 13,  5,  0,  1,  4,  5],\n",
              "        [ 7, 77,  1, 17,  5,  0,  0,  5,  3,  7],\n",
              "        [ 5, 12, 64,  8,  5,  5,  1,  7,  4, 20],\n",
              "        [ 1,  6,  1, 87,  9,  0,  0,  9,  2,  1],\n",
              "        [ 1,  0,  0,  3, 47,  0,  0, 42,  6,  6],\n",
              "        [ 3,  3,  3,  1,  4, 65, 10,  4,  3,  7],\n",
              "        [ 1, 13, 17,  5,  4, 13, 47,  1,  1, 14],\n",
              "        [ 0,  2,  2,  2,  5,  3,  2, 96,  2,  7],\n",
              "        [ 3,  3,  2,  2, 16,  1,  0,  7, 95,  1],\n",
              "        [ 1,  2,  2,  9, 11,  1,  1, 30,  2, 43]]),\n",
              " array([[89,  2,  1, 13,  0,  5,  3,  1,  2,  4],\n",
              "        [ 9, 68,  3, 21,  3,  2,  2,  2,  3,  9],\n",
              "        [12,  1, 68,  6,  1, 11, 11,  1,  1, 19],\n",
              "        [ 4,  4,  4, 90,  3,  0,  1,  6,  1,  3],\n",
              "        [ 1,  0,  0,  3, 42,  0,  0, 31,  6, 22],\n",
              "        [ 3,  2,  3,  0,  2, 72, 13,  3,  1,  4],\n",
              "        [ 2,  7, 12,  2,  2, 16, 70,  0,  0,  5],\n",
              "        [ 1,  1,  1,  5,  4,  3,  4, 87,  2, 13],\n",
              "        [ 8,  4,  1,  4,  8,  4,  0,  3, 95,  3],\n",
              "        [ 1,  1,  1, 10,  3,  3,  4,  9,  2, 68]]),\n",
              " array([[77,  5,  2, 17,  1,  5,  2,  0,  1, 10],\n",
              "        [ 6, 85,  3, 12,  3,  2,  1,  1,  1,  8],\n",
              "        [ 6,  8, 74,  3,  0,  3, 12,  0,  0, 25],\n",
              "        [ 2, 14,  4, 88,  1,  0,  1,  3,  0,  3],\n",
              "        [ 1,  1,  0,  3, 48,  0,  0,  7,  5, 40],\n",
              "        [ 2,  3,  6,  0,  3, 61, 20,  2,  0,  6],\n",
              "        [ 1,  9, 18,  2,  1,  8, 69,  0,  0,  8],\n",
              "        [ 0,  2,  1,  6,  7,  3,  3, 73,  2, 24],\n",
              "        [ 8, 11,  2,  5, 10,  1,  1,  2, 87,  3],\n",
              "        [ 0,  6,  1,  4,  3,  2,  5,  2,  2, 77]])]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "kX7q-_lZDDCO",
        "outputId": "6244d292-402c-491a-a610-5e1551f38bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenamiento número 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 585ms/step - accuracy: 0.1014 - loss: 2.4620 - val_accuracy: 0.1009 - val_loss: 2.3749\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 439ms/step - accuracy: 0.1037 - loss: 2.3591 - val_accuracy: 0.1009 - val_loss: 2.3331\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 426ms/step - accuracy: 0.1027 - loss: 2.3284 - val_accuracy: 0.1009 - val_loss: 2.3161\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 426ms/step - accuracy: 0.1030 - loss: 2.3145 - val_accuracy: 0.1043 - val_loss: 2.3079\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 429ms/step - accuracy: 0.1068 - loss: 2.3071 - val_accuracy: 0.1009 - val_loss: 2.3042\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 428ms/step - accuracy: 0.1056 - loss: 2.3033 - val_accuracy: 0.1098 - val_loss: 2.3020\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 428ms/step - accuracy: 0.1135 - loss: 2.3021 - val_accuracy: 0.1064 - val_loss: 2.3008\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 452ms/step - accuracy: 0.1009 - loss: 2.3003 - val_accuracy: 0.1220 - val_loss: 2.3001\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 426ms/step - accuracy: 0.1133 - loss: 2.3000 - val_accuracy: 0.1260 - val_loss: 2.2995\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 424ms/step - accuracy: 0.1265 - loss: 2.2991 - val_accuracy: 0.1321 - val_loss: 2.2991\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 424ms/step - accuracy: 0.1253 - loss: 2.2986 - val_accuracy: 0.1545 - val_loss: 2.2986\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 425ms/step - accuracy: 0.1620 - loss: 2.2983 - val_accuracy: 0.1341 - val_loss: 2.2982\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 429ms/step - accuracy: 0.1329 - loss: 2.2979 - val_accuracy: 0.1518 - val_loss: 2.2978\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 427ms/step - accuracy: 0.1749 - loss: 2.2974 - val_accuracy: 0.1457 - val_loss: 2.2974\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 424ms/step - accuracy: 0.1674 - loss: 2.2969 - val_accuracy: 0.1335 - val_loss: 2.2970\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 422ms/step - accuracy: 0.1473 - loss: 2.2965 - val_accuracy: 0.1416 - val_loss: 2.2965\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 452ms/step - accuracy: 0.1514 - loss: 2.2961 - val_accuracy: 0.1057 - val_loss: 2.2961\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 430ms/step - accuracy: 0.1177 - loss: 2.2954 - val_accuracy: 0.1518 - val_loss: 2.2957\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 423ms/step - accuracy: 0.1922 - loss: 2.2953 - val_accuracy: 0.1477 - val_loss: 2.2952\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 449ms/step - accuracy: 0.1482 - loss: 2.2949 - val_accuracy: 0.1308 - val_loss: 2.2948\n",
            "\n",
            "Entrenamiento número 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 453ms/step - accuracy: 0.0994 - loss: 2.4716 - val_accuracy: 0.0982 - val_loss: 2.3816\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 425ms/step - accuracy: 0.0962 - loss: 2.3684 - val_accuracy: 0.0976 - val_loss: 2.3359\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 425ms/step - accuracy: 0.1009 - loss: 2.3313 - val_accuracy: 0.1009 - val_loss: 2.3165\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 431ms/step - accuracy: 0.1085 - loss: 2.3150 - val_accuracy: 0.1118 - val_loss: 2.3077\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 429ms/step - accuracy: 0.1203 - loss: 2.3068 - val_accuracy: 0.1538 - val_loss: 2.3034\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 429ms/step - accuracy: 0.1341 - loss: 2.3035 - val_accuracy: 0.1341 - val_loss: 2.3013\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 436ms/step - accuracy: 0.1358 - loss: 2.3016 - val_accuracy: 0.1463 - val_loss: 2.3001\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 428ms/step - accuracy: 0.1167 - loss: 2.3004 - val_accuracy: 0.1355 - val_loss: 2.2994\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 432ms/step - accuracy: 0.1422 - loss: 2.2995 - val_accuracy: 0.1430 - val_loss: 2.2988\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 436ms/step - accuracy: 0.1343 - loss: 2.2990 - val_accuracy: 0.2039 - val_loss: 2.2984\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 427ms/step - accuracy: 0.1771 - loss: 2.2986 - val_accuracy: 0.1965 - val_loss: 2.2980\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 428ms/step - accuracy: 0.1886 - loss: 2.2981 - val_accuracy: 0.1863 - val_loss: 2.2976\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.1673 - loss: 2.2977 - val_accuracy: 0.2249 - val_loss: 2.2972\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 438ms/step - accuracy: 0.2124 - loss: 2.2974 - val_accuracy: 0.1558 - val_loss: 2.2967\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 430ms/step - accuracy: 0.1398 - loss: 2.2967 - val_accuracy: 0.1443 - val_loss: 2.2963\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 429ms/step - accuracy: 0.1433 - loss: 2.2965 - val_accuracy: 0.1491 - val_loss: 2.2959\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 432ms/step - accuracy: 0.1611 - loss: 2.2958 - val_accuracy: 0.1612 - val_loss: 2.2955\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 449ms/step - accuracy: 0.1598 - loss: 2.2955 - val_accuracy: 0.1856 - val_loss: 2.2951\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 429ms/step - accuracy: 0.1776 - loss: 2.2952 - val_accuracy: 0.1985 - val_loss: 2.2946\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 426ms/step - accuracy: 0.1956 - loss: 2.2948 - val_accuracy: 0.1335 - val_loss: 2.2942\n",
            "\n",
            "Entrenamiento número 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_7', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 484ms/step - accuracy: 0.1012 - loss: 2.4345 - val_accuracy: 0.0996 - val_loss: 2.3843\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 441ms/step - accuracy: 0.0986 - loss: 2.3716 - val_accuracy: 0.0996 - val_loss: 2.3466\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 436ms/step - accuracy: 0.0977 - loss: 2.3443 - val_accuracy: 0.0989 - val_loss: 2.3260\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 433ms/step - accuracy: 0.1043 - loss: 2.3239 - val_accuracy: 0.1064 - val_loss: 2.3140\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 468ms/step - accuracy: 0.1050 - loss: 2.3123 - val_accuracy: 0.1077 - val_loss: 2.3078\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 439ms/step - accuracy: 0.1165 - loss: 2.3061 - val_accuracy: 0.1220 - val_loss: 2.3041\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 441ms/step - accuracy: 0.1356 - loss: 2.3037 - val_accuracy: 0.1165 - val_loss: 2.3023\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 446ms/step - accuracy: 0.1241 - loss: 2.3015 - val_accuracy: 0.1152 - val_loss: 2.3010\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.1250 - loss: 2.3005 - val_accuracy: 0.1531 - val_loss: 2.3004\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 446ms/step - accuracy: 0.1490 - loss: 2.3001 - val_accuracy: 0.1335 - val_loss: 2.3000\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 441ms/step - accuracy: 0.1469 - loss: 2.2996 - val_accuracy: 0.1070 - val_loss: 2.2997\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 443ms/step - accuracy: 0.1090 - loss: 2.2993 - val_accuracy: 0.1369 - val_loss: 2.2993\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 439ms/step - accuracy: 0.1434 - loss: 2.2988 - val_accuracy: 0.1260 - val_loss: 2.2989\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 441ms/step - accuracy: 0.1288 - loss: 2.2986 - val_accuracy: 0.1328 - val_loss: 2.2986\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 441ms/step - accuracy: 0.1354 - loss: 2.2981 - val_accuracy: 0.2154 - val_loss: 2.2983\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 448ms/step - accuracy: 0.2138 - loss: 2.2977 - val_accuracy: 0.1931 - val_loss: 2.2979\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 439ms/step - accuracy: 0.2000 - loss: 2.2978 - val_accuracy: 0.2046 - val_loss: 2.2976\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 443ms/step - accuracy: 0.2111 - loss: 2.2973 - val_accuracy: 0.1911 - val_loss: 2.2973\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.1853 - loss: 2.2969 - val_accuracy: 0.1741 - val_loss: 2.2969\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 440ms/step - accuracy: 0.1844 - loss: 2.2964 - val_accuracy: 0.2087 - val_loss: 2.2966\n",
            "\n",
            "Entrenamiento número 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_8', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 482ms/step - accuracy: 0.1026 - loss: 2.5686 - val_accuracy: 0.1016 - val_loss: 2.3756\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 464ms/step - accuracy: 0.1016 - loss: 2.3613 - val_accuracy: 0.1016 - val_loss: 2.3267\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 434ms/step - accuracy: 0.1008 - loss: 2.3221 - val_accuracy: 0.1016 - val_loss: 2.3115\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 442ms/step - accuracy: 0.1018 - loss: 2.3087 - val_accuracy: 0.1016 - val_loss: 2.3057\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.1021 - loss: 2.3053 - val_accuracy: 0.1016 - val_loss: 2.3030\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 442ms/step - accuracy: 0.1063 - loss: 2.3021 - val_accuracy: 0.1016 - val_loss: 2.3015\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 438ms/step - accuracy: 0.1005 - loss: 2.3013 - val_accuracy: 0.0996 - val_loss: 2.3008\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.1071 - loss: 2.3005 - val_accuracy: 0.1084 - val_loss: 2.3003\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.1069 - loss: 2.3004 - val_accuracy: 0.1416 - val_loss: 2.2999\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 458ms/step - accuracy: 0.1338 - loss: 2.2999 - val_accuracy: 0.1287 - val_loss: 2.2995\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 469ms/step - accuracy: 0.1286 - loss: 2.2993 - val_accuracy: 0.1511 - val_loss: 2.2992\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 463ms/step - accuracy: 0.1451 - loss: 2.2991 - val_accuracy: 0.1287 - val_loss: 2.2988\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 468ms/step - accuracy: 0.1260 - loss: 2.2985 - val_accuracy: 0.1247 - val_loss: 2.2985\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 0.1290 - loss: 2.2985 - val_accuracy: 0.1125 - val_loss: 2.2981\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 467ms/step - accuracy: 0.1243 - loss: 2.2981 - val_accuracy: 0.1314 - val_loss: 2.2977\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 464ms/step - accuracy: 0.1348 - loss: 2.2976 - val_accuracy: 0.1389 - val_loss: 2.2974\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 471ms/step - accuracy: 0.1360 - loss: 2.2970 - val_accuracy: 0.1355 - val_loss: 2.2970\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 473ms/step - accuracy: 0.1360 - loss: 2.2969 - val_accuracy: 0.1755 - val_loss: 2.2967\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.1407 - loss: 2.2966 - val_accuracy: 0.2066 - val_loss: 2.2963\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 474ms/step - accuracy: 0.1934 - loss: 2.2960 - val_accuracy: 0.1470 - val_loss: 2.2959\n",
            "\n",
            "Entrenamiento número 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_9', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 501ms/step - accuracy: 0.1004 - loss: 2.4242 - val_accuracy: 0.1016 - val_loss: 2.3608\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 452ms/step - accuracy: 0.1029 - loss: 2.3526 - val_accuracy: 0.1016 - val_loss: 2.3294\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 444ms/step - accuracy: 0.1003 - loss: 2.3274 - val_accuracy: 0.1016 - val_loss: 2.3142\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 441ms/step - accuracy: 0.1016 - loss: 2.3138 - val_accuracy: 0.1023 - val_loss: 2.3062\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 446ms/step - accuracy: 0.1069 - loss: 2.3075 - val_accuracy: 0.1938 - val_loss: 2.3023\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.1791 - loss: 2.3013 - val_accuracy: 0.1579 - val_loss: 2.3003\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 442ms/step - accuracy: 0.1643 - loss: 2.2998 - val_accuracy: 0.1477 - val_loss: 2.2990\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 440ms/step - accuracy: 0.1709 - loss: 2.2987 - val_accuracy: 0.2209 - val_loss: 2.2981\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 439ms/step - accuracy: 0.1799 - loss: 2.2983 - val_accuracy: 0.2019 - val_loss: 2.2975\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 445ms/step - accuracy: 0.1757 - loss: 2.2974 - val_accuracy: 0.1944 - val_loss: 2.2968\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 449ms/step - accuracy: 0.1829 - loss: 2.2969 - val_accuracy: 0.1843 - val_loss: 2.2963\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 439ms/step - accuracy: 0.1900 - loss: 2.2962 - val_accuracy: 0.1829 - val_loss: 2.2958\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 443ms/step - accuracy: 0.1695 - loss: 2.2957 - val_accuracy: 0.1673 - val_loss: 2.2952\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 446ms/step - accuracy: 0.1605 - loss: 2.2949 - val_accuracy: 0.1619 - val_loss: 2.2947\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 448ms/step - accuracy: 0.1673 - loss: 2.2946 - val_accuracy: 0.1728 - val_loss: 2.2942\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 446ms/step - accuracy: 0.1932 - loss: 2.2936 - val_accuracy: 0.1768 - val_loss: 2.2936\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 442ms/step - accuracy: 0.1876 - loss: 2.2934 - val_accuracy: 0.2663 - val_loss: 2.2931\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 437ms/step - accuracy: 0.2268 - loss: 2.2929 - val_accuracy: 0.2832 - val_loss: 2.2925\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 444ms/step - accuracy: 0.2756 - loss: 2.2919 - val_accuracy: 0.1782 - val_loss: 2.2919\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 435ms/step - accuracy: 0.2048 - loss: 2.2915 - val_accuracy: 0.1673 - val_loss: 2.2914\n"
          ]
        }
      ],
      "source": [
        "# Modelo 2 - QuickDraw-10\n",
        "# Detalles:\n",
        "# Activacion: sigmoid\n",
        "# Perdida: CrossEntropy\n",
        "# Capas: 3\n",
        "# Epocas: 20\n",
        "# Batch size: 1000\n",
        "\n",
        "acc_total_2, acc_clase_2, cm_2 = experiment(\n",
        "        train_images_10,\n",
        "        train_labels_10,\n",
        "        val_image_10,\n",
        "        val_labels_10,\n",
        "        test_images_10,\n",
        "        test_labels_10,\n",
        "        layers_size=[512, 256, 128],\n",
        "        n_classes=10,\n",
        "        activation='sigmoid',\n",
        "        loss_fn='categorical_crossentropy',\n",
        "        class_names=clases,\n",
        "        epochs=20,\n",
        "        batch_size=1000,\n",
        "        n_experiments=5\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'bandage': 0.16666666666666666,\n",
              "  'blackberry': 0.0,\n",
              "  'castle': 0.0,\n",
              "  'flashlight': 0.0,\n",
              "  'lion': 1.0,\n",
              "  'remote-control': 0.11650485436893204,\n",
              "  'sink': 0.0,\n",
              "  'spreadsheet': 0.0,\n",
              "  'teapot': 0.0,\n",
              "  'trombone': 0.0},\n",
              " {'bandage': 0.0,\n",
              "  'blackberry': 0.00819672131147541,\n",
              "  'castle': 0.0,\n",
              "  'flashlight': 0.0,\n",
              "  'lion': 0.0,\n",
              "  'remote-control': 0.04854368932038835,\n",
              "  'sink': 0.04310344827586207,\n",
              "  'spreadsheet': 0.1322314049586777,\n",
              "  'teapot': 0.06153846153846154,\n",
              "  'trombone': 0.9607843137254902},\n",
              " {'bandage': 0.008333333333333333,\n",
              "  'blackberry': 0.04918032786885246,\n",
              "  'castle': 0.0,\n",
              "  'flashlight': 0.0,\n",
              "  'lion': 0.5904761904761905,\n",
              "  'remote-control': 0.6893203883495146,\n",
              "  'sink': 0.07758620689655173,\n",
              "  'spreadsheet': 0.32231404958677684,\n",
              "  'teapot': 0.18461538461538463,\n",
              "  'trombone': 0.09803921568627451},\n",
              " {'bandage': 0.075,\n",
              "  'blackberry': 0.0,\n",
              "  'castle': 0.0,\n",
              "  'flashlight': 0.07758620689655173,\n",
              "  'lion': 0.24761904761904763,\n",
              "  'remote-control': 0.14563106796116504,\n",
              "  'sink': 0.0,\n",
              "  'spreadsheet': 0.008264462809917356,\n",
              "  'teapot': 0.0,\n",
              "  'trombone': 0.8823529411764706},\n",
              " {'bandage': 0.0,\n",
              "  'blackberry': 0.0,\n",
              "  'castle': 0.0,\n",
              "  'flashlight': 0.0,\n",
              "  'lion': 1.0,\n",
              "  'remote-control': 0.6796116504854369,\n",
              "  'sink': 0.0,\n",
              "  'spreadsheet': 0.0,\n",
              "  'teapot': 0.038461538461538464,\n",
              "  'trombone': 0.0}]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_clase_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.11749571183533447,\n",
              " 0.11406518010291596,\n",
              " 0.19039451114922812,\n",
              " 0.12864493996569468,\n",
              " 0.15437392795883362]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_total_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[ 20,   0,   0,   0,  99,   1,   0,   0,   0,   0],\n",
              "        [ 12,   0,   0,   1, 108,   0,   0,   0,   0,   1],\n",
              "        [  8,   0,   0,   0, 121,   2,   0,   0,   0,   0],\n",
              "        [  9,   0,   0,   0, 106,   0,   0,   0,   0,   1],\n",
              "        [  0,   0,   0,   0, 105,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  91,  12,   0,   0,   0,   0],\n",
              "        [  6,   0,   0,   0, 106,   4,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 120,   1,   0,   0,   0,   0],\n",
              "        [  3,   0,   0,   0, 127,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 102,   0,   0,   0,   0,   0]]),\n",
              " array([[  0,   0,   0,   0,   0,   0,   1,   6,   0, 113],\n",
              "        [  0,   1,   0,   0,   0,   0,   0,   5,   1, 115],\n",
              "        [  2,   0,   0,   0,   0,   0,   3,   2,   0, 124],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   1,   0, 115],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   9,   0,  96],\n",
              "        [  2,   0,   0,   0,   0,   5,   8,   0,   0,  88],\n",
              "        [  0,   0,   0,   0,   0,   1,   5,   1,   1, 108],\n",
              "        [  0,   0,   0,   0,   0,   0,   2,  16,   0, 103],\n",
              "        [  0,   0,   0,   0,   1,   0,   0,  22,   8,  99],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   4,   0,  98]]),\n",
              " array([[ 1,  3,  0,  0, 61, 34,  3,  5,  1, 12],\n",
              "        [ 0,  6,  0,  0, 64,  8,  6, 19,  1, 18],\n",
              "        [ 0,  1,  0,  0, 66, 40,  4,  8,  0, 12],\n",
              "        [ 0,  0,  0,  0, 81,  0,  5, 21,  0,  9],\n",
              "        [ 0,  0,  0,  0, 62, 11,  0, 20,  0, 12],\n",
              "        [ 0,  0,  0,  0, 23, 71,  0,  5,  1,  3],\n",
              "        [ 0,  5,  0,  0, 31, 62,  9,  3,  1,  5],\n",
              "        [ 0,  0,  0,  0, 54, 18,  0, 39,  0, 10],\n",
              "        [ 0,  0,  0,  0, 65, 16,  0, 23, 24,  2],\n",
              "        [ 0,  0,  0,  0, 75,  8,  2,  7,  0, 10]]),\n",
              " array([[  9,   0,   0,   1,   8,   3,   0,   3,   0,  96],\n",
              "        [  9,   0,   0,   0,  28,   0,   0,   1,   0,  84],\n",
              "        [  2,   0,   0,   0,   4,   0,   0,   0,   0, 125],\n",
              "        [  3,   0,   0,   9,  18,   0,   0,   0,   0,  86],\n",
              "        [  0,   0,   0,   0,  26,   1,   0,   6,   0,  72],\n",
              "        [  0,   0,   0,   0,   4,  15,   0,   0,   0,  84],\n",
              "        [  3,   0,   0,   1,   0,   2,   0,   0,   0, 110],\n",
              "        [  0,   0,   0,   0,  10,   0,   0,   1,   0, 110],\n",
              "        [  4,   0,   0,   0,  39,   2,   0,   0,   0,  85],\n",
              "        [  0,   0,   0,   1,  11,   0,   0,   0,   0,  90]]),\n",
              " array([[  0,   0,   0,   0, 111,   8,   0,   0,   1,   0],\n",
              "        [  0,   0,   0,   0, 120,   2,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 117,  14,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 116,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 105,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  32,  70,   0,   0,   0,   1],\n",
              "        [  0,   0,   0,   0,  75,  40,   0,   0,   0,   1],\n",
              "        [  0,   0,   0,   0, 115,   6,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 117,   8,   0,   0,   5,   0],\n",
              "        [  0,   0,   0,   0,  98,   4,   0,   0,   0,   0]])]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tRMvGYDiEGz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenamiento número 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_10', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 318ms/step - accuracy: 0.0924 - loss: 1.4827 - val_accuracy: 0.0962 - val_loss: 1.1270\n",
            "Epoch 2/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.1015 - loss: 1.1392 - val_accuracy: 0.1091 - val_loss: 1.0366\n",
            "Epoch 3/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.1066 - loss: 1.0329 - val_accuracy: 0.1253 - val_loss: 1.0164\n",
            "Epoch 4/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.1006 - loss: 1.0207 - val_accuracy: 0.1030 - val_loss: 1.0156\n",
            "Epoch 5/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.1150 - loss: 1.0117 - val_accuracy: 0.1321 - val_loss: 1.0050\n",
            "Epoch 6/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.1291 - loss: 1.0058 - val_accuracy: 0.1240 - val_loss: 1.0068\n",
            "Epoch 7/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.1480 - loss: 1.0049 - val_accuracy: 0.1558 - val_loss: 1.0034\n",
            "Epoch 8/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.1593 - loss: 1.0036 - val_accuracy: 0.1524 - val_loss: 1.0042\n",
            "Epoch 9/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.1609 - loss: 1.0037 - val_accuracy: 0.1457 - val_loss: 1.0045\n",
            "Epoch 10/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.1578 - loss: 1.0038 - val_accuracy: 0.1762 - val_loss: 1.0027\n",
            "Epoch 11/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.1813 - loss: 1.0024 - val_accuracy: 0.1687 - val_loss: 1.0034\n",
            "Epoch 12/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.1883 - loss: 1.0025 - val_accuracy: 0.1640 - val_loss: 1.0038\n",
            "Epoch 13/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.1930 - loss: 1.0024 - val_accuracy: 0.1978 - val_loss: 1.0024\n",
            "Epoch 14/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.2032 - loss: 1.0025 - val_accuracy: 0.1951 - val_loss: 1.0025\n",
            "Epoch 15/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.2053 - loss: 1.0021 - val_accuracy: 0.1538 - val_loss: 1.0039\n",
            "Epoch 16/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.2199 - loss: 1.0021 - val_accuracy: 0.1924 - val_loss: 1.0025\n",
            "Epoch 17/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2126 - loss: 1.0020 - val_accuracy: 0.2080 - val_loss: 1.0021\n",
            "Epoch 18/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2197 - loss: 1.0018 - val_accuracy: 0.2046 - val_loss: 1.0017\n",
            "Epoch 19/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2368 - loss: 1.0016 - val_accuracy: 0.2480 - val_loss: 1.0018\n",
            "Epoch 20/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2473 - loss: 1.0014 - val_accuracy: 0.1802 - val_loss: 1.0023\n",
            "Epoch 21/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2178 - loss: 1.0017 - val_accuracy: 0.2534 - val_loss: 1.0014\n",
            "Epoch 22/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2494 - loss: 1.0013 - val_accuracy: 0.2459 - val_loss: 1.0015\n",
            "Epoch 23/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.2458 - loss: 1.0013 - val_accuracy: 0.2358 - val_loss: 1.0016\n",
            "Epoch 24/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.2636 - loss: 1.0011 - val_accuracy: 0.2317 - val_loss: 1.0020\n",
            "Epoch 25/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2575 - loss: 1.0013 - val_accuracy: 0.2568 - val_loss: 1.0013\n",
            "Epoch 26/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2727 - loss: 1.0010 - val_accuracy: 0.2419 - val_loss: 1.0018\n",
            "Epoch 27/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.2766 - loss: 1.0010 - val_accuracy: 0.2608 - val_loss: 1.0012\n",
            "Epoch 28/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.2746 - loss: 1.0009 - val_accuracy: 0.2791 - val_loss: 1.0013\n",
            "Epoch 29/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3048 - loss: 1.0005 - val_accuracy: 0.2581 - val_loss: 1.0013\n",
            "Epoch 30/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2920 - loss: 1.0007 - val_accuracy: 0.2805 - val_loss: 1.0007\n",
            "Epoch 31/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.3031 - loss: 1.0004 - val_accuracy: 0.2337 - val_loss: 1.0020\n",
            "Epoch 32/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.2793 - loss: 1.0008 - val_accuracy: 0.2480 - val_loss: 1.0017\n",
            "Epoch 33/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.2793 - loss: 1.0008 - val_accuracy: 0.2588 - val_loss: 1.0012\n",
            "Epoch 34/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2968 - loss: 1.0004 - val_accuracy: 0.2812 - val_loss: 1.0008\n",
            "Epoch 35/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.3141 - loss: 1.0000 - val_accuracy: 0.2846 - val_loss: 1.0007\n",
            "Epoch 36/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.3259 - loss: 1.0000 - val_accuracy: 0.2453 - val_loss: 1.0016\n",
            "Epoch 37/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.2947 - loss: 1.0004 - val_accuracy: 0.2818 - val_loss: 1.0008\n",
            "Epoch 38/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.3025 - loss: 1.0004 - val_accuracy: 0.3211 - val_loss: 1.0004\n",
            "Epoch 39/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.3276 - loss: 0.9996 - val_accuracy: 0.2791 - val_loss: 1.0012\n",
            "Epoch 40/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.3102 - loss: 1.0000 - val_accuracy: 0.2771 - val_loss: 1.0006\n",
            "Epoch 41/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.3225 - loss: 0.9998 - val_accuracy: 0.2846 - val_loss: 1.0006\n",
            "Epoch 42/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.3179 - loss: 0.9997 - val_accuracy: 0.3144 - val_loss: 1.0005\n",
            "Epoch 43/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3347 - loss: 0.9997 - val_accuracy: 0.2907 - val_loss: 1.0001\n",
            "Epoch 44/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.3325 - loss: 0.9993 - val_accuracy: 0.3178 - val_loss: 0.9999\n",
            "Epoch 45/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3430 - loss: 0.9994 - val_accuracy: 0.2575 - val_loss: 1.0008\n",
            "Epoch 46/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.3181 - loss: 0.9994 - val_accuracy: 0.2825 - val_loss: 1.0002\n",
            "Epoch 47/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3175 - loss: 0.9987 - val_accuracy: 0.2717 - val_loss: 1.0001\n",
            "Epoch 48/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.3348 - loss: 0.9986 - val_accuracy: 0.3469 - val_loss: 0.9991\n",
            "Epoch 49/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3483 - loss: 0.9979 - val_accuracy: 0.3211 - val_loss: 0.9993\n",
            "Epoch 50/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.3280 - loss: 0.9987 - val_accuracy: 0.2995 - val_loss: 0.9993\n",
            "Epoch 51/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.3229 - loss: 0.9983 - val_accuracy: 0.3252 - val_loss: 0.9988\n",
            "Epoch 52/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.3346 - loss: 0.9976 - val_accuracy: 0.2873 - val_loss: 0.9989\n",
            "Epoch 53/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.3268 - loss: 0.9974 - val_accuracy: 0.3245 - val_loss: 0.9982\n",
            "Epoch 54/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3369 - loss: 0.9968 - val_accuracy: 0.2954 - val_loss: 0.9986\n",
            "Epoch 55/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.3218 - loss: 0.9967 - val_accuracy: 0.2907 - val_loss: 0.9982\n",
            "Epoch 56/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.3307 - loss: 0.9957 - val_accuracy: 0.3198 - val_loss: 0.9977\n",
            "Epoch 57/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3180 - loss: 0.9957 - val_accuracy: 0.2541 - val_loss: 1.0001\n",
            "Epoch 58/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.2952 - loss: 0.9950 - val_accuracy: 0.2927 - val_loss: 0.9976\n",
            "Epoch 59/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3170 - loss: 0.9943 - val_accuracy: 0.2778 - val_loss: 0.9968\n",
            "Epoch 60/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2959 - loss: 0.9940 - val_accuracy: 0.2656 - val_loss: 0.9955\n",
            "\n",
            "Entrenamiento número 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_11', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 338ms/step - accuracy: 0.1031 - loss: 1.3406 - val_accuracy: 0.0982 - val_loss: 1.2591\n",
            "Epoch 2/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.0936 - loss: 1.1180 - val_accuracy: 0.1037 - val_loss: 1.0363\n",
            "Epoch 3/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.0987 - loss: 1.0278 - val_accuracy: 0.1240 - val_loss: 1.0128\n",
            "Epoch 4/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.1198 - loss: 1.0154 - val_accuracy: 0.1131 - val_loss: 1.0177\n",
            "Epoch 5/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.1219 - loss: 1.0132 - val_accuracy: 0.1253 - val_loss: 1.0120\n",
            "Epoch 6/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 309ms/step - accuracy: 0.1233 - loss: 1.0099 - val_accuracy: 0.1240 - val_loss: 1.0064\n",
            "Epoch 7/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 309ms/step - accuracy: 0.1278 - loss: 1.0081 - val_accuracy: 0.1389 - val_loss: 1.0070\n",
            "Epoch 8/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.1485 - loss: 1.0057 - val_accuracy: 0.1409 - val_loss: 1.0054\n",
            "Epoch 9/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.1368 - loss: 1.0051 - val_accuracy: 0.1531 - val_loss: 1.0038\n",
            "Epoch 10/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.1547 - loss: 1.0041 - val_accuracy: 0.1579 - val_loss: 1.0049\n",
            "Epoch 11/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.1559 - loss: 1.0039 - val_accuracy: 0.1362 - val_loss: 1.0037\n",
            "Epoch 12/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.1587 - loss: 1.0034 - val_accuracy: 0.1436 - val_loss: 1.0030\n",
            "Epoch 13/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.1657 - loss: 1.0027 - val_accuracy: 0.1538 - val_loss: 1.0027\n",
            "Epoch 14/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.1650 - loss: 1.0024 - val_accuracy: 0.1402 - val_loss: 1.0024\n",
            "Epoch 15/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.1752 - loss: 1.0021 - val_accuracy: 0.1748 - val_loss: 1.0029\n",
            "Epoch 16/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1888 - loss: 1.0020 - val_accuracy: 0.1470 - val_loss: 1.0018\n",
            "Epoch 17/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.1878 - loss: 1.0017 - val_accuracy: 0.1870 - val_loss: 1.0014\n",
            "Epoch 18/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.1913 - loss: 1.0017 - val_accuracy: 0.1789 - val_loss: 1.0014\n",
            "Epoch 19/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.1989 - loss: 1.0016 - val_accuracy: 0.2005 - val_loss: 1.0014\n",
            "Epoch 20/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 331ms/step - accuracy: 0.2058 - loss: 1.0016 - val_accuracy: 0.1592 - val_loss: 1.0020\n",
            "Epoch 21/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 308ms/step - accuracy: 0.1975 - loss: 1.0016 - val_accuracy: 0.1667 - val_loss: 1.0018\n",
            "Epoch 22/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.2002 - loss: 1.0016 - val_accuracy: 0.1673 - val_loss: 1.0016\n",
            "Epoch 23/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2225 - loss: 1.0013 - val_accuracy: 0.1999 - val_loss: 1.0014\n",
            "Epoch 24/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2212 - loss: 1.0014 - val_accuracy: 0.1958 - val_loss: 1.0013\n",
            "Epoch 25/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.2361 - loss: 1.0012 - val_accuracy: 0.1687 - val_loss: 1.0018\n",
            "Epoch 26/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2385 - loss: 1.0012 - val_accuracy: 0.2005 - val_loss: 1.0014\n",
            "Epoch 27/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2446 - loss: 1.0011 - val_accuracy: 0.1829 - val_loss: 1.0017\n",
            "Epoch 28/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2316 - loss: 1.0012 - val_accuracy: 0.2039 - val_loss: 1.0014\n",
            "Epoch 29/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2373 - loss: 1.0011 - val_accuracy: 0.1958 - val_loss: 1.0013\n",
            "Epoch 30/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.2526 - loss: 1.0011 - val_accuracy: 0.1999 - val_loss: 1.0014\n",
            "Epoch 31/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2420 - loss: 1.0012 - val_accuracy: 0.2297 - val_loss: 1.0015\n",
            "Epoch 32/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2537 - loss: 1.0010 - val_accuracy: 0.2446 - val_loss: 1.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2454 - loss: 1.0010 - val_accuracy: 0.1944 - val_loss: 1.0014\n",
            "Epoch 34/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2628 - loss: 1.0010 - val_accuracy: 0.2608 - val_loss: 1.0009\n",
            "Epoch 35/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.2747 - loss: 1.0008 - val_accuracy: 0.2419 - val_loss: 1.0009\n",
            "Epoch 36/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2666 - loss: 1.0008 - val_accuracy: 0.1972 - val_loss: 1.0015\n",
            "Epoch 37/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2416 - loss: 1.0011 - val_accuracy: 0.2459 - val_loss: 1.0010\n",
            "Epoch 38/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.2679 - loss: 1.0008 - val_accuracy: 0.2608 - val_loss: 1.0009\n",
            "Epoch 39/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 323ms/step - accuracy: 0.2775 - loss: 1.0007 - val_accuracy: 0.2358 - val_loss: 1.0009\n",
            "Epoch 40/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2587 - loss: 1.0008 - val_accuracy: 0.2676 - val_loss: 1.0009\n",
            "Epoch 41/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2768 - loss: 1.0007 - val_accuracy: 0.2392 - val_loss: 1.0010\n",
            "Epoch 42/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.2722 - loss: 1.0006 - val_accuracy: 0.2337 - val_loss: 1.0008\n",
            "Epoch 43/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2727 - loss: 1.0006 - val_accuracy: 0.2398 - val_loss: 1.0012\n",
            "Epoch 44/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2912 - loss: 1.0003 - val_accuracy: 0.2561 - val_loss: 1.0011\n",
            "Epoch 45/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.2860 - loss: 1.0004 - val_accuracy: 0.2940 - val_loss: 1.0006\n",
            "Epoch 46/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.2926 - loss: 1.0001 - val_accuracy: 0.2466 - val_loss: 1.0014\n",
            "Epoch 47/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2673 - loss: 1.0005 - val_accuracy: 0.2588 - val_loss: 1.0007\n",
            "Epoch 48/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2783 - loss: 1.0004 - val_accuracy: 0.2927 - val_loss: 1.0005\n",
            "Epoch 49/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3046 - loss: 0.9999 - val_accuracy: 0.2696 - val_loss: 1.0005\n",
            "Epoch 50/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.2968 - loss: 0.9998 - val_accuracy: 0.2581 - val_loss: 1.0009\n",
            "Epoch 51/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2828 - loss: 1.0002 - val_accuracy: 0.2737 - val_loss: 1.0006\n",
            "Epoch 52/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2934 - loss: 0.9997 - val_accuracy: 0.3042 - val_loss: 1.0001\n",
            "Epoch 53/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.3059 - loss: 0.9998 - val_accuracy: 0.3056 - val_loss: 1.0001\n",
            "Epoch 54/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.3034 - loss: 0.9995 - val_accuracy: 0.2866 - val_loss: 1.0001\n",
            "Epoch 55/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.3015 - loss: 0.9994 - val_accuracy: 0.2270 - val_loss: 1.0010\n",
            "Epoch 56/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 307ms/step - accuracy: 0.2877 - loss: 0.9998 - val_accuracy: 0.3171 - val_loss: 0.9996\n",
            "Epoch 57/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.3137 - loss: 0.9991 - val_accuracy: 0.2663 - val_loss: 0.9999\n",
            "Epoch 58/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2933 - loss: 0.9990 - val_accuracy: 0.2839 - val_loss: 0.9998\n",
            "Epoch 59/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2897 - loss: 0.9989 - val_accuracy: 0.2636 - val_loss: 0.9998\n",
            "Epoch 60/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2968 - loss: 0.9988 - val_accuracy: 0.2900 - val_loss: 0.9992\n",
            "\n",
            "Entrenamiento número 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 329ms/step - accuracy: 0.0987 - loss: 1.2833 - val_accuracy: 0.0982 - val_loss: 1.1913\n",
            "Epoch 2/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 306ms/step - accuracy: 0.0986 - loss: 1.0772 - val_accuracy: 0.1037 - val_loss: 1.0343\n",
            "Epoch 3/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1085 - loss: 1.0214 - val_accuracy: 0.1070 - val_loss: 1.0212\n",
            "Epoch 4/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.0970 - loss: 1.0200 - val_accuracy: 0.1064 - val_loss: 1.0091\n",
            "Epoch 5/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.1049 - loss: 1.0099 - val_accuracy: 0.1206 - val_loss: 1.0088\n",
            "Epoch 6/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.1343 - loss: 1.0073 - val_accuracy: 0.1538 - val_loss: 1.0064\n",
            "Epoch 7/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.1374 - loss: 1.0063 - val_accuracy: 0.1511 - val_loss: 1.0058\n",
            "Epoch 8/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1440 - loss: 1.0060 - val_accuracy: 0.1369 - val_loss: 1.0057\n",
            "Epoch 9/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.1525 - loss: 1.0059 - val_accuracy: 0.1511 - val_loss: 1.0050\n",
            "Epoch 10/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1573 - loss: 1.0040 - val_accuracy: 0.1233 - val_loss: 1.0048\n",
            "Epoch 11/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.1557 - loss: 1.0052 - val_accuracy: 0.2053 - val_loss: 1.0044\n",
            "Epoch 12/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.1986 - loss: 1.0033 - val_accuracy: 0.2168 - val_loss: 1.0035\n",
            "Epoch 13/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.1994 - loss: 1.0030 - val_accuracy: 0.2195 - val_loss: 1.0028\n",
            "Epoch 14/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.1998 - loss: 1.0032 - val_accuracy: 0.1992 - val_loss: 1.0025\n",
            "Epoch 15/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.2233 - loss: 1.0026 - val_accuracy: 0.2331 - val_loss: 1.0028\n",
            "Epoch 16/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.2283 - loss: 1.0024 - val_accuracy: 0.1883 - val_loss: 1.0042\n",
            "Epoch 17/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.2307 - loss: 1.0025 - val_accuracy: 0.2141 - val_loss: 1.0022\n",
            "Epoch 18/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2394 - loss: 1.0022 - val_accuracy: 0.1877 - val_loss: 1.0045\n",
            "Epoch 19/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2313 - loss: 1.0027 - val_accuracy: 0.2005 - val_loss: 1.0041\n",
            "Epoch 20/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2235 - loss: 1.0029 - val_accuracy: 0.2425 - val_loss: 1.0034\n",
            "Epoch 21/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2413 - loss: 1.0025 - val_accuracy: 0.2656 - val_loss: 1.0020\n",
            "Epoch 22/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2552 - loss: 1.0021 - val_accuracy: 0.2588 - val_loss: 1.0020\n",
            "Epoch 23/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2723 - loss: 1.0016 - val_accuracy: 0.2331 - val_loss: 1.0025\n",
            "Epoch 24/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2438 - loss: 1.0023 - val_accuracy: 0.2575 - val_loss: 1.0019\n",
            "Epoch 25/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2669 - loss: 1.0018 - val_accuracy: 0.2608 - val_loss: 1.0019\n",
            "Epoch 26/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2880 - loss: 1.0015 - val_accuracy: 0.2683 - val_loss: 1.0016\n",
            "Epoch 27/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2634 - loss: 1.0015 - val_accuracy: 0.2629 - val_loss: 1.0023\n",
            "Epoch 28/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.2803 - loss: 1.0017 - val_accuracy: 0.2730 - val_loss: 1.0012\n",
            "Epoch 29/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.2903 - loss: 1.0015 - val_accuracy: 0.2730 - val_loss: 1.0023\n",
            "Epoch 30/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2798 - loss: 1.0017 - val_accuracy: 0.2676 - val_loss: 1.0021\n",
            "Epoch 31/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 308ms/step - accuracy: 0.2896 - loss: 1.0015 - val_accuracy: 0.2466 - val_loss: 1.0030\n",
            "Epoch 32/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.2912 - loss: 1.0018 - val_accuracy: 0.2656 - val_loss: 1.0016\n",
            "Epoch 33/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3101 - loss: 1.0010 - val_accuracy: 0.2927 - val_loss: 1.0019\n",
            "Epoch 34/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.3008 - loss: 1.0012 - val_accuracy: 0.2846 - val_loss: 1.0018\n",
            "Epoch 35/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3008 - loss: 1.0011 - val_accuracy: 0.3110 - val_loss: 1.0011\n",
            "Epoch 36/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.3083 - loss: 1.0010 - val_accuracy: 0.2866 - val_loss: 1.0013\n",
            "Epoch 37/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3239 - loss: 1.0008 - val_accuracy: 0.3042 - val_loss: 1.0014\n",
            "Epoch 38/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 315ms/step - accuracy: 0.3080 - loss: 1.0010 - val_accuracy: 0.2818 - val_loss: 1.0015\n",
            "Epoch 39/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 317ms/step - accuracy: 0.3143 - loss: 1.0009 - val_accuracy: 0.3096 - val_loss: 1.0016\n",
            "Epoch 40/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 323ms/step - accuracy: 0.3290 - loss: 1.0008 - val_accuracy: 0.3062 - val_loss: 1.0014\n",
            "Epoch 41/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317ms/step - accuracy: 0.3374 - loss: 1.0007 - val_accuracy: 0.3313 - val_loss: 1.0009\n",
            "Epoch 42/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.3282 - loss: 1.0005 - val_accuracy: 0.2791 - val_loss: 1.0011\n",
            "Epoch 43/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3243 - loss: 1.0006 - val_accuracy: 0.2839 - val_loss: 1.0011\n",
            "Epoch 44/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3222 - loss: 1.0007 - val_accuracy: 0.3022 - val_loss: 1.0009\n",
            "Epoch 45/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.3367 - loss: 1.0004 - val_accuracy: 0.3191 - val_loss: 1.0013\n",
            "Epoch 46/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3397 - loss: 1.0002 - val_accuracy: 0.2934 - val_loss: 1.0011\n",
            "Epoch 47/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3426 - loss: 1.0001 - val_accuracy: 0.3069 - val_loss: 1.0011\n",
            "Epoch 48/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.3436 - loss: 1.0001 - val_accuracy: 0.2934 - val_loss: 1.0007\n",
            "Epoch 49/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3524 - loss: 0.9998 - val_accuracy: 0.3130 - val_loss: 1.0008\n",
            "Epoch 50/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3556 - loss: 0.9997 - val_accuracy: 0.3123 - val_loss: 1.0007\n",
            "Epoch 51/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3490 - loss: 0.9998 - val_accuracy: 0.2961 - val_loss: 1.0007\n",
            "Epoch 52/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3453 - loss: 0.9996 - val_accuracy: 0.3211 - val_loss: 1.0008\n",
            "Epoch 53/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3447 - loss: 0.9996 - val_accuracy: 0.2696 - val_loss: 1.0014\n",
            "Epoch 54/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3332 - loss: 0.9996 - val_accuracy: 0.3428 - val_loss: 0.9998\n",
            "Epoch 55/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3569 - loss: 0.9992 - val_accuracy: 0.3428 - val_loss: 1.0001\n",
            "Epoch 56/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3599 - loss: 0.9994 - val_accuracy: 0.3604 - val_loss: 0.9998\n",
            "Epoch 57/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3657 - loss: 0.9990 - val_accuracy: 0.3218 - val_loss: 0.9997\n",
            "Epoch 58/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3393 - loss: 0.9990 - val_accuracy: 0.3191 - val_loss: 1.0005\n",
            "Epoch 59/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.3519 - loss: 0.9992 - val_accuracy: 0.3232 - val_loss: 0.9999\n",
            "Epoch 60/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.3650 - loss: 0.9981 - val_accuracy: 0.2636 - val_loss: 1.0007\n",
            "\n",
            "Entrenamiento número 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318ms/step - accuracy: 0.0974 - loss: 1.1894 - val_accuracy: 0.0928 - val_loss: 1.1350\n",
            "Epoch 2/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.0929 - loss: 1.1191 - val_accuracy: 0.0989 - val_loss: 1.0290\n",
            "Epoch 3/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.1024 - loss: 1.0262 - val_accuracy: 0.1043 - val_loss: 1.0142\n",
            "Epoch 4/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.0989 - loss: 1.0162 - val_accuracy: 0.1043 - val_loss: 1.0088\n",
            "Epoch 5/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.1043 - loss: 1.0110 - val_accuracy: 0.0996 - val_loss: 1.0086\n",
            "Epoch 6/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.1127 - loss: 1.0106 - val_accuracy: 0.1084 - val_loss: 1.0072\n",
            "Epoch 7/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.1252 - loss: 1.0059 - val_accuracy: 0.1009 - val_loss: 1.0124\n",
            "Epoch 8/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1235 - loss: 1.0071 - val_accuracy: 0.1037 - val_loss: 1.0059\n",
            "Epoch 9/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1352 - loss: 1.0043 - val_accuracy: 0.1619 - val_loss: 1.0039\n",
            "Epoch 10/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.1452 - loss: 1.0038 - val_accuracy: 0.1551 - val_loss: 1.0023\n",
            "Epoch 11/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.1747 - loss: 1.0025 - val_accuracy: 0.1680 - val_loss: 1.0035\n",
            "Epoch 12/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1791 - loss: 1.0029 - val_accuracy: 0.1477 - val_loss: 1.0027\n",
            "Epoch 13/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1789 - loss: 1.0024 - val_accuracy: 0.1328 - val_loss: 1.0025\n",
            "Epoch 14/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1692 - loss: 1.0025 - val_accuracy: 0.1416 - val_loss: 1.0021\n",
            "Epoch 15/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1925 - loss: 1.0020 - val_accuracy: 0.1762 - val_loss: 1.0018\n",
            "Epoch 16/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.2113 - loss: 1.0017 - val_accuracy: 0.1741 - val_loss: 1.0028\n",
            "Epoch 17/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.1845 - loss: 1.0024 - val_accuracy: 0.2202 - val_loss: 1.0016\n",
            "Epoch 18/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2139 - loss: 1.0018 - val_accuracy: 0.1572 - val_loss: 1.0022\n",
            "Epoch 19/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2175 - loss: 1.0017 - val_accuracy: 0.1545 - val_loss: 1.0022\n",
            "Epoch 20/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.2224 - loss: 1.0017 - val_accuracy: 0.2188 - val_loss: 1.0015\n",
            "Epoch 21/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.2368 - loss: 1.0015 - val_accuracy: 0.1775 - val_loss: 1.0022\n",
            "Epoch 22/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2315 - loss: 1.0015 - val_accuracy: 0.1965 - val_loss: 1.0029\n",
            "Epoch 23/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2138 - loss: 1.0020 - val_accuracy: 0.2310 - val_loss: 1.0015\n",
            "Epoch 24/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2336 - loss: 1.0015 - val_accuracy: 0.1965 - val_loss: 1.0015\n",
            "Epoch 25/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2449 - loss: 1.0013 - val_accuracy: 0.2188 - val_loss: 1.0017\n",
            "Epoch 26/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2439 - loss: 1.0013 - val_accuracy: 0.2107 - val_loss: 1.0024\n",
            "Epoch 27/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2435 - loss: 1.0016 - val_accuracy: 0.2331 - val_loss: 1.0017\n",
            "Epoch 28/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2576 - loss: 1.0012 - val_accuracy: 0.2243 - val_loss: 1.0014\n",
            "Epoch 29/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.2794 - loss: 1.0010 - val_accuracy: 0.2188 - val_loss: 1.0016\n",
            "Epoch 30/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2665 - loss: 1.0011 - val_accuracy: 0.2398 - val_loss: 1.0012\n",
            "Epoch 31/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.2780 - loss: 1.0010 - val_accuracy: 0.2188 - val_loss: 1.0015\n",
            "Epoch 32/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2678 - loss: 1.0009 - val_accuracy: 0.2575 - val_loss: 1.0012\n",
            "Epoch 33/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2839 - loss: 1.0008 - val_accuracy: 0.2364 - val_loss: 1.0013\n",
            "Epoch 34/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2606 - loss: 1.0012 - val_accuracy: 0.2480 - val_loss: 1.0012\n",
            "Epoch 35/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2959 - loss: 1.0007 - val_accuracy: 0.2486 - val_loss: 1.0013\n",
            "Epoch 36/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.2768 - loss: 1.0009 - val_accuracy: 0.2493 - val_loss: 1.0011\n",
            "Epoch 37/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3052 - loss: 1.0006 - val_accuracy: 0.2581 - val_loss: 1.0011\n",
            "Epoch 38/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2836 - loss: 1.0008 - val_accuracy: 0.2568 - val_loss: 1.0009\n",
            "Epoch 39/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2962 - loss: 1.0006 - val_accuracy: 0.2575 - val_loss: 1.0011\n",
            "Epoch 40/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2944 - loss: 1.0006 - val_accuracy: 0.2432 - val_loss: 1.0011\n",
            "Epoch 41/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.2924 - loss: 1.0007 - val_accuracy: 0.2663 - val_loss: 1.0009\n",
            "Epoch 42/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.3047 - loss: 1.0004 - val_accuracy: 0.2696 - val_loss: 1.0008\n",
            "Epoch 43/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.3100 - loss: 1.0004 - val_accuracy: 0.2717 - val_loss: 1.0008\n",
            "Epoch 44/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3065 - loss: 1.0004 - val_accuracy: 0.2507 - val_loss: 1.0008\n",
            "Epoch 45/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3081 - loss: 1.0003 - val_accuracy: 0.2527 - val_loss: 1.0012\n",
            "Epoch 46/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.3028 - loss: 1.0005 - val_accuracy: 0.2737 - val_loss: 1.0009\n",
            "Epoch 47/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.3162 - loss: 1.0003 - val_accuracy: 0.2778 - val_loss: 1.0011\n",
            "Epoch 48/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3156 - loss: 1.0003 - val_accuracy: 0.2751 - val_loss: 1.0007\n",
            "Epoch 49/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3181 - loss: 1.0002 - val_accuracy: 0.2629 - val_loss: 1.0009\n",
            "Epoch 50/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3201 - loss: 1.0002 - val_accuracy: 0.2439 - val_loss: 1.0010\n",
            "Epoch 51/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3153 - loss: 1.0002 - val_accuracy: 0.2663 - val_loss: 1.0007\n",
            "Epoch 52/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.3057 - loss: 1.0003 - val_accuracy: 0.2703 - val_loss: 1.0010\n",
            "Epoch 53/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3220 - loss: 1.0001 - val_accuracy: 0.2757 - val_loss: 1.0005\n",
            "Epoch 54/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3139 - loss: 1.0001 - val_accuracy: 0.2832 - val_loss: 1.0005\n",
            "Epoch 55/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3271 - loss: 0.9999 - val_accuracy: 0.2825 - val_loss: 1.0006\n",
            "Epoch 56/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.3359 - loss: 1.0000 - val_accuracy: 0.2649 - val_loss: 1.0005\n",
            "Epoch 57/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3228 - loss: 0.9999 - val_accuracy: 0.2757 - val_loss: 1.0005\n",
            "Epoch 58/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.3303 - loss: 0.9999 - val_accuracy: 0.2744 - val_loss: 1.0005\n",
            "Epoch 59/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3295 - loss: 0.9998 - val_accuracy: 0.2920 - val_loss: 1.0005\n",
            "Epoch 60/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.3417 - loss: 0.9997 - val_accuracy: 0.2981 - val_loss: 1.0002\n",
            "\n",
            "Entrenamiento número 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 310ms/step - accuracy: 0.1007 - loss: 1.2592 - val_accuracy: 0.0982 - val_loss: 1.1239\n",
            "Epoch 2/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 310ms/step - accuracy: 0.0983 - loss: 1.0894 - val_accuracy: 0.0942 - val_loss: 1.0293\n",
            "Epoch 3/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1023 - loss: 1.0317 - val_accuracy: 0.0996 - val_loss: 1.0178\n",
            "Epoch 4/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 305ms/step - accuracy: 0.1016 - loss: 1.0156 - val_accuracy: 0.1314 - val_loss: 1.0115\n",
            "Epoch 5/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1285 - loss: 1.0085 - val_accuracy: 0.0982 - val_loss: 1.0064\n",
            "Epoch 6/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.1196 - loss: 1.0071 - val_accuracy: 0.1213 - val_loss: 1.0071\n",
            "Epoch 7/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.1268 - loss: 1.0080 - val_accuracy: 0.1565 - val_loss: 1.0068\n",
            "Epoch 8/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.1323 - loss: 1.0064 - val_accuracy: 0.2039 - val_loss: 1.0063\n",
            "Epoch 9/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1449 - loss: 1.0066 - val_accuracy: 0.1667 - val_loss: 1.0055\n",
            "Epoch 10/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.1525 - loss: 1.0050 - val_accuracy: 0.1606 - val_loss: 1.0058\n",
            "Epoch 11/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1522 - loss: 1.0049 - val_accuracy: 0.1694 - val_loss: 1.0046\n",
            "Epoch 12/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.1739 - loss: 1.0042 - val_accuracy: 0.1612 - val_loss: 1.0037\n",
            "Epoch 13/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.1738 - loss: 1.0038 - val_accuracy: 0.1911 - val_loss: 1.0047\n",
            "Epoch 14/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.1754 - loss: 1.0040 - val_accuracy: 0.2121 - val_loss: 1.0022\n",
            "Epoch 15/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.1856 - loss: 1.0033 - val_accuracy: 0.1701 - val_loss: 1.0052\n",
            "Epoch 16/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.1890 - loss: 1.0034 - val_accuracy: 0.1951 - val_loss: 1.0023\n",
            "Epoch 17/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 314ms/step - accuracy: 0.2143 - loss: 1.0022 - val_accuracy: 0.1843 - val_loss: 1.0040\n",
            "Epoch 18/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2119 - loss: 1.0027 - val_accuracy: 0.2182 - val_loss: 1.0023\n",
            "Epoch 19/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2201 - loss: 1.0022 - val_accuracy: 0.2107 - val_loss: 1.0022\n",
            "Epoch 20/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.1984 - loss: 1.0031 - val_accuracy: 0.1782 - val_loss: 1.0026\n",
            "Epoch 21/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2185 - loss: 1.0024 - val_accuracy: 0.2256 - val_loss: 1.0034\n",
            "Epoch 22/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2161 - loss: 1.0026 - val_accuracy: 0.2398 - val_loss: 1.0017\n",
            "Epoch 23/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2405 - loss: 1.0017 - val_accuracy: 0.2629 - val_loss: 1.0017\n",
            "Epoch 24/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.2394 - loss: 1.0018 - val_accuracy: 0.2304 - val_loss: 1.0025\n",
            "Epoch 25/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.2449 - loss: 1.0020 - val_accuracy: 0.2507 - val_loss: 1.0020\n",
            "Epoch 26/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.2682 - loss: 1.0017 - val_accuracy: 0.2480 - val_loss: 1.0014\n",
            "Epoch 27/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2616 - loss: 1.0015 - val_accuracy: 0.2581 - val_loss: 1.0020\n",
            "Epoch 28/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.2737 - loss: 1.0015 - val_accuracy: 0.2486 - val_loss: 1.0016\n",
            "Epoch 29/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.2754 - loss: 1.0016 - val_accuracy: 0.2547 - val_loss: 1.0015\n",
            "Epoch 30/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2876 - loss: 1.0012 - val_accuracy: 0.2622 - val_loss: 1.0017\n",
            "Epoch 31/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2720 - loss: 1.0014 - val_accuracy: 0.2033 - val_loss: 1.0023\n",
            "Epoch 32/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2787 - loss: 1.0013 - val_accuracy: 0.2270 - val_loss: 1.0023\n",
            "Epoch 33/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2758 - loss: 1.0014 - val_accuracy: 0.2703 - val_loss: 1.0014\n",
            "Epoch 34/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.2846 - loss: 1.0013 - val_accuracy: 0.2757 - val_loss: 1.0015\n",
            "Epoch 35/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.2896 - loss: 1.0012 - val_accuracy: 0.2886 - val_loss: 1.0013\n",
            "Epoch 36/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3050 - loss: 1.0010 - val_accuracy: 0.2757 - val_loss: 1.0011\n",
            "Epoch 37/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3147 - loss: 1.0009 - val_accuracy: 0.2480 - val_loss: 1.0020\n",
            "Epoch 38/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.3003 - loss: 1.0012 - val_accuracy: 0.2798 - val_loss: 1.0011\n",
            "Epoch 39/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3122 - loss: 1.0009 - val_accuracy: 0.3022 - val_loss: 1.0013\n",
            "Epoch 40/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.2998 - loss: 1.0011 - val_accuracy: 0.2866 - val_loss: 1.0013\n",
            "Epoch 41/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3225 - loss: 1.0007 - val_accuracy: 0.3035 - val_loss: 1.0012\n",
            "Epoch 42/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3163 - loss: 1.0008 - val_accuracy: 0.2873 - val_loss: 1.0015\n",
            "Epoch 43/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.3237 - loss: 1.0009 - val_accuracy: 0.2940 - val_loss: 1.0013\n",
            "Epoch 44/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3128 - loss: 1.0008 - val_accuracy: 0.2839 - val_loss: 1.0012\n",
            "Epoch 45/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3133 - loss: 1.0008 - val_accuracy: 0.2791 - val_loss: 1.0017\n",
            "Epoch 46/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.3205 - loss: 1.0007 - val_accuracy: 0.3042 - val_loss: 1.0010\n",
            "Epoch 47/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3273 - loss: 1.0006 - val_accuracy: 0.3076 - val_loss: 1.0009\n",
            "Epoch 48/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3330 - loss: 1.0004 - val_accuracy: 0.3164 - val_loss: 1.0011\n",
            "Epoch 49/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3251 - loss: 1.0005 - val_accuracy: 0.3123 - val_loss: 1.0008\n",
            "Epoch 50/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.3368 - loss: 1.0003 - val_accuracy: 0.3340 - val_loss: 1.0007\n",
            "Epoch 51/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.3503 - loss: 1.0002 - val_accuracy: 0.3157 - val_loss: 1.0007\n",
            "Epoch 52/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.3487 - loss: 1.0000 - val_accuracy: 0.2866 - val_loss: 1.0012\n",
            "Epoch 53/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3435 - loss: 1.0001 - val_accuracy: 0.3062 - val_loss: 1.0011\n",
            "Epoch 54/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3231 - loss: 1.0003 - val_accuracy: 0.2778 - val_loss: 1.0013\n",
            "Epoch 55/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3424 - loss: 1.0001 - val_accuracy: 0.2798 - val_loss: 1.0014\n",
            "Epoch 56/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3389 - loss: 1.0002 - val_accuracy: 0.3076 - val_loss: 1.0009\n",
            "Epoch 57/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - accuracy: 0.3448 - loss: 0.9997 - val_accuracy: 0.2974 - val_loss: 1.0009\n",
            "Epoch 58/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.3488 - loss: 0.9998 - val_accuracy: 0.2967 - val_loss: 1.0007\n",
            "Epoch 59/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3384 - loss: 0.9998 - val_accuracy: 0.3137 - val_loss: 1.0006\n",
            "Epoch 60/60\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.3471 - loss: 0.9996 - val_accuracy: 0.3069 - val_loss: 1.0003\n"
          ]
        }
      ],
      "source": [
        "# Modelo 3 - QuickDraw-10\n",
        "# Detalles:\n",
        "# Activacion: relu\n",
        "# Perdida: Categorical Hinge\n",
        "# Capas: 4\n",
        "# Epocas: 60\n",
        "# Batch size: 550\n",
        "\n",
        "acc_total_3, acc_clase_3, cm_3 = experiment(\n",
        "        train_images_10,\n",
        "        train_labels_10,\n",
        "        val_image_10,\n",
        "        val_labels_10,\n",
        "        test_images_10,\n",
        "        test_labels_10,\n",
        "        layers_size=[512, 256, 128, 64],\n",
        "        n_classes=10,\n",
        "        activation='relu',\n",
        "        loss_fn='categorical_hinge',\n",
        "        class_names=clases,\n",
        "        epochs=60,\n",
        "        batch_size=550,\n",
        "        n_experiments=5\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'bandage': 0.5,\n",
              "  'blackberry': 0.09836065573770492,\n",
              "  'castle': 0.022900763358778626,\n",
              "  'flashlight': 0.034482758620689655,\n",
              "  'lion': 0.0,\n",
              "  'remote-control': 0.8446601941747572,\n",
              "  'sink': 0.3793103448275862,\n",
              "  'spreadsheet': 0.2809917355371901,\n",
              "  'teapot': 0.46153846153846156,\n",
              "  'trombone': 0.2549019607843137},\n",
              " {'bandage': 0.2,\n",
              "  'blackberry': 0.3442622950819672,\n",
              "  'castle': 0.04580152671755725,\n",
              "  'flashlight': 0.16379310344827586,\n",
              "  'lion': 0.3523809523809524,\n",
              "  'remote-control': 0.8155339805825242,\n",
              "  'sink': 0.08620689655172414,\n",
              "  'spreadsheet': 0.4132231404958678,\n",
              "  'teapot': 0.5307692307692308,\n",
              "  'trombone': 0.12745098039215685},\n",
              " {'bandage': 0.16666666666666666,\n",
              "  'blackberry': 0.5983606557377049,\n",
              "  'castle': 0.09923664122137404,\n",
              "  'flashlight': 0.25,\n",
              "  'lion': 0.0,\n",
              "  'remote-control': 0.8252427184466019,\n",
              "  'sink': 0.3448275862068966,\n",
              "  'spreadsheet': 0.0,\n",
              "  'teapot': 0.23846153846153847,\n",
              "  'trombone': 0.19607843137254902},\n",
              " {'bandage': 0.03333333333333333,\n",
              "  'blackberry': 0.05737704918032787,\n",
              "  'castle': 0.29770992366412213,\n",
              "  'flashlight': 0.3448275862068966,\n",
              "  'lion': 0.01904761904761905,\n",
              "  'remote-control': 0.7184466019417476,\n",
              "  'sink': 0.3103448275862069,\n",
              "  'spreadsheet': 0.3884297520661157,\n",
              "  'teapot': 0.7384615384615385,\n",
              "  'trombone': 0.3235294117647059},\n",
              " {'bandage': 0.30833333333333335,\n",
              "  'blackberry': 0.27049180327868855,\n",
              "  'castle': 0.31297709923664124,\n",
              "  'flashlight': 0.15517241379310345,\n",
              "  'lion': 0.1619047619047619,\n",
              "  'remote-control': 0.8446601941747572,\n",
              "  'sink': 0.28448275862068967,\n",
              "  'spreadsheet': 0.39669421487603307,\n",
              "  'teapot': 0.5076923076923077,\n",
              "  'trombone': 0.0}]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_clase_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.2830188679245283,\n",
              " 0.30360205831903947,\n",
              " 0.2667238421955403,\n",
              " 0.3241852487135506,\n",
              " 0.3259005145797599]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_total_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[60,  1,  1,  0,  1, 28, 20,  1,  4,  4],\n",
              "        [57, 12,  1,  1,  1,  4, 21,  9,  5, 11],\n",
              "        [27,  0,  3,  1,  0, 48, 46,  1,  1,  4],\n",
              "        [61,  1,  1,  4,  0,  2, 23,  5,  0, 19],\n",
              "        [35,  0,  0,  1,  0, 16, 13, 11,  7, 22],\n",
              "        [ 4,  1,  0,  0,  0, 87,  7,  0,  2,  2],\n",
              "        [15,  2,  2,  0,  0, 49, 44,  2,  1,  1],\n",
              "        [30,  0,  0,  1,  1, 14, 10, 34,  4, 27],\n",
              "        [24,  0,  0,  1,  0, 35,  6,  4, 60,  0],\n",
              "        [18,  0,  1,  0,  0, 11, 30, 14,  2, 26]]),\n",
              " array([[24,  4,  0, 11, 20, 25,  0, 13, 21,  2],\n",
              "        [ 6, 42,  3, 16, 16,  5,  0, 16,  9,  9],\n",
              "        [21,  4,  6, 11,  9, 45,  5,  7, 12, 11],\n",
              "        [ 3, 18,  0, 19, 27,  3,  0, 25, 17,  4],\n",
              "        [ 0,  5,  1,  2, 37, 12,  0, 23, 20,  5],\n",
              "        [ 4,  0,  0,  1,  5, 84,  1,  1,  4,  3],\n",
              "        [ 9,  9,  5, 11,  4, 53, 10,  4,  4,  7],\n",
              "        [ 1,  5,  1,  2, 35, 11,  0, 50, 13,  3],\n",
              "        [ 1,  3,  1,  1, 13, 32,  0,  8, 69,  2],\n",
              "        [ 3,  4,  1,  5, 26, 12,  0, 31,  7, 13]]),\n",
              " array([[20, 44,  1, 12,  1, 30, 11,  0,  0,  1],\n",
              "        [ 6, 73,  0, 12,  0,  9, 18,  0,  0,  4],\n",
              "        [ 6, 34, 13,  7,  0, 45, 25,  0,  0,  1],\n",
              "        [ 6, 51,  1, 29,  1,  3, 21,  0,  0,  4],\n",
              "        [ 2, 44,  4, 10,  0, 18, 12,  0,  0, 15],\n",
              "        [ 2,  5,  0,  2,  0, 85,  9,  0,  0,  0],\n",
              "        [ 1, 21,  3,  3,  0, 47, 40,  0,  0,  1],\n",
              "        [ 5, 35,  3, 22,  0, 16, 18,  0,  0, 22],\n",
              "        [16, 37,  6,  3,  1, 28,  7,  0, 31,  1],\n",
              "        [ 0, 24,  5, 10,  0, 22, 20,  1,  0, 20]]),\n",
              " array([[ 4,  1, 20, 14,  9, 22, 11,  9, 11, 19],\n",
              "        [ 7,  7, 22, 14, 13,  5, 15, 14,  8, 17],\n",
              "        [ 2,  2, 39, 11,  6, 28, 18,  8,  6, 11],\n",
              "        [ 3,  0, 21, 40,  5,  1, 10, 11,  7, 18],\n",
              "        [ 2,  0,  7, 17,  2,  5,  6, 15, 17, 34],\n",
              "        [ 1,  0,  4,  2,  1, 74, 13,  2,  4,  2],\n",
              "        [ 3,  1, 15,  8,  7, 25, 36,  4,  7, 10],\n",
              "        [ 1,  0,  7, 14,  8,  7,  9, 47,  7, 21],\n",
              "        [ 1,  0,  6,  1,  4, 11,  3,  2, 96,  6],\n",
              "        [ 0,  0,  7, 17,  2,  3, 20, 15,  5, 33]]),\n",
              " array([[37, 11, 20,  2,  4, 28,  8,  3,  7,  0],\n",
              "        [31, 33, 17,  4,  1,  5, 18,  6,  7,  0],\n",
              "        [19,  6, 41,  1,  1, 37, 18,  6,  2,  0],\n",
              "        [29, 12, 20, 18,  8,  2, 13, 10,  4,  0],\n",
              "        [13,  7,  4,  1, 17, 17,  8, 20, 18,  0],\n",
              "        [ 3,  2,  5,  1,  0, 87,  3,  1,  1,  0],\n",
              "        [ 9,  2, 19,  2,  2, 45, 33,  1,  3,  0],\n",
              "        [ 8,  9,  9,  3,  8, 17, 15, 48,  4,  0],\n",
              "        [16,  6,  1,  2,  0, 33,  2,  4, 66,  0],\n",
              "        [13,  7, 14,  0, 13, 19, 18, 14,  4,  0]])]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFUn89PMI5Kh"
      },
      "source": [
        "Experimentos para QuickDraw-Animals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nVX-q6E4OYc3"
      },
      "outputs": [],
      "source": [
        "clases = ['sheep', 'bear', 'bee', 'cat', 'camel', 'cow', 'crab', 'crocodile', 'duck', 'elephant', 'dog', 'giraffe']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0w2q9DXBI9WQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenamiento número 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 852ms/step - accuracy: 0.0836 - loss: 1.5067 - val_accuracy: 0.0833 - val_loss: 1.1727\n",
            "Epoch 2/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 469ms/step - accuracy: 0.0819 - loss: 1.1416 - val_accuracy: 0.0861 - val_loss: 1.0924\n",
            "Epoch 3/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 460ms/step - accuracy: 0.0778 - loss: 1.1055 - val_accuracy: 0.0833 - val_loss: 1.0925\n",
            "Epoch 4/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 461ms/step - accuracy: 0.0833 - loss: 1.0915 - val_accuracy: 0.0822 - val_loss: 1.0794\n",
            "Epoch 5/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 468ms/step - accuracy: 0.0838 - loss: 1.0781 - val_accuracy: 0.0833 - val_loss: 1.0823\n",
            "Epoch 6/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.0861 - loss: 1.0710 - val_accuracy: 0.0833 - val_loss: 1.0564\n",
            "Epoch 7/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 468ms/step - accuracy: 0.0849 - loss: 1.0538 - val_accuracy: 0.0833 - val_loss: 1.0456\n",
            "Epoch 8/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 462ms/step - accuracy: 0.0785 - loss: 1.0496 - val_accuracy: 0.0850 - val_loss: 1.0358\n",
            "Epoch 9/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.0930 - loss: 1.0381 - val_accuracy: 0.0833 - val_loss: 1.0452\n",
            "Epoch 10/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 457ms/step - accuracy: 0.0889 - loss: 1.0383 - val_accuracy: 0.0822 - val_loss: 1.0289\n",
            "\n",
            "Entrenamiento número 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_16', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 482ms/step - accuracy: 0.0858 - loss: 1.1617 - val_accuracy: 0.0828 - val_loss: 1.0452\n",
            "Epoch 2/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 443ms/step - accuracy: 0.0904 - loss: 1.0422 - val_accuracy: 0.0867 - val_loss: 1.0255\n",
            "Epoch 3/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 438ms/step - accuracy: 0.0888 - loss: 1.0302 - val_accuracy: 0.0817 - val_loss: 1.0301\n",
            "Epoch 4/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 441ms/step - accuracy: 0.0829 - loss: 1.0342 - val_accuracy: 0.0833 - val_loss: 1.0304\n",
            "Epoch 5/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 441ms/step - accuracy: 0.0802 - loss: 1.0322 - val_accuracy: 0.0828 - val_loss: 1.0371\n",
            "Epoch 6/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 443ms/step - accuracy: 0.0811 - loss: 1.0346 - val_accuracy: 0.0833 - val_loss: 1.0337\n",
            "Epoch 7/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 440ms/step - accuracy: 0.0837 - loss: 1.0333 - val_accuracy: 0.0833 - val_loss: 1.0365\n",
            "Epoch 8/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 444ms/step - accuracy: 0.0858 - loss: 1.0352 - val_accuracy: 0.0800 - val_loss: 1.0339\n",
            "Epoch 9/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 446ms/step - accuracy: 0.0837 - loss: 1.0347 - val_accuracy: 0.0850 - val_loss: 1.0312\n",
            "Epoch 10/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 443ms/step - accuracy: 0.0855 - loss: 1.0323 - val_accuracy: 0.0833 - val_loss: 1.0395\n",
            "\n",
            "Entrenamiento número 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_17', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 496ms/step - accuracy: 0.0906 - loss: 1.2832 - val_accuracy: 0.0833 - val_loss: 1.1112\n",
            "Epoch 2/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 480ms/step - accuracy: 0.0828 - loss: 1.0777 - val_accuracy: 0.0833 - val_loss: 1.0598\n",
            "Epoch 3/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 468ms/step - accuracy: 0.0823 - loss: 1.0603 - val_accuracy: 0.0833 - val_loss: 1.0605\n",
            "Epoch 4/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 462ms/step - accuracy: 0.0895 - loss: 1.0621 - val_accuracy: 0.0828 - val_loss: 1.0595\n",
            "Epoch 5/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 445ms/step - accuracy: 0.0836 - loss: 1.0576 - val_accuracy: 0.0833 - val_loss: 1.0514\n",
            "Epoch 6/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 460ms/step - accuracy: 0.0852 - loss: 1.0493 - val_accuracy: 0.0739 - val_loss: 1.0444\n",
            "Epoch 7/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 463ms/step - accuracy: 0.0747 - loss: 1.0463 - val_accuracy: 0.0833 - val_loss: 1.0356\n",
            "Epoch 8/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 463ms/step - accuracy: 0.0823 - loss: 1.0388 - val_accuracy: 0.0783 - val_loss: 1.0348\n",
            "Epoch 9/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 455ms/step - accuracy: 0.0798 - loss: 1.0322 - val_accuracy: 0.0822 - val_loss: 1.0283\n",
            "Epoch 10/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.0905 - loss: 1.0319 - val_accuracy: 0.0711 - val_loss: 1.0321\n",
            "\n",
            "Entrenamiento número 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_18', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 493ms/step - accuracy: 0.0839 - loss: 1.2707 - val_accuracy: 0.0833 - val_loss: 1.1414\n",
            "Epoch 2/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 462ms/step - accuracy: 0.0814 - loss: 1.1241 - val_accuracy: 0.0833 - val_loss: 1.0663\n",
            "Epoch 3/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.0848 - loss: 1.0576 - val_accuracy: 0.0844 - val_loss: 1.0381\n",
            "Epoch 4/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 462ms/step - accuracy: 0.0933 - loss: 1.0352 - val_accuracy: 0.0833 - val_loss: 1.0348\n",
            "Epoch 5/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 465ms/step - accuracy: 0.0837 - loss: 1.0369 - val_accuracy: 0.0839 - val_loss: 1.0367\n",
            "Epoch 6/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 465ms/step - accuracy: 0.0797 - loss: 1.0370 - val_accuracy: 0.0833 - val_loss: 1.0334\n",
            "Epoch 7/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 458ms/step - accuracy: 0.0844 - loss: 1.0324 - val_accuracy: 0.0833 - val_loss: 1.0343\n",
            "Epoch 8/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 462ms/step - accuracy: 0.0838 - loss: 1.0331 - val_accuracy: 0.0850 - val_loss: 1.0339\n",
            "Epoch 9/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 462ms/step - accuracy: 0.0840 - loss: 1.0342 - val_accuracy: 0.0972 - val_loss: 1.0371\n",
            "Epoch 10/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 465ms/step - accuracy: 0.0854 - loss: 1.0355 - val_accuracy: 0.0867 - val_loss: 1.0332\n",
            "\n",
            "Entrenamiento número 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_19', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 530ms/step - accuracy: 0.0827 - loss: 1.2412 - val_accuracy: 0.0833 - val_loss: 1.1277\n",
            "Epoch 2/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 494ms/step - accuracy: 0.0843 - loss: 1.0952 - val_accuracy: 0.0950 - val_loss: 1.0371\n",
            "Epoch 3/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 476ms/step - accuracy: 0.0882 - loss: 1.0399 - val_accuracy: 0.0833 - val_loss: 1.0447\n",
            "Epoch 4/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 476ms/step - accuracy: 0.0909 - loss: 1.0402 - val_accuracy: 0.0800 - val_loss: 1.0318\n",
            "Epoch 5/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 476ms/step - accuracy: 0.0867 - loss: 1.0370 - val_accuracy: 0.0833 - val_loss: 1.0382\n",
            "Epoch 6/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 483ms/step - accuracy: 0.0860 - loss: 1.0369 - val_accuracy: 0.0833 - val_loss: 1.0339\n",
            "Epoch 7/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 482ms/step - accuracy: 0.0766 - loss: 1.0341 - val_accuracy: 0.0833 - val_loss: 1.0264\n",
            "Epoch 8/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 489ms/step - accuracy: 0.0799 - loss: 1.0275 - val_accuracy: 0.0889 - val_loss: 1.0349\n",
            "Epoch 9/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 478ms/step - accuracy: 0.0825 - loss: 1.0326 - val_accuracy: 0.0833 - val_loss: 1.0341\n",
            "Epoch 10/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 484ms/step - accuracy: 0.0846 - loss: 1.0336 - val_accuracy: 0.0833 - val_loss: 1.0307\n"
          ]
        }
      ],
      "source": [
        "# Modelo 1 - QuickDraw-Animals\n",
        "# Detalles:\n",
        "# Activacion: tanh\n",
        "# Perdida: categorical_hinge\n",
        "# Capas: 2\n",
        "# Epocas: 60\n",
        "# Batch size: 550\n",
        "\n",
        "acc_total_4, acc_clase_4, cm_4 = experiment(\n",
        "        train_images_animals,\n",
        "        train_labels_animals,\n",
        "        val_images_animals,\n",
        "        val_labels_animals,\n",
        "        test_images_animals,\n",
        "        test_labels_animals,\n",
        "        layers_size=[512, 256],\n",
        "        n_classes=12,\n",
        "        activation='tanh',\n",
        "        loss_fn='categorical_hinge',\n",
        "        class_names=clases,\n",
        "        epochs=10,\n",
        "        batch_size=1000,\n",
        "        n_experiments=5\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'sheep': 0.0,\n",
              "  'bear': 0.005,\n",
              "  'bee': 0.465,\n",
              "  'cat': 0.0,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.54,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.0,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.0},\n",
              " {'sheep': 0.0,\n",
              "  'bear': 0.0,\n",
              "  'bee': 1.0,\n",
              "  'cat': 0.0,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.0,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.0,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.0},\n",
              " {'sheep': 0.0,\n",
              "  'bear': 0.0,\n",
              "  'bee': 0.0,\n",
              "  'cat': 0.845,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.0,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.045,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.0},\n",
              " {'sheep': 0.0,\n",
              "  'bear': 0.0,\n",
              "  'bee': 0.0,\n",
              "  'cat': 0.095,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.0,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.0,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.9949748743718593},\n",
              " {'sheep': 0.0,\n",
              "  'bear': 0.0,\n",
              "  'bee': 0.0,\n",
              "  'cat': 0.0,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.0,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.0,\n",
              "  'duck': 1.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.0}]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_clase_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.08420175072947061,\n",
              " 0.08336807002917883,\n",
              " 0.07419758232596915,\n",
              " 0.09045435598165902,\n",
              " 0.08336807002917883]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_total_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[  0,   0,  84,   0,   0, 116,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   1,  78,   0,   0, 121,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   3,  93,   0,   0, 104,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   1,  83,   0,   0, 116,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   1,  63,   0,   0, 136,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   1,  91,   0,   0, 108,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,  80,   0,   0, 120,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   1,  91,   0,   0, 108,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,  72,   0,   0, 128,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   1, 101,   0,   0,  98,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,  75,   0,   0, 125,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   2,  44,   0,   0, 153,   0,   0,   0,   0,   0,   0]]),\n",
              " array([[  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0, 199,   0,   0,   0,   0,   0,   0,   0,   0,   0]]),\n",
              " array([[  0,   0,   3, 180,   0,   0,   0,  17,   0,   0,   0,   0],\n",
              "        [  0,   0,   1, 160,   0,   0,   0,  39,   0,   0,   0,   0],\n",
              "        [  0,   0,   0, 154,   0,   0,   0,  46,   0,   0,   0,   0],\n",
              "        [  0,   0,   0, 169,   0,   0,   0,  31,   0,   0,   0,   0],\n",
              "        [  0,   0,   0, 180,   0,   0,   0,  20,   0,   0,   0,   0],\n",
              "        [  0,   0,   1, 161,   0,   0,   0,  38,   0,   0,   0,   0],\n",
              "        [  0,   0,   0, 179,   0,   0,   0,  21,   0,   0,   0,   0],\n",
              "        [  0,   0,   1, 190,   0,   0,   0,   9,   0,   0,   0,   0],\n",
              "        [  0,   0,   1, 174,   0,   0,   0,  25,   0,   0,   0,   0],\n",
              "        [  0,   0,   2, 162,   0,   0,   0,  36,   0,   0,   0,   0],\n",
              "        [  0,   0,   0, 178,   0,   0,   0,  22,   0,   0,   0,   0],\n",
              "        [  0,   0,   0, 176,   0,   0,   0,  23,   0,   0,   0,   0]]),\n",
              " array([[  0,   0,   0,   9,   0,   0,   0,   0,   0,   0,   0, 191],\n",
              "        [  0,   0,   0,   5,   0,   0,   0,   0,   0,   0,   0, 195],\n",
              "        [  0,   0,   0,   9,   0,   0,   0,   0,   0,   0,   0, 191],\n",
              "        [  0,   0,   0,  19,   0,   0,   0,   0,   0,   0,   0, 181],\n",
              "        [  0,   0,   0,   8,   0,   0,   0,   0,   0,   0,   0, 192],\n",
              "        [  0,   0,   0,  14,   0,   0,   0,   0,   0,   0,   0, 186],\n",
              "        [  0,   0,   0,  13,   0,   0,   0,   0,   0,   0,   0, 187],\n",
              "        [  0,   0,   0,   8,   0,   0,   0,   0,   0,   0,   0, 192],\n",
              "        [  0,   0,   0,   8,   0,   0,   0,   0,   0,   0,   0, 192],\n",
              "        [  0,   0,   0,  13,   0,   0,   0,   0,   0,   0,   0, 187],\n",
              "        [  0,   0,   0,   9,   0,   0,   0,   0,   0,   0,   0, 191],\n",
              "        [  0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0, 198]]),\n",
              " array([[  0,   0,   0,   0,   0,   0,   0,   0, 198,   0,   0,   2],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 199,   0,   0,   1],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 197,   0,   0,   3],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 198,   0,   0,   2],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0, 199,   0,   0,   0]])]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fekRLfs0Jj4L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenamiento número 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_20', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 171ms/step - accuracy: 0.0891 - loss: 2.6282 - val_accuracy: 0.1344 - val_loss: 2.4731\n",
            "Epoch 2/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1130 - loss: 2.4745 - val_accuracy: 0.1667 - val_loss: 2.4235\n",
            "Epoch 3/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1340 - loss: 2.4556 - val_accuracy: 0.1389 - val_loss: 2.4599\n",
            "Epoch 4/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1348 - loss: 2.4489 - val_accuracy: 0.1894 - val_loss: 2.4429\n",
            "Epoch 5/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1450 - loss: 2.4371 - val_accuracy: 0.0911 - val_loss: 2.4255\n",
            "Epoch 6/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1410 - loss: 2.4339 - val_accuracy: 0.1456 - val_loss: 2.4135\n",
            "Epoch 7/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1407 - loss: 2.4249 - val_accuracy: 0.2306 - val_loss: 2.3625\n",
            "Epoch 8/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1479 - loss: 2.4266 - val_accuracy: 0.0972 - val_loss: 2.4412\n",
            "Epoch 9/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1434 - loss: 2.4252 - val_accuracy: 0.1311 - val_loss: 2.4883\n",
            "Epoch 10/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1434 - loss: 2.4351 - val_accuracy: 0.2183 - val_loss: 2.3978\n",
            "Epoch 11/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1453 - loss: 2.4284 - val_accuracy: 0.1578 - val_loss: 2.4195\n",
            "Epoch 12/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step - accuracy: 0.1341 - loss: 2.4453 - val_accuracy: 0.0872 - val_loss: 2.4404\n",
            "Epoch 13/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1302 - loss: 2.4445 - val_accuracy: 0.1272 - val_loss: 2.4404\n",
            "Epoch 14/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1264 - loss: 2.4337 - val_accuracy: 0.2183 - val_loss: 2.3966\n",
            "Epoch 15/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1367 - loss: 2.4326 - val_accuracy: 0.1839 - val_loss: 2.4234\n",
            "Epoch 16/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1497 - loss: 2.4200 - val_accuracy: 0.1244 - val_loss: 2.4304\n",
            "Epoch 17/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1335 - loss: 2.4420 - val_accuracy: 0.1583 - val_loss: 2.4307\n",
            "Epoch 18/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1495 - loss: 2.4365 - val_accuracy: 0.1417 - val_loss: 2.4626\n",
            "Epoch 19/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1270 - loss: 2.4360 - val_accuracy: 0.1961 - val_loss: 2.3673\n",
            "Epoch 20/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1365 - loss: 2.4444 - val_accuracy: 0.1706 - val_loss: 2.4269\n",
            "Epoch 21/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1328 - loss: 2.4385 - val_accuracy: 0.1650 - val_loss: 2.4402\n",
            "Epoch 22/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1357 - loss: 2.4243 - val_accuracy: 0.1089 - val_loss: 2.4519\n",
            "Epoch 23/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1257 - loss: 2.4521 - val_accuracy: 0.1750 - val_loss: 2.4005\n",
            "Epoch 24/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1347 - loss: 2.4391 - val_accuracy: 0.0978 - val_loss: 2.4546\n",
            "Epoch 25/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1190 - loss: 2.4541 - val_accuracy: 0.0978 - val_loss: 2.4830\n",
            "Epoch 26/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1251 - loss: 2.4543 - val_accuracy: 0.1417 - val_loss: 2.4343\n",
            "Epoch 27/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1340 - loss: 2.4436 - val_accuracy: 0.1250 - val_loss: 2.4584\n",
            "Epoch 28/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1355 - loss: 2.4423 - val_accuracy: 0.1794 - val_loss: 2.4029\n",
            "Epoch 29/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1076 - loss: 2.4685 - val_accuracy: 0.1361 - val_loss: 2.4545\n",
            "Epoch 30/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1146 - loss: 2.4560 - val_accuracy: 0.1722 - val_loss: 2.4195\n",
            "Epoch 31/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1341 - loss: 2.4512 - val_accuracy: 0.1456 - val_loss: 2.4089\n",
            "Epoch 32/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1267 - loss: 2.4543 - val_accuracy: 0.0839 - val_loss: 2.4901\n",
            "Epoch 33/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1293 - loss: 2.4629 - val_accuracy: 0.0839 - val_loss: 2.4729\n",
            "Epoch 34/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1218 - loss: 2.4646 - val_accuracy: 0.1383 - val_loss: 2.4439\n",
            "Epoch 35/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1181 - loss: 2.4634 - val_accuracy: 0.0833 - val_loss: 2.5026\n",
            "Epoch 36/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1185 - loss: 2.4655 - val_accuracy: 0.0844 - val_loss: 2.4953\n",
            "Epoch 37/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1013 - loss: 2.4740 - val_accuracy: 0.0822 - val_loss: 2.4680\n",
            "Epoch 38/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1067 - loss: 2.4675 - val_accuracy: 0.0833 - val_loss: 2.4876\n",
            "Epoch 39/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1302 - loss: 2.4539 - val_accuracy: 0.0833 - val_loss: 2.4767\n",
            "Epoch 40/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.0991 - loss: 2.4745 - val_accuracy: 0.1283 - val_loss: 2.5103\n",
            "Epoch 41/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1414 - loss: 2.4328 - val_accuracy: 0.1344 - val_loss: 2.4097\n",
            "Epoch 42/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1160 - loss: 2.4585 - val_accuracy: 0.1494 - val_loss: 2.4551\n",
            "Epoch 43/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1346 - loss: 2.4509 - val_accuracy: 0.0883 - val_loss: 2.5229\n",
            "Epoch 44/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1326 - loss: 2.4349 - val_accuracy: 0.1700 - val_loss: 2.3927\n",
            "Epoch 45/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 157ms/step - accuracy: 0.1407 - loss: 2.4230 - val_accuracy: 0.1472 - val_loss: 2.4685\n",
            "Epoch 46/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1193 - loss: 2.4746 - val_accuracy: 0.0833 - val_loss: 2.4915\n",
            "Epoch 47/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1140 - loss: 2.4721 - val_accuracy: 0.1244 - val_loss: 2.4567\n",
            "Epoch 48/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.0988 - loss: 2.4817 - val_accuracy: 0.1222 - val_loss: 2.4648\n",
            "Epoch 49/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1060 - loss: 2.4825 - val_accuracy: 0.1083 - val_loss: 2.4629\n",
            "Epoch 50/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1065 - loss: 2.4713 - val_accuracy: 0.1067 - val_loss: 2.5112\n",
            "Epoch 51/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.0994 - loss: 2.4848 - val_accuracy: 0.1144 - val_loss: 2.4753\n",
            "Epoch 52/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1106 - loss: 2.4809 - val_accuracy: 0.0833 - val_loss: 2.5181\n",
            "Epoch 53/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.0969 - loss: 2.4809 - val_accuracy: 0.0917 - val_loss: 2.4699\n",
            "Epoch 54/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 156ms/step - accuracy: 0.1022 - loss: 2.4750 - val_accuracy: 0.0844 - val_loss: 2.4791\n",
            "Epoch 55/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1273 - loss: 2.4506 - val_accuracy: 0.0833 - val_loss: 2.5314\n",
            "Epoch 56/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1070 - loss: 2.4800 - val_accuracy: 0.1089 - val_loss: 2.4601\n",
            "Epoch 57/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1104 - loss: 2.4743 - val_accuracy: 0.0833 - val_loss: 2.4795\n",
            "Epoch 58/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1144 - loss: 2.4670 - val_accuracy: 0.0950 - val_loss: 2.4731\n",
            "Epoch 59/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1080 - loss: 2.4700 - val_accuracy: 0.0961 - val_loss: 2.4778\n",
            "Epoch 60/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 156ms/step - accuracy: 0.1198 - loss: 2.4599 - val_accuracy: 0.0933 - val_loss: 2.5087\n",
            "Epoch 61/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1071 - loss: 2.4692 - val_accuracy: 0.1356 - val_loss: 2.4702\n",
            "Epoch 62/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1150 - loss: 2.4724 - val_accuracy: 0.0844 - val_loss: 2.5255\n",
            "Epoch 63/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1015 - loss: 2.4766 - val_accuracy: 0.1417 - val_loss: 2.4443\n",
            "Epoch 64/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1252 - loss: 2.4465 - val_accuracy: 0.1094 - val_loss: 2.4824\n",
            "Epoch 65/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1089 - loss: 2.4752 - val_accuracy: 0.1289 - val_loss: 2.4685\n",
            "Epoch 66/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 157ms/step - accuracy: 0.1177 - loss: 2.4661 - val_accuracy: 0.1822 - val_loss: 2.4143\n",
            "Epoch 67/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.1253 - loss: 2.4347 - val_accuracy: 0.1772 - val_loss: 2.3765\n",
            "Epoch 68/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1438 - loss: 2.4120 - val_accuracy: 0.0883 - val_loss: 2.5222\n",
            "Epoch 69/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step - accuracy: 0.1309 - loss: 2.4262 - val_accuracy: 0.1511 - val_loss: 2.3603\n",
            "Epoch 70/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 158ms/step - accuracy: 0.1572 - loss: 2.3823 - val_accuracy: 0.1500 - val_loss: 2.4282\n",
            "Epoch 71/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1156 - loss: 2.4549 - val_accuracy: 0.1156 - val_loss: 2.4599\n",
            "Epoch 72/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1180 - loss: 2.4567 - val_accuracy: 0.0850 - val_loss: 2.4617\n",
            "Epoch 73/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1232 - loss: 2.4607 - val_accuracy: 0.1578 - val_loss: 2.3967\n",
            "Epoch 74/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1467 - loss: 2.4359 - val_accuracy: 0.1667 - val_loss: 2.3926\n",
            "Epoch 75/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1319 - loss: 2.4497 - val_accuracy: 0.0889 - val_loss: 2.4829\n",
            "Epoch 76/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.0941 - loss: 2.4817 - val_accuracy: 0.0989 - val_loss: 2.4505\n",
            "Epoch 77/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1209 - loss: 2.4481 - val_accuracy: 0.0833 - val_loss: 2.4875\n",
            "Epoch 78/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1106 - loss: 2.4700 - val_accuracy: 0.1017 - val_loss: 2.4523\n",
            "Epoch 79/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1225 - loss: 2.4410 - val_accuracy: 0.1128 - val_loss: 2.4070\n",
            "Epoch 80/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1366 - loss: 2.4146 - val_accuracy: 0.1556 - val_loss: 2.4499\n",
            "Epoch 81/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1062 - loss: 2.4696 - val_accuracy: 0.0928 - val_loss: 2.4965\n",
            "Epoch 82/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1145 - loss: 2.4733 - val_accuracy: 0.1256 - val_loss: 2.4498\n",
            "Epoch 83/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1216 - loss: 2.4530 - val_accuracy: 0.1428 - val_loss: 2.4235\n",
            "Epoch 84/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1180 - loss: 2.4520 - val_accuracy: 0.0878 - val_loss: 2.5375\n",
            "Epoch 85/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1383 - loss: 2.4284 - val_accuracy: 0.1828 - val_loss: 2.3736\n",
            "Epoch 86/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1399 - loss: 2.4244 - val_accuracy: 0.1106 - val_loss: 2.4618\n",
            "Epoch 87/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1386 - loss: 2.4143 - val_accuracy: 0.1283 - val_loss: 2.3947\n",
            "Epoch 88/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1150 - loss: 2.4679 - val_accuracy: 0.0900 - val_loss: 2.4868\n",
            "Epoch 89/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1097 - loss: 2.4612 - val_accuracy: 0.1217 - val_loss: 2.4562\n",
            "Epoch 90/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1232 - loss: 2.4564 - val_accuracy: 0.1311 - val_loss: 2.4343\n",
            "Epoch 91/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1346 - loss: 2.4425 - val_accuracy: 0.1483 - val_loss: 2.4149\n",
            "Epoch 92/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1345 - loss: 2.4269 - val_accuracy: 0.1322 - val_loss: 2.4645\n",
            "Epoch 93/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1334 - loss: 2.4511 - val_accuracy: 0.1028 - val_loss: 2.4771\n",
            "Epoch 94/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1261 - loss: 2.4480 - val_accuracy: 0.1589 - val_loss: 2.3922\n",
            "Epoch 95/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1460 - loss: 2.4164 - val_accuracy: 0.1822 - val_loss: 2.3710\n",
            "Epoch 96/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1365 - loss: 2.4339 - val_accuracy: 0.1600 - val_loss: 2.4119\n",
            "Epoch 97/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.1395 - loss: 2.4221 - val_accuracy: 0.1444 - val_loss: 2.4358\n",
            "Epoch 98/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 162ms/step - accuracy: 0.1667 - loss: 2.3741 - val_accuracy: 0.0839 - val_loss: 2.6004\n",
            "Epoch 99/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1651 - loss: 2.3817 - val_accuracy: 0.2250 - val_loss: 2.2883\n",
            "Epoch 100/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.1588 - loss: 2.3800 - val_accuracy: 0.1950 - val_loss: 2.3262\n",
            "\n",
            "Entrenamiento número 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_21', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 172ms/step - accuracy: 0.0966 - loss: 2.6077 - val_accuracy: 0.0922 - val_loss: 2.4741\n",
            "Epoch 2/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1091 - loss: 2.4732 - val_accuracy: 0.1139 - val_loss: 2.4521\n",
            "Epoch 3/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1362 - loss: 2.4468 - val_accuracy: 0.0883 - val_loss: 2.4409\n",
            "Epoch 4/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1252 - loss: 2.4442 - val_accuracy: 0.1044 - val_loss: 2.4333\n",
            "Epoch 5/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1425 - loss: 2.4257 - val_accuracy: 0.1739 - val_loss: 2.4259\n",
            "Epoch 6/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1477 - loss: 2.4225 - val_accuracy: 0.2261 - val_loss: 2.3861\n",
            "Epoch 7/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1630 - loss: 2.4234 - val_accuracy: 0.1194 - val_loss: 2.4351\n",
            "Epoch 8/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1398 - loss: 2.4208 - val_accuracy: 0.1039 - val_loss: 2.4199\n",
            "Epoch 9/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 168ms/step - accuracy: 0.1359 - loss: 2.4255 - val_accuracy: 0.1772 - val_loss: 2.4166\n",
            "Epoch 10/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1628 - loss: 2.4211 - val_accuracy: 0.1539 - val_loss: 2.3728\n",
            "Epoch 11/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1474 - loss: 2.4135 - val_accuracy: 0.1456 - val_loss: 2.4305\n",
            "Epoch 12/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1334 - loss: 2.4323 - val_accuracy: 0.1628 - val_loss: 2.3969\n",
            "Epoch 13/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1312 - loss: 2.4353 - val_accuracy: 0.0833 - val_loss: 2.4899\n",
            "Epoch 14/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1504 - loss: 2.4254 - val_accuracy: 0.1711 - val_loss: 2.4119\n",
            "Epoch 15/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1414 - loss: 2.4300 - val_accuracy: 0.0833 - val_loss: 2.5704\n",
            "Epoch 16/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1300 - loss: 2.4565 - val_accuracy: 0.1239 - val_loss: 2.4156\n",
            "Epoch 17/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1313 - loss: 2.4333 - val_accuracy: 0.1494 - val_loss: 2.4094\n",
            "Epoch 18/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1567 - loss: 2.4103 - val_accuracy: 0.1961 - val_loss: 2.3874\n",
            "Epoch 19/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1575 - loss: 2.4193 - val_accuracy: 0.1211 - val_loss: 2.4242\n",
            "Epoch 20/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1320 - loss: 2.4343 - val_accuracy: 0.1422 - val_loss: 2.4506\n",
            "Epoch 21/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1394 - loss: 2.4372 - val_accuracy: 0.0833 - val_loss: 2.5486\n",
            "Epoch 22/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1191 - loss: 2.4651 - val_accuracy: 0.0839 - val_loss: 2.4703\n",
            "Epoch 23/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1210 - loss: 2.4559 - val_accuracy: 0.0900 - val_loss: 2.4358\n",
            "Epoch 24/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1111 - loss: 2.4620 - val_accuracy: 0.1422 - val_loss: 2.4254\n",
            "Epoch 25/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1532 - loss: 2.4149 - val_accuracy: 0.0839 - val_loss: 2.4568\n",
            "Epoch 26/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 169ms/step - accuracy: 0.1252 - loss: 2.4470 - val_accuracy: 0.1678 - val_loss: 2.4336\n",
            "Epoch 27/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 168ms/step - accuracy: 0.1246 - loss: 2.4503 - val_accuracy: 0.1061 - val_loss: 2.4621\n",
            "Epoch 28/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1379 - loss: 2.4435 - val_accuracy: 0.0861 - val_loss: 2.4944\n",
            "Epoch 29/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1303 - loss: 2.4366 - val_accuracy: 0.0833 - val_loss: 2.4765\n",
            "Epoch 30/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1153 - loss: 2.4586 - val_accuracy: 0.0867 - val_loss: 2.5931\n",
            "Epoch 31/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1283 - loss: 2.4516 - val_accuracy: 0.1233 - val_loss: 2.4468\n",
            "Epoch 32/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1291 - loss: 2.4572 - val_accuracy: 0.1406 - val_loss: 2.4448\n",
            "Epoch 33/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1326 - loss: 2.4635 - val_accuracy: 0.1378 - val_loss: 2.5085\n",
            "Epoch 34/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1325 - loss: 2.4438 - val_accuracy: 0.1494 - val_loss: 2.4465\n",
            "Epoch 35/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1475 - loss: 2.4308 - val_accuracy: 0.1172 - val_loss: 2.4251\n",
            "Epoch 36/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1372 - loss: 2.4333 - val_accuracy: 0.1417 - val_loss: 2.4167\n",
            "Epoch 37/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1269 - loss: 2.4325 - val_accuracy: 0.2022 - val_loss: 2.3957\n",
            "Epoch 38/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1539 - loss: 2.4226 - val_accuracy: 0.1156 - val_loss: 2.3948\n",
            "Epoch 39/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 167ms/step - accuracy: 0.1484 - loss: 2.4206 - val_accuracy: 0.1183 - val_loss: 2.4064\n",
            "Epoch 40/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1336 - loss: 2.4235 - val_accuracy: 0.1044 - val_loss: 2.4760\n",
            "Epoch 41/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1102 - loss: 2.4774 - val_accuracy: 0.1061 - val_loss: 2.4613\n",
            "Epoch 42/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1060 - loss: 2.4689 - val_accuracy: 0.1361 - val_loss: 2.4714\n",
            "Epoch 43/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1108 - loss: 2.4631 - val_accuracy: 0.1428 - val_loss: 2.4164\n",
            "Epoch 44/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1416 - loss: 2.4290 - val_accuracy: 0.1083 - val_loss: 2.4507\n",
            "Epoch 45/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1281 - loss: 2.4511 - val_accuracy: 0.1661 - val_loss: 2.4055\n",
            "Epoch 46/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1535 - loss: 2.4096 - val_accuracy: 0.1094 - val_loss: 2.4529\n",
            "Epoch 47/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1225 - loss: 2.4504 - val_accuracy: 0.1478 - val_loss: 2.4100\n",
            "Epoch 48/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.1407 - loss: 2.4071 - val_accuracy: 0.1467 - val_loss: 2.3699\n",
            "Epoch 49/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1384 - loss: 2.4083 - val_accuracy: 0.1300 - val_loss: 2.4621\n",
            "Epoch 50/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1364 - loss: 2.4231 - val_accuracy: 0.1911 - val_loss: 2.4058\n",
            "Epoch 51/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1615 - loss: 2.4007 - val_accuracy: 0.1989 - val_loss: 2.3395\n",
            "Epoch 52/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1463 - loss: 2.4053 - val_accuracy: 0.1706 - val_loss: 2.3625\n",
            "Epoch 53/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 173ms/step - accuracy: 0.1544 - loss: 2.3819 - val_accuracy: 0.1617 - val_loss: 2.4324\n",
            "Epoch 54/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 173ms/step - accuracy: 0.1569 - loss: 2.3958 - val_accuracy: 0.1539 - val_loss: 2.3615\n",
            "Epoch 55/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1381 - loss: 2.4085 - val_accuracy: 0.1394 - val_loss: 2.3544\n",
            "Epoch 56/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1607 - loss: 2.3627 - val_accuracy: 0.1533 - val_loss: 2.3745\n",
            "Epoch 57/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1649 - loss: 2.3415 - val_accuracy: 0.2494 - val_loss: 2.2976\n",
            "Epoch 58/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1913 - loss: 2.3249 - val_accuracy: 0.1844 - val_loss: 2.3239\n",
            "Epoch 59/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1470 - loss: 2.4152 - val_accuracy: 0.2561 - val_loss: 2.2248\n",
            "Epoch 60/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1651 - loss: 2.3329 - val_accuracy: 0.1717 - val_loss: 2.3703\n",
            "Epoch 61/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1494 - loss: 2.3712 - val_accuracy: 0.1906 - val_loss: 2.3117\n",
            "Epoch 62/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1609 - loss: 2.3730 - val_accuracy: 0.2389 - val_loss: 2.2615\n",
            "Epoch 63/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1898 - loss: 2.3100 - val_accuracy: 0.1350 - val_loss: 2.3698\n",
            "Epoch 64/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1524 - loss: 2.3601 - val_accuracy: 0.1822 - val_loss: 2.3333\n",
            "Epoch 65/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1754 - loss: 2.3126 - val_accuracy: 0.1994 - val_loss: 2.3121\n",
            "Epoch 66/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1725 - loss: 2.3300 - val_accuracy: 0.1767 - val_loss: 2.2757\n",
            "Epoch 67/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1765 - loss: 2.3045 - val_accuracy: 0.1650 - val_loss: 2.2689\n",
            "Epoch 68/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1887 - loss: 2.2972 - val_accuracy: 0.1506 - val_loss: 2.3381\n",
            "Epoch 69/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1933 - loss: 2.3177 - val_accuracy: 0.1644 - val_loss: 2.3895\n",
            "Epoch 70/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1902 - loss: 2.3115 - val_accuracy: 0.1133 - val_loss: 2.5691\n",
            "Epoch 71/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1691 - loss: 2.3505 - val_accuracy: 0.1617 - val_loss: 2.4397\n",
            "Epoch 72/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.2008 - loss: 2.2889 - val_accuracy: 0.2289 - val_loss: 2.2210\n",
            "Epoch 73/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.2078 - loss: 2.2616 - val_accuracy: 0.1894 - val_loss: 2.2429\n",
            "Epoch 74/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1899 - loss: 2.2858 - val_accuracy: 0.1733 - val_loss: 2.2921\n",
            "Epoch 75/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.2112 - loss: 2.2556 - val_accuracy: 0.1728 - val_loss: 2.2654\n",
            "Epoch 76/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1759 - loss: 2.3007 - val_accuracy: 0.1400 - val_loss: 2.4624\n",
            "Epoch 77/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1743 - loss: 2.3080 - val_accuracy: 0.1856 - val_loss: 2.3080\n",
            "Epoch 78/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1749 - loss: 2.3196 - val_accuracy: 0.1528 - val_loss: 2.4346\n",
            "Epoch 79/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.1810 - loss: 2.3021 - val_accuracy: 0.1939 - val_loss: 2.2598\n",
            "Epoch 80/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1769 - loss: 2.2993 - val_accuracy: 0.2017 - val_loss: 2.2749\n",
            "Epoch 81/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.1904 - loss: 2.3053 - val_accuracy: 0.1806 - val_loss: 2.2950\n",
            "Epoch 82/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.2055 - loss: 2.2626 - val_accuracy: 0.2100 - val_loss: 2.2457\n",
            "Epoch 83/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.2100 - loss: 2.2499 - val_accuracy: 0.2028 - val_loss: 2.2578\n",
            "Epoch 84/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1863 - loss: 2.3047 - val_accuracy: 0.1928 - val_loss: 2.2730\n",
            "Epoch 85/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1978 - loss: 2.2830 - val_accuracy: 0.1633 - val_loss: 2.3419\n",
            "Epoch 86/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.1924 - loss: 2.2925 - val_accuracy: 0.1522 - val_loss: 2.2879\n",
            "Epoch 87/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.2078 - loss: 2.2784 - val_accuracy: 0.1822 - val_loss: 2.2438\n",
            "Epoch 88/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.2119 - loss: 2.2492 - val_accuracy: 0.2761 - val_loss: 2.1876\n",
            "Epoch 89/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.2473 - loss: 2.2012 - val_accuracy: 0.1489 - val_loss: 2.3931\n",
            "Epoch 90/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.2221 - loss: 2.2471 - val_accuracy: 0.1944 - val_loss: 2.2912\n",
            "Epoch 91/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.2197 - loss: 2.2366 - val_accuracy: 0.1900 - val_loss: 2.2589\n",
            "Epoch 92/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 168ms/step - accuracy: 0.2454 - loss: 2.2017 - val_accuracy: 0.2050 - val_loss: 2.3462\n",
            "Epoch 93/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.2154 - loss: 2.2767 - val_accuracy: 0.2428 - val_loss: 2.2259\n",
            "Epoch 94/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.2308 - loss: 2.2158 - val_accuracy: 0.2011 - val_loss: 2.3248\n",
            "Epoch 95/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.2448 - loss: 2.2102 - val_accuracy: 0.2639 - val_loss: 2.1906\n",
            "Epoch 96/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step - accuracy: 0.2471 - loss: 2.1874 - val_accuracy: 0.2139 - val_loss: 2.2191\n",
            "Epoch 97/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.2417 - loss: 2.2077 - val_accuracy: 0.2389 - val_loss: 2.2066\n",
            "Epoch 98/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.2677 - loss: 2.1756 - val_accuracy: 0.2239 - val_loss: 2.3153\n",
            "Epoch 99/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.2601 - loss: 2.1858 - val_accuracy: 0.3089 - val_loss: 2.0931\n",
            "Epoch 100/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.2623 - loss: 2.1693 - val_accuracy: 0.1578 - val_loss: 2.4404\n",
            "\n",
            "Entrenamiento número 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_22', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 187ms/step - accuracy: 0.0839 - loss: 2.6437 - val_accuracy: 0.0911 - val_loss: 2.4777\n",
            "Epoch 2/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.0993 - loss: 2.4800 - val_accuracy: 0.0900 - val_loss: 2.4722\n",
            "Epoch 3/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1107 - loss: 2.4587 - val_accuracy: 0.1300 - val_loss: 2.4720\n",
            "Epoch 4/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1082 - loss: 2.4615 - val_accuracy: 0.2000 - val_loss: 2.4150\n",
            "Epoch 5/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1334 - loss: 2.4570 - val_accuracy: 0.1811 - val_loss: 2.4342\n",
            "Epoch 6/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1293 - loss: 2.4471 - val_accuracy: 0.0833 - val_loss: 2.4514\n",
            "Epoch 7/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1354 - loss: 2.4352 - val_accuracy: 0.0900 - val_loss: 2.4550\n",
            "Epoch 8/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1428 - loss: 2.4234 - val_accuracy: 0.1144 - val_loss: 2.4454\n",
            "Epoch 9/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1454 - loss: 2.4293 - val_accuracy: 0.1606 - val_loss: 2.4074\n",
            "Epoch 10/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1435 - loss: 2.4176 - val_accuracy: 0.1250 - val_loss: 2.4477\n",
            "Epoch 11/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1566 - loss: 2.4273 - val_accuracy: 0.1233 - val_loss: 2.4496\n",
            "Epoch 12/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1373 - loss: 2.4389 - val_accuracy: 0.0950 - val_loss: 2.4524\n",
            "Epoch 13/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1353 - loss: 2.4322 - val_accuracy: 0.1533 - val_loss: 2.4048\n",
            "Epoch 14/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1415 - loss: 2.4331 - val_accuracy: 0.2017 - val_loss: 2.4123\n",
            "Epoch 15/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1370 - loss: 2.4443 - val_accuracy: 0.1700 - val_loss: 2.4422\n",
            "Epoch 16/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1437 - loss: 2.4387 - val_accuracy: 0.0822 - val_loss: 2.4505\n",
            "Epoch 17/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1286 - loss: 2.4412 - val_accuracy: 0.1100 - val_loss: 2.4891\n",
            "Epoch 18/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1516 - loss: 2.4243 - val_accuracy: 0.1444 - val_loss: 2.4538\n",
            "Epoch 19/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1275 - loss: 2.4500 - val_accuracy: 0.0833 - val_loss: 2.4787\n",
            "Epoch 20/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1326 - loss: 2.4415 - val_accuracy: 0.0833 - val_loss: 2.5098\n",
            "Epoch 21/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1174 - loss: 2.4585 - val_accuracy: 0.1128 - val_loss: 2.4461\n",
            "Epoch 22/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1362 - loss: 2.4413 - val_accuracy: 0.1028 - val_loss: 2.4570\n",
            "Epoch 23/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1235 - loss: 2.4666 - val_accuracy: 0.1006 - val_loss: 2.4697\n",
            "Epoch 24/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1268 - loss: 2.4554 - val_accuracy: 0.0828 - val_loss: 2.5174\n",
            "Epoch 25/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1027 - loss: 2.4822 - val_accuracy: 0.1311 - val_loss: 2.4422\n",
            "Epoch 26/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1419 - loss: 2.4531 - val_accuracy: 0.1772 - val_loss: 2.4005\n",
            "Epoch 27/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1430 - loss: 2.4368 - val_accuracy: 0.1617 - val_loss: 2.4012\n",
            "Epoch 28/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1352 - loss: 2.4407 - val_accuracy: 0.1189 - val_loss: 2.4344\n",
            "Epoch 29/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1195 - loss: 2.4559 - val_accuracy: 0.1089 - val_loss: 2.4685\n",
            "Epoch 30/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1181 - loss: 2.4693 - val_accuracy: 0.0822 - val_loss: 2.4688\n",
            "Epoch 31/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.1094 - loss: 2.4598 - val_accuracy: 0.0911 - val_loss: 2.4595\n",
            "Epoch 32/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1135 - loss: 2.4622 - val_accuracy: 0.0833 - val_loss: 2.4538\n",
            "Epoch 33/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1171 - loss: 2.4547 - val_accuracy: 0.0833 - val_loss: 2.5161\n",
            "Epoch 34/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1178 - loss: 2.4565 - val_accuracy: 0.1344 - val_loss: 2.4313\n",
            "Epoch 35/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1471 - loss: 2.4497 - val_accuracy: 0.1244 - val_loss: 2.5128\n",
            "Epoch 36/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1298 - loss: 2.4538 - val_accuracy: 0.1306 - val_loss: 2.4339\n",
            "Epoch 37/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1343 - loss: 2.4390 - val_accuracy: 0.1289 - val_loss: 2.4628\n",
            "Epoch 38/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1236 - loss: 2.4609 - val_accuracy: 0.1156 - val_loss: 2.4603\n",
            "Epoch 39/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1207 - loss: 2.4647 - val_accuracy: 0.0833 - val_loss: 2.5240\n",
            "Epoch 40/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1306 - loss: 2.4594 - val_accuracy: 0.1011 - val_loss: 2.5135\n",
            "Epoch 41/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1200 - loss: 2.4535 - val_accuracy: 0.1300 - val_loss: 2.4305\n",
            "Epoch 42/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1248 - loss: 2.4439 - val_accuracy: 0.1050 - val_loss: 2.4764\n",
            "Epoch 43/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 189ms/step - accuracy: 0.1219 - loss: 2.4640 - val_accuracy: 0.1022 - val_loss: 2.4712\n",
            "Epoch 44/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 191ms/step - accuracy: 0.1074 - loss: 2.4621 - val_accuracy: 0.1333 - val_loss: 2.4492\n",
            "Epoch 45/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 189ms/step - accuracy: 0.1283 - loss: 2.4639 - val_accuracy: 0.1533 - val_loss: 2.4173\n",
            "Epoch 46/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1314 - loss: 2.4452 - val_accuracy: 0.0900 - val_loss: 2.4807\n",
            "Epoch 47/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1210 - loss: 2.4567 - val_accuracy: 0.0933 - val_loss: 2.4912\n",
            "Epoch 48/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1271 - loss: 2.4607 - val_accuracy: 0.0922 - val_loss: 2.4886\n",
            "Epoch 49/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1177 - loss: 2.4550 - val_accuracy: 0.1211 - val_loss: 2.4333\n",
            "Epoch 50/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1147 - loss: 2.4524 - val_accuracy: 0.1606 - val_loss: 2.4031\n",
            "Epoch 51/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1385 - loss: 2.4445 - val_accuracy: 0.1378 - val_loss: 2.4184\n",
            "Epoch 52/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1376 - loss: 2.4341 - val_accuracy: 0.1478 - val_loss: 2.4060\n",
            "Epoch 53/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1162 - loss: 2.4544 - val_accuracy: 0.2133 - val_loss: 2.3389\n",
            "Epoch 54/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 172ms/step - accuracy: 0.1561 - loss: 2.4120 - val_accuracy: 0.2139 - val_loss: 2.3352\n",
            "Epoch 55/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1428 - loss: 2.4147 - val_accuracy: 0.1383 - val_loss: 2.4054\n",
            "Epoch 56/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1578 - loss: 2.3951 - val_accuracy: 0.1989 - val_loss: 2.3094\n",
            "Epoch 57/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1554 - loss: 2.3935 - val_accuracy: 0.1467 - val_loss: 2.3172\n",
            "Epoch 58/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1517 - loss: 2.4012 - val_accuracy: 0.1522 - val_loss: 2.4207\n",
            "Epoch 59/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1321 - loss: 2.4301 - val_accuracy: 0.1556 - val_loss: 2.4078\n",
            "Epoch 60/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1300 - loss: 2.4396 - val_accuracy: 0.1100 - val_loss: 2.4778\n",
            "Epoch 61/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step - accuracy: 0.1296 - loss: 2.4332 - val_accuracy: 0.1250 - val_loss: 2.4084\n",
            "Epoch 62/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step - accuracy: 0.1325 - loss: 2.4222 - val_accuracy: 0.1628 - val_loss: 2.4057\n",
            "Epoch 63/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1390 - loss: 2.4314 - val_accuracy: 0.1794 - val_loss: 2.3483\n",
            "Epoch 64/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1712 - loss: 2.3564 - val_accuracy: 0.1756 - val_loss: 2.3436\n",
            "Epoch 65/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1579 - loss: 2.3889 - val_accuracy: 0.1067 - val_loss: 2.4897\n",
            "Epoch 66/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1585 - loss: 2.3826 - val_accuracy: 0.1567 - val_loss: 2.3110\n",
            "Epoch 67/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1780 - loss: 2.3402 - val_accuracy: 0.1844 - val_loss: 2.3235\n",
            "Epoch 68/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1612 - loss: 2.3558 - val_accuracy: 0.1306 - val_loss: 2.4849\n",
            "Epoch 69/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1710 - loss: 2.3478 - val_accuracy: 0.1700 - val_loss: 2.3371\n",
            "Epoch 70/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1829 - loss: 2.3266 - val_accuracy: 0.1806 - val_loss: 2.3343\n",
            "Epoch 71/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1693 - loss: 2.3416 - val_accuracy: 0.1272 - val_loss: 2.3838\n",
            "Epoch 72/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1418 - loss: 2.3880 - val_accuracy: 0.1400 - val_loss: 2.3836\n",
            "Epoch 73/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 176ms/step - accuracy: 0.1461 - loss: 2.4046 - val_accuracy: 0.0833 - val_loss: 2.6898\n",
            "Epoch 74/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1537 - loss: 2.4085 - val_accuracy: 0.1094 - val_loss: 2.5478\n",
            "Epoch 75/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step - accuracy: 0.1579 - loss: 2.3966 - val_accuracy: 0.1800 - val_loss: 2.3252\n",
            "Epoch 76/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1661 - loss: 2.3507 - val_accuracy: 0.1217 - val_loss: 2.3829\n",
            "Epoch 77/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1481 - loss: 2.3856 - val_accuracy: 0.1428 - val_loss: 2.5001\n",
            "Epoch 78/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1492 - loss: 2.3893 - val_accuracy: 0.1306 - val_loss: 2.4032\n",
            "Epoch 79/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1623 - loss: 2.3500 - val_accuracy: 0.2289 - val_loss: 2.3047\n",
            "Epoch 80/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1649 - loss: 2.3474 - val_accuracy: 0.1383 - val_loss: 2.3653\n",
            "Epoch 81/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1498 - loss: 2.3674 - val_accuracy: 0.1867 - val_loss: 2.3154\n",
            "Epoch 82/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1683 - loss: 2.3443 - val_accuracy: 0.1822 - val_loss: 2.3090\n",
            "Epoch 83/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1665 - loss: 2.3419 - val_accuracy: 0.1328 - val_loss: 2.4425\n",
            "Epoch 84/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1606 - loss: 2.3506 - val_accuracy: 0.1767 - val_loss: 2.3231\n",
            "Epoch 85/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1567 - loss: 2.3820 - val_accuracy: 0.1311 - val_loss: 2.3865\n",
            "Epoch 86/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1622 - loss: 2.3564 - val_accuracy: 0.1228 - val_loss: 2.3998\n",
            "Epoch 87/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1644 - loss: 2.3533 - val_accuracy: 0.1822 - val_loss: 2.3312\n",
            "Epoch 88/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1671 - loss: 2.3384 - val_accuracy: 0.2317 - val_loss: 2.2874\n",
            "Epoch 89/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1714 - loss: 2.3422 - val_accuracy: 0.1717 - val_loss: 2.3329\n",
            "Epoch 90/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1813 - loss: 2.3312 - val_accuracy: 0.2194 - val_loss: 2.2771\n",
            "Epoch 91/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1699 - loss: 2.3697 - val_accuracy: 0.1889 - val_loss: 2.3242\n",
            "Epoch 92/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1765 - loss: 2.3252 - val_accuracy: 0.1933 - val_loss: 2.2886\n",
            "Epoch 93/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1788 - loss: 2.3287 - val_accuracy: 0.1822 - val_loss: 2.2802\n",
            "Epoch 94/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step - accuracy: 0.1751 - loss: 2.3359 - val_accuracy: 0.2389 - val_loss: 2.2454\n",
            "Epoch 95/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1959 - loss: 2.3035 - val_accuracy: 0.1550 - val_loss: 2.3432\n",
            "Epoch 96/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1693 - loss: 2.3198 - val_accuracy: 0.1589 - val_loss: 2.3540\n",
            "Epoch 97/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1851 - loss: 2.3299 - val_accuracy: 0.1683 - val_loss: 2.3076\n",
            "Epoch 98/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1611 - loss: 2.3506 - val_accuracy: 0.1806 - val_loss: 2.3195\n",
            "Epoch 99/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1893 - loss: 2.3086 - val_accuracy: 0.2089 - val_loss: 2.3284\n",
            "Epoch 100/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1781 - loss: 2.3263 - val_accuracy: 0.1217 - val_loss: 2.7033\n",
            "\n",
            "Entrenamiento número 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_23', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 188ms/step - accuracy: 0.0929 - loss: 2.6400 - val_accuracy: 0.1089 - val_loss: 2.4901\n",
            "Epoch 2/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.0910 - loss: 2.4829 - val_accuracy: 0.0833 - val_loss: 2.4866\n",
            "Epoch 3/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1108 - loss: 2.4675 - val_accuracy: 0.1078 - val_loss: 2.4478\n",
            "Epoch 4/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 193ms/step - accuracy: 0.1095 - loss: 2.4596 - val_accuracy: 0.1189 - val_loss: 2.4406\n",
            "Epoch 5/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 188ms/step - accuracy: 0.1298 - loss: 2.4451 - val_accuracy: 0.0883 - val_loss: 2.4862\n",
            "Epoch 6/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1369 - loss: 2.4435 - val_accuracy: 0.2183 - val_loss: 2.4156\n",
            "Epoch 7/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1406 - loss: 2.4293 - val_accuracy: 0.1028 - val_loss: 2.4692\n",
            "Epoch 8/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1342 - loss: 2.4250 - val_accuracy: 0.1350 - val_loss: 2.4185\n",
            "Epoch 9/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.1499 - loss: 2.4263 - val_accuracy: 0.0833 - val_loss: 2.5042\n",
            "Epoch 10/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1466 - loss: 2.4454 - val_accuracy: 0.1289 - val_loss: 2.4329\n",
            "Epoch 11/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1386 - loss: 2.4315 - val_accuracy: 0.1094 - val_loss: 2.4610\n",
            "Epoch 12/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1268 - loss: 2.4372 - val_accuracy: 0.0900 - val_loss: 2.4605\n",
            "Epoch 13/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.1334 - loss: 2.4426 - val_accuracy: 0.1161 - val_loss: 2.4417\n",
            "Epoch 14/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1274 - loss: 2.4373 - val_accuracy: 0.2367 - val_loss: 2.3973\n",
            "Epoch 15/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1449 - loss: 2.4374 - val_accuracy: 0.1133 - val_loss: 2.4353\n",
            "Epoch 16/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1445 - loss: 2.4295 - val_accuracy: 0.0833 - val_loss: 2.5337\n",
            "Epoch 17/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 197ms/step - accuracy: 0.1253 - loss: 2.4573 - val_accuracy: 0.0861 - val_loss: 2.4854\n",
            "Epoch 18/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 188ms/step - accuracy: 0.1226 - loss: 2.4525 - val_accuracy: 0.1789 - val_loss: 2.3804\n",
            "Epoch 19/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1297 - loss: 2.4401 - val_accuracy: 0.2122 - val_loss: 2.4077\n",
            "Epoch 20/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1424 - loss: 2.4327 - val_accuracy: 0.1217 - val_loss: 2.4488\n",
            "Epoch 21/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1371 - loss: 2.4312 - val_accuracy: 0.1506 - val_loss: 2.3999\n",
            "Epoch 22/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1534 - loss: 2.4189 - val_accuracy: 0.1361 - val_loss: 2.4459\n",
            "Epoch 23/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1150 - loss: 2.4603 - val_accuracy: 0.0844 - val_loss: 2.4841\n",
            "Epoch 24/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1139 - loss: 2.4679 - val_accuracy: 0.0867 - val_loss: 2.4943\n",
            "Epoch 25/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.0888 - loss: 2.4833 - val_accuracy: 0.0833 - val_loss: 2.4789\n",
            "Epoch 26/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1389 - loss: 2.4421 - val_accuracy: 0.1311 - val_loss: 2.4370\n",
            "Epoch 27/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.1107 - loss: 2.4514 - val_accuracy: 0.1611 - val_loss: 2.4306\n",
            "Epoch 28/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step - accuracy: 0.1291 - loss: 2.4449 - val_accuracy: 0.0856 - val_loss: 2.4863\n",
            "Epoch 29/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1463 - loss: 2.4390 - val_accuracy: 0.1806 - val_loss: 2.4010\n",
            "Epoch 30/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.1488 - loss: 2.4267 - val_accuracy: 0.0983 - val_loss: 2.4640\n",
            "Epoch 31/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1419 - loss: 2.4428 - val_accuracy: 0.0839 - val_loss: 2.4845\n",
            "Epoch 32/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1176 - loss: 2.4533 - val_accuracy: 0.1361 - val_loss: 2.4470\n",
            "Epoch 33/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1483 - loss: 2.4122 - val_accuracy: 0.1406 - val_loss: 2.5085\n",
            "Epoch 34/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step - accuracy: 0.1498 - loss: 2.4273 - val_accuracy: 0.2383 - val_loss: 2.3134\n",
            "Epoch 35/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1632 - loss: 2.4017 - val_accuracy: 0.1294 - val_loss: 2.4067\n",
            "Epoch 36/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1445 - loss: 2.4108 - val_accuracy: 0.1222 - val_loss: 2.4260\n",
            "Epoch 37/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1283 - loss: 2.4436 - val_accuracy: 0.1878 - val_loss: 2.3524\n",
            "Epoch 38/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1469 - loss: 2.4323 - val_accuracy: 0.1244 - val_loss: 2.4695\n",
            "Epoch 39/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1421 - loss: 2.4349 - val_accuracy: 0.1994 - val_loss: 2.3986\n",
            "Epoch 40/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1572 - loss: 2.4117 - val_accuracy: 0.1400 - val_loss: 2.3978\n",
            "Epoch 41/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 188ms/step - accuracy: 0.1604 - loss: 2.4054 - val_accuracy: 0.1211 - val_loss: 2.4605\n",
            "Epoch 42/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1634 - loss: 2.3965 - val_accuracy: 0.1644 - val_loss: 2.5279\n",
            "Epoch 43/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1687 - loss: 2.4013 - val_accuracy: 0.1878 - val_loss: 2.3456\n",
            "Epoch 44/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1779 - loss: 2.3751 - val_accuracy: 0.1256 - val_loss: 2.4641\n",
            "Epoch 45/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1697 - loss: 2.3769 - val_accuracy: 0.2278 - val_loss: 2.2736\n",
            "Epoch 46/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1951 - loss: 2.3367 - val_accuracy: 0.2500 - val_loss: 2.2676\n",
            "Epoch 47/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1919 - loss: 2.3463 - val_accuracy: 0.2528 - val_loss: 2.2419\n",
            "Epoch 48/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2081 - loss: 2.3229 - val_accuracy: 0.1000 - val_loss: 2.5536\n",
            "Epoch 49/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1700 - loss: 2.3747 - val_accuracy: 0.1922 - val_loss: 2.3249\n",
            "Epoch 50/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1967 - loss: 2.3286 - val_accuracy: 0.2111 - val_loss: 2.3422\n",
            "Epoch 51/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.1916 - loss: 2.3354 - val_accuracy: 0.2372 - val_loss: 2.2372\n",
            "Epoch 52/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.2235 - loss: 2.2748 - val_accuracy: 0.1328 - val_loss: 2.3598\n",
            "Epoch 53/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1785 - loss: 2.3593 - val_accuracy: 0.1583 - val_loss: 2.4538\n",
            "Epoch 54/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.2052 - loss: 2.3204 - val_accuracy: 0.1933 - val_loss: 2.3388\n",
            "Epoch 55/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1959 - loss: 2.3374 - val_accuracy: 0.2233 - val_loss: 2.2742\n",
            "Epoch 56/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.2158 - loss: 2.2579 - val_accuracy: 0.3022 - val_loss: 2.1545\n",
            "Epoch 57/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2415 - loss: 2.2566 - val_accuracy: 0.1906 - val_loss: 2.3701\n",
            "Epoch 58/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2249 - loss: 2.2780 - val_accuracy: 0.1711 - val_loss: 2.3456\n",
            "Epoch 59/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2035 - loss: 2.2981 - val_accuracy: 0.2283 - val_loss: 2.2551\n",
            "Epoch 60/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2248 - loss: 2.2425 - val_accuracy: 0.2056 - val_loss: 2.3464\n",
            "Epoch 61/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2068 - loss: 2.3006 - val_accuracy: 0.1928 - val_loss: 2.2512\n",
            "Epoch 62/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2390 - loss: 2.2185 - val_accuracy: 0.2167 - val_loss: 2.2875\n",
            "Epoch 63/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2256 - loss: 2.2102 - val_accuracy: 0.2983 - val_loss: 2.1043\n",
            "Epoch 64/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2636 - loss: 2.1716 - val_accuracy: 0.2394 - val_loss: 2.2706\n",
            "Epoch 65/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.2512 - loss: 2.2058 - val_accuracy: 0.2444 - val_loss: 2.2151\n",
            "Epoch 66/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2500 - loss: 2.2085 - val_accuracy: 0.3033 - val_loss: 2.1215\n",
            "Epoch 67/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2768 - loss: 2.1441 - val_accuracy: 0.2883 - val_loss: 2.1310\n",
            "Epoch 68/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 176ms/step - accuracy: 0.2716 - loss: 2.1521 - val_accuracy: 0.2533 - val_loss: 2.1967\n",
            "Epoch 69/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2530 - loss: 2.1746 - val_accuracy: 0.2883 - val_loss: 2.1616\n",
            "Epoch 70/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step - accuracy: 0.2809 - loss: 2.1425 - val_accuracy: 0.2100 - val_loss: 2.2977\n",
            "Epoch 71/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2381 - loss: 2.2094 - val_accuracy: 0.2594 - val_loss: 2.1960\n",
            "Epoch 72/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2660 - loss: 2.1654 - val_accuracy: 0.2450 - val_loss: 2.2101\n",
            "Epoch 73/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2656 - loss: 2.1594 - val_accuracy: 0.2817 - val_loss: 2.1417\n",
            "Epoch 74/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2919 - loss: 2.1101 - val_accuracy: 0.2356 - val_loss: 2.1912\n",
            "Epoch 75/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.2786 - loss: 2.1149 - val_accuracy: 0.2911 - val_loss: 2.1071\n",
            "Epoch 76/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 189ms/step - accuracy: 0.2838 - loss: 2.1010 - val_accuracy: 0.2889 - val_loss: 2.1287\n",
            "Epoch 77/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.2821 - loss: 2.1207 - val_accuracy: 0.1922 - val_loss: 2.4737\n",
            "Epoch 78/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2681 - loss: 2.1549 - val_accuracy: 0.2756 - val_loss: 2.2107\n",
            "Epoch 79/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2812 - loss: 2.1291 - val_accuracy: 0.2933 - val_loss: 2.1345\n",
            "Epoch 80/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.2941 - loss: 2.0923 - val_accuracy: 0.2728 - val_loss: 2.1494\n",
            "Epoch 81/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.3059 - loss: 2.0744 - val_accuracy: 0.2861 - val_loss: 2.1056\n",
            "Epoch 82/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.2739 - loss: 2.1555 - val_accuracy: 0.2911 - val_loss: 2.0954\n",
            "Epoch 83/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2808 - loss: 2.1104 - val_accuracy: 0.2722 - val_loss: 2.1370\n",
            "Epoch 84/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.2441 - loss: 2.1777 - val_accuracy: 0.2244 - val_loss: 2.2464\n",
            "Epoch 85/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.2910 - loss: 2.0874 - val_accuracy: 0.2783 - val_loss: 2.1282\n",
            "Epoch 86/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.3016 - loss: 2.0744 - val_accuracy: 0.3122 - val_loss: 2.0613\n",
            "Epoch 87/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.3073 - loss: 2.0518 - val_accuracy: 0.2817 - val_loss: 2.0845\n",
            "Epoch 88/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.3020 - loss: 2.0698 - val_accuracy: 0.2522 - val_loss: 2.0999\n",
            "Epoch 89/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.2795 - loss: 2.0906 - val_accuracy: 0.3056 - val_loss: 2.0748\n",
            "Epoch 90/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.3023 - loss: 2.0591 - val_accuracy: 0.1917 - val_loss: 2.4339\n",
            "Epoch 91/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2804 - loss: 2.1137 - val_accuracy: 0.2022 - val_loss: 2.4882\n",
            "Epoch 92/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2946 - loss: 2.1044 - val_accuracy: 0.3128 - val_loss: 2.0406\n",
            "Epoch 93/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2975 - loss: 2.0708 - val_accuracy: 0.3067 - val_loss: 2.0686\n",
            "Epoch 94/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.3112 - loss: 2.0402 - val_accuracy: 0.3183 - val_loss: 2.0417\n",
            "Epoch 95/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.2947 - loss: 2.0616 - val_accuracy: 0.2656 - val_loss: 2.1046\n",
            "Epoch 96/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.2979 - loss: 2.0599 - val_accuracy: 0.2567 - val_loss: 2.2208\n",
            "Epoch 97/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.3081 - loss: 2.0512 - val_accuracy: 0.3283 - val_loss: 2.0495\n",
            "Epoch 98/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 188ms/step - accuracy: 0.3107 - loss: 2.0319 - val_accuracy: 0.2822 - val_loss: 2.0994\n",
            "Epoch 99/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.3033 - loss: 2.0606 - val_accuracy: 0.2983 - val_loss: 2.0673\n",
            "Epoch 100/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.2976 - loss: 2.0580 - val_accuracy: 0.3128 - val_loss: 2.0648\n",
            "\n",
            "Entrenamiento número 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_24', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 189ms/step - accuracy: 0.0930 - loss: 2.5813 - val_accuracy: 0.1133 - val_loss: 2.4772\n",
            "Epoch 2/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1113 - loss: 2.4764 - val_accuracy: 0.0894 - val_loss: 2.4823\n",
            "Epoch 3/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1147 - loss: 2.4651 - val_accuracy: 0.1406 - val_loss: 2.4509\n",
            "Epoch 4/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1249 - loss: 2.4518 - val_accuracy: 0.1144 - val_loss: 2.4462\n",
            "Epoch 5/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1414 - loss: 2.4448 - val_accuracy: 0.1333 - val_loss: 2.4446\n",
            "Epoch 6/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1398 - loss: 2.4322 - val_accuracy: 0.0972 - val_loss: 2.4510\n",
            "Epoch 7/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1338 - loss: 2.4362 - val_accuracy: 0.1389 - val_loss: 2.4243\n",
            "Epoch 8/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1485 - loss: 2.4294 - val_accuracy: 0.0850 - val_loss: 2.4221\n",
            "Epoch 9/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1437 - loss: 2.4214 - val_accuracy: 0.0833 - val_loss: 2.4501\n",
            "Epoch 10/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step - accuracy: 0.1449 - loss: 2.4212 - val_accuracy: 0.1967 - val_loss: 2.4003\n",
            "Epoch 11/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 167ms/step - accuracy: 0.1351 - loss: 2.4195 - val_accuracy: 0.1272 - val_loss: 2.4600\n",
            "Epoch 12/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1367 - loss: 2.4275 - val_accuracy: 0.1122 - val_loss: 2.4457\n",
            "Epoch 13/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 187ms/step - accuracy: 0.1315 - loss: 2.4359 - val_accuracy: 0.1167 - val_loss: 2.4477\n",
            "Epoch 14/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1478 - loss: 2.4249 - val_accuracy: 0.1022 - val_loss: 2.4532\n",
            "Epoch 15/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step - accuracy: 0.1289 - loss: 2.4388 - val_accuracy: 0.0833 - val_loss: 2.4928\n",
            "Epoch 16/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step - accuracy: 0.1304 - loss: 2.4428 - val_accuracy: 0.1339 - val_loss: 2.4302\n",
            "Epoch 17/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1343 - loss: 2.4356 - val_accuracy: 0.1228 - val_loss: 2.4506\n",
            "Epoch 18/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1476 - loss: 2.4343 - val_accuracy: 0.1389 - val_loss: 2.4338\n",
            "Epoch 19/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1337 - loss: 2.4279 - val_accuracy: 0.1350 - val_loss: 2.4371\n",
            "Epoch 20/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1340 - loss: 2.4424 - val_accuracy: 0.1244 - val_loss: 2.4549\n",
            "Epoch 21/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1128 - loss: 2.4526 - val_accuracy: 0.0839 - val_loss: 2.4616\n",
            "Epoch 22/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1337 - loss: 2.4308 - val_accuracy: 0.1133 - val_loss: 2.4198\n",
            "Epoch 23/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1528 - loss: 2.4175 - val_accuracy: 0.1717 - val_loss: 2.4177\n",
            "Epoch 24/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1372 - loss: 2.4268 - val_accuracy: 0.1167 - val_loss: 2.4437\n",
            "Epoch 25/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1199 - loss: 2.4389 - val_accuracy: 0.1433 - val_loss: 2.4249\n",
            "Epoch 26/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1223 - loss: 2.4472 - val_accuracy: 0.2339 - val_loss: 2.3666\n",
            "Epoch 27/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1573 - loss: 2.4187 - val_accuracy: 0.0878 - val_loss: 2.4923\n",
            "Epoch 28/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1493 - loss: 2.4108 - val_accuracy: 0.1944 - val_loss: 2.3990\n",
            "Epoch 29/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1413 - loss: 2.4332 - val_accuracy: 0.1511 - val_loss: 2.3840\n",
            "Epoch 30/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1397 - loss: 2.4209 - val_accuracy: 0.1206 - val_loss: 2.3951\n",
            "Epoch 31/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1251 - loss: 2.4176 - val_accuracy: 0.1672 - val_loss: 2.3710\n",
            "Epoch 32/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1450 - loss: 2.4156 - val_accuracy: 0.1233 - val_loss: 2.3930\n",
            "Epoch 33/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1410 - loss: 2.4007 - val_accuracy: 0.1911 - val_loss: 2.3628\n",
            "Epoch 34/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1620 - loss: 2.3799 - val_accuracy: 0.1283 - val_loss: 2.4024\n",
            "Epoch 35/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1573 - loss: 2.3758 - val_accuracy: 0.1828 - val_loss: 2.3353\n",
            "Epoch 36/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1499 - loss: 2.4037 - val_accuracy: 0.1250 - val_loss: 2.4160\n",
            "Epoch 37/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1400 - loss: 2.3973 - val_accuracy: 0.1644 - val_loss: 2.3399\n",
            "Epoch 38/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1607 - loss: 2.3731 - val_accuracy: 0.2439 - val_loss: 2.3218\n",
            "Epoch 39/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1609 - loss: 2.3743 - val_accuracy: 0.1706 - val_loss: 2.3492\n",
            "Epoch 40/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1436 - loss: 2.4067 - val_accuracy: 0.1722 - val_loss: 2.3221\n",
            "Epoch 41/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1668 - loss: 2.3604 - val_accuracy: 0.1567 - val_loss: 2.3475\n",
            "Epoch 42/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1638 - loss: 2.3627 - val_accuracy: 0.1683 - val_loss: 2.3463\n",
            "Epoch 43/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1448 - loss: 2.3813 - val_accuracy: 0.1522 - val_loss: 2.3610\n",
            "Epoch 44/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1494 - loss: 2.3936 - val_accuracy: 0.1150 - val_loss: 2.4164\n",
            "Epoch 45/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1643 - loss: 2.3616 - val_accuracy: 0.1522 - val_loss: 2.4228\n",
            "Epoch 46/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1537 - loss: 2.3762 - val_accuracy: 0.1911 - val_loss: 2.3258\n",
            "Epoch 47/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1659 - loss: 2.3720 - val_accuracy: 0.1761 - val_loss: 2.3422\n",
            "Epoch 48/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1641 - loss: 2.3497 - val_accuracy: 0.1572 - val_loss: 2.3563\n",
            "Epoch 49/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1538 - loss: 2.3839 - val_accuracy: 0.1611 - val_loss: 2.3394\n",
            "Epoch 50/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.1670 - loss: 2.3444 - val_accuracy: 0.1533 - val_loss: 2.3971\n",
            "Epoch 51/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1587 - loss: 2.3814 - val_accuracy: 0.2139 - val_loss: 2.3428\n",
            "Epoch 52/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1578 - loss: 2.3588 - val_accuracy: 0.1111 - val_loss: 2.5267\n",
            "Epoch 53/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1629 - loss: 2.3658 - val_accuracy: 0.1233 - val_loss: 2.4147\n",
            "Epoch 54/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1521 - loss: 2.3639 - val_accuracy: 0.1911 - val_loss: 2.3441\n",
            "Epoch 55/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1897 - loss: 2.3316 - val_accuracy: 0.1656 - val_loss: 2.3361\n",
            "Epoch 56/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1679 - loss: 2.3457 - val_accuracy: 0.1611 - val_loss: 2.3329\n",
            "Epoch 57/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1576 - loss: 2.3657 - val_accuracy: 0.1389 - val_loss: 2.3491\n",
            "Epoch 58/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1654 - loss: 2.3486 - val_accuracy: 0.1856 - val_loss: 2.2977\n",
            "Epoch 59/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1723 - loss: 2.3239 - val_accuracy: 0.1511 - val_loss: 2.3559\n",
            "Epoch 60/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1601 - loss: 2.3433 - val_accuracy: 0.1406 - val_loss: 2.3509\n",
            "Epoch 61/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step - accuracy: 0.1680 - loss: 2.3340 - val_accuracy: 0.2150 - val_loss: 2.3021\n",
            "Epoch 62/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1772 - loss: 2.3162 - val_accuracy: 0.1683 - val_loss: 2.3706\n",
            "Epoch 63/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1749 - loss: 2.3292 - val_accuracy: 0.1894 - val_loss: 2.2828\n",
            "Epoch 64/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1860 - loss: 2.3020 - val_accuracy: 0.1483 - val_loss: 2.3332\n",
            "Epoch 65/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1254 - loss: 2.4307 - val_accuracy: 0.1072 - val_loss: 2.4915\n",
            "Epoch 66/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1671 - loss: 2.3447 - val_accuracy: 0.1611 - val_loss: 2.3384\n",
            "Epoch 67/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1952 - loss: 2.3119 - val_accuracy: 0.1533 - val_loss: 2.2881\n",
            "Epoch 68/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1869 - loss: 2.2977 - val_accuracy: 0.1056 - val_loss: 2.4799\n",
            "Epoch 69/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1815 - loss: 2.3039 - val_accuracy: 0.1906 - val_loss: 2.3271\n",
            "Epoch 70/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step - accuracy: 0.1926 - loss: 2.2965 - val_accuracy: 0.1956 - val_loss: 2.3354\n",
            "Epoch 71/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1821 - loss: 2.3097 - val_accuracy: 0.1583 - val_loss: 2.3997\n",
            "Epoch 72/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 185ms/step - accuracy: 0.1802 - loss: 2.3386 - val_accuracy: 0.2589 - val_loss: 2.2467\n",
            "Epoch 73/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1880 - loss: 2.3134 - val_accuracy: 0.1806 - val_loss: 2.3423\n",
            "Epoch 74/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.2009 - loss: 2.2927 - val_accuracy: 0.1489 - val_loss: 2.4384\n",
            "Epoch 75/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1984 - loss: 2.3034 - val_accuracy: 0.2039 - val_loss: 2.2928\n",
            "Epoch 76/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.2017 - loss: 2.2962 - val_accuracy: 0.2328 - val_loss: 2.2598\n",
            "Epoch 77/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.1856 - loss: 2.3023 - val_accuracy: 0.1661 - val_loss: 2.3163\n",
            "Epoch 78/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.1862 - loss: 2.3071 - val_accuracy: 0.1622 - val_loss: 2.2913\n",
            "Epoch 79/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 184ms/step - accuracy: 0.1843 - loss: 2.2964 - val_accuracy: 0.2022 - val_loss: 2.2856\n",
            "Epoch 80/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.1986 - loss: 2.2815 - val_accuracy: 0.1567 - val_loss: 2.3553\n",
            "Epoch 81/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.2034 - loss: 2.2641 - val_accuracy: 0.1483 - val_loss: 2.4091\n",
            "Epoch 82/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1894 - loss: 2.2922 - val_accuracy: 0.1511 - val_loss: 2.4288\n",
            "Epoch 83/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1801 - loss: 2.3118 - val_accuracy: 0.2144 - val_loss: 2.2480\n",
            "Epoch 84/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.2106 - loss: 2.2598 - val_accuracy: 0.1772 - val_loss: 2.3131\n",
            "Epoch 85/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - accuracy: 0.2024 - loss: 2.2747 - val_accuracy: 0.2322 - val_loss: 2.2210\n",
            "Epoch 86/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.2119 - loss: 2.2523 - val_accuracy: 0.1267 - val_loss: 2.4006\n",
            "Epoch 87/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 182ms/step - accuracy: 0.1955 - loss: 2.2780 - val_accuracy: 0.2189 - val_loss: 2.2129\n",
            "Epoch 88/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2171 - loss: 2.2478 - val_accuracy: 0.1378 - val_loss: 2.3370\n",
            "Epoch 89/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1932 - loss: 2.2781 - val_accuracy: 0.2061 - val_loss: 2.2572\n",
            "Epoch 90/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2061 - loss: 2.2801 - val_accuracy: 0.1672 - val_loss: 2.4234\n",
            "Epoch 91/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2119 - loss: 2.2443 - val_accuracy: 0.2411 - val_loss: 2.2035\n",
            "Epoch 92/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2231 - loss: 2.2277 - val_accuracy: 0.2033 - val_loss: 2.2266\n",
            "Epoch 93/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2236 - loss: 2.2279 - val_accuracy: 0.1644 - val_loss: 2.3338\n",
            "Epoch 94/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1940 - loss: 2.3122 - val_accuracy: 0.1989 - val_loss: 2.2585\n",
            "Epoch 95/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2202 - loss: 2.2387 - val_accuracy: 0.2039 - val_loss: 2.2252\n",
            "Epoch 96/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.2102 - loss: 2.2493 - val_accuracy: 0.1711 - val_loss: 2.3274\n",
            "Epoch 97/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2219 - loss: 2.2325 - val_accuracy: 0.1772 - val_loss: 2.3644\n",
            "Epoch 98/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2161 - loss: 2.2523 - val_accuracy: 0.2144 - val_loss: 2.2141\n",
            "Epoch 99/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.1954 - loss: 2.2667 - val_accuracy: 0.2217 - val_loss: 2.2087\n",
            "Epoch 100/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step - accuracy: 0.2285 - loss: 2.2330 - val_accuracy: 0.2472 - val_loss: 2.2081\n"
          ]
        }
      ],
      "source": [
        "# Modelo 2 - QuickDraw-Animals\n",
        "# Detalles:\n",
        "# Activacion: tanh\n",
        "# Perdida: categorical_crossentropy\n",
        "# Capas: 2\n",
        "# Epocas: 10\n",
        "# Batch size: 1000\n",
        "\n",
        "acc_total_5, acc_clase_5, cm_5 = experiment(\n",
        "        train_images_animals,\n",
        "        train_labels_animals,\n",
        "        val_images_animals,\n",
        "        val_labels_animals,\n",
        "        test_images_animals,\n",
        "        test_labels_animals,\n",
        "        layers_size=[512, 256, 128],\n",
        "        n_classes=12,\n",
        "        activation='tanh',\n",
        "        loss_fn='categorical_crossentropy',\n",
        "        class_names=clases,\n",
        "        epochs=100,\n",
        "        batch_size=150,\n",
        "        n_experiments=5\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'sheep': 0.71,\n",
              "  'bear': 0.0,\n",
              "  'bee': 0.35,\n",
              "  'cat': 0.0,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.315,\n",
              "  'crab': 0.05,\n",
              "  'crocodile': 0.765,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.04,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.0},\n",
              " {'sheep': 0.0,\n",
              "  'bear': 0.005,\n",
              "  'bee': 0.95,\n",
              "  'cat': 0.0,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.055,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.015,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.8844221105527639},\n",
              " {'sheep': 0.0,\n",
              "  'bear': 0.0,\n",
              "  'bee': 0.0,\n",
              "  'cat': 0.0,\n",
              "  'camel': 0.005,\n",
              "  'cow': 0.0,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.995,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.5577889447236181},\n",
              " {'sheep': 0.585,\n",
              "  'bear': 0.235,\n",
              "  'bee': 0.535,\n",
              "  'cat': 0.055,\n",
              "  'camel': 0.0,\n",
              "  'cow': 0.435,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.78,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.0,\n",
              "  'giraffe': 0.8140703517587939},\n",
              " {'sheep': 0.295,\n",
              "  'bear': 0.025,\n",
              "  'bee': 0.805,\n",
              "  'cat': 0.0,\n",
              "  'camel': 0.02,\n",
              "  'cow': 0.04,\n",
              "  'crab': 0.0,\n",
              "  'crocodile': 0.81,\n",
              "  'duck': 0.0,\n",
              "  'elephant': 0.0,\n",
              "  'dog': 0.11,\n",
              "  'giraffe': 0.8391959798994975}]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_clase_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.18591079616506878,\n",
              " 0.15881617340558565,\n",
              " 0.12963734889537307,\n",
              " 0.2863693205502293,\n",
              " 0.24510212588578575]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_total_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[142,   0,   2,   0,   0,  42,   2,   8,   0,   4,   0,   0],\n",
              "        [108,   0,  61,   0,   0,   9,   0,   8,   0,  14,   0,   0],\n",
              "        [ 59,   0,  70,   0,   0,  29,   0,  27,   0,  15,   0,   0],\n",
              "        [121,   0,  26,   0,   0,  17,   3,  20,   0,  13,   0,   0],\n",
              "        [155,   0,   5,   0,   0,  19,   2,   7,   0,  12,   0,   0],\n",
              "        [ 90,   0,  10,   0,   0,  63,   6,  27,   0,   4,   0,   0],\n",
              "        [ 62,   0,  11,   0,   0,  39,  10,  68,   0,  10,   0,   0],\n",
              "        [ 22,   0,   2,   0,   0,  19,   2, 153,   0,   2,   0,   0],\n",
              "        [122,   0,  25,   0,   0,  14,   2,   9,   0,  28,   0,   0],\n",
              "        [109,   0,  20,   0,   0,  37,   2,  24,   0,   8,   0,   0],\n",
              "        [ 94,   0,  13,   0,   0,  45,   3,  33,   0,  12,   0,   0],\n",
              "        [ 35,   0, 151,   0,   0,   0,   0,   1,   0,  12,   0,   0]]),\n",
              " array([[  0,   0, 138,   0,   0,  24,   0,   0,   0,   6,   0,  32],\n",
              "        [  0,   1,  93,   0,   0,   5,   0,   0,   0,   1,   0, 100],\n",
              "        [  0,   0, 190,   0,   0,   0,   0,   0,   0,   1,   0,   9],\n",
              "        [  0,   0, 143,   0,   0,   1,   0,   0,   0,   0,   0,  56],\n",
              "        [  0,   0, 144,   0,   0,   0,   0,   0,   0,   3,   0,  53],\n",
              "        [  0,   0, 174,   0,   0,  11,   0,   0,   0,   1,   0,  14],\n",
              "        [  0,   0, 181,   0,   0,   5,   0,   2,   0,   1,   0,  11],\n",
              "        [  0,   0, 192,   0,   0,   2,   0,   3,   0,   2,   0,   1],\n",
              "        [  0,   0, 140,   0,   0,   0,   0,   1,   0,   1,   0,  58],\n",
              "        [  0,   0, 179,   0,   0,   1,   0,   0,   0,   0,   0,  20],\n",
              "        [  0,   0, 174,   0,   0,   1,   0,   1,   0,   0,   0,  24],\n",
              "        [  0,   0,  23,   0,   0,   0,   0,   0,   0,   0,   0, 176]]),\n",
              " array([[  0,   0,   0,   0,   0,   0,   0, 199,   0,   0,   0,   1],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 159,   0,   2,   0,  39],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 197,   0,   0,   0,   3],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 190,   0,   0,   0,  10],\n",
              "        [  0,   0,   0,   0,   1,   0,   0, 192,   0,   0,   0,   7],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 197,   0,   0,   0,   3],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 197,   0,   0,   0,   3],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 199,   0,   0,   0,   1],\n",
              "        [  0,   0,   0,   0,   2,   0,   0, 186,   0,   1,   0,  11],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 196,   0,   0,   0,   4],\n",
              "        [  0,   0,   0,   0,   0,   0,   0, 194,   0,   0,   0,   6],\n",
              "        [  0,   0,   1,   0,   3,   0,   0,  81,   0,   3,   0, 111]]),\n",
              " array([[117,  24,   2,   4,   0,  44,   3,   5,   0,   0,   0,   1],\n",
              "        [ 61,  47,   9,   9,   0,  15,   1,   9,   0,   0,   0,  49],\n",
              "        [  5,   4, 107,   2,   0,  32,   2,  31,   0,   0,   0,  17],\n",
              "        [ 49,  29,  25,  11,   0,  29,   2,  34,   0,   0,   0,  21],\n",
              "        [ 82,  39,   4,  20,   0,  18,   2,  16,   0,   0,   0,  19],\n",
              "        [ 41,   9,  20,   4,   0,  87,   4,  31,   0,   0,   0,   4],\n",
              "        [ 30,   8,   7,   7,   0,  77,   0,  61,   0,   0,   0,  10],\n",
              "        [  6,   6,   4,   9,   0,  13,   3, 156,   0,   0,   0,   3],\n",
              "        [ 44,  59,  13,  16,   0,  13,   3,  24,   0,   0,   0,  28],\n",
              "        [ 37,  25,  18,  20,   0,  35,   0,  52,   0,   0,   0,  13],\n",
              "        [ 44,  17,   8,  10,   0,  60,   4,  43,   0,   0,   0,  14],\n",
              "        [  2,  22,   6,   5,   0,   1,   0,   1,   0,   0,   0, 162]]),\n",
              " array([[ 59,   2,  14,   0,   3,   3,   2,  65,   0,   0,  38,  14],\n",
              "        [ 18,   5,  21,   0,   3,   1,   1,  36,   0,   0,  21,  94],\n",
              "        [  0,   0, 161,   0,   0,   3,   1,   8,   0,   0,   9,  18],\n",
              "        [ 13,   5,  56,   0,   1,   5,   2,  32,   1,   0,  24,  61],\n",
              "        [ 22,   6,  15,   0,   4,   5,   2,  68,   1,   0,  37,  40],\n",
              "        [ 12,   0,  99,   0,   3,   8,   1,  39,   0,   0,  28,  10],\n",
              "        [  4,   0,  34,   0,   1,   7,   0, 112,   0,   0,  30,  12],\n",
              "        [  2,   0,  18,   0,   0,   8,   0, 162,   0,   0,   9,   1],\n",
              "        [ 18,   5,  28,   0,   4,   6,   1,  59,   0,   0,  29,  50],\n",
              "        [ 27,   1,  48,   0,   3,   7,   0,  73,   0,   0,  25,  16],\n",
              "        [ 15,   4,  44,   0,   3,  11,   2,  81,   0,   0,  22,  18],\n",
              "        [  1,   4,  15,   0,   5,   1,   0,   3,   0,   0,   3, 167]])]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lE_OGxeDNtno"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenamiento número 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'mlp_25', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Could not interpret loss identifier: kullback_leibler_divergence",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Modelo 3 - QuickDraw-Animals\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Detalles:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Activacion: sigmoid\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Epocas: 10\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Batch size: 1000\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m acc_total_6, acc_clase_6, cm_6 \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_images_animals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_labels_animals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_images_animals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_labels_animals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_images_animals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_labels_animals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayers_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkullback_leibler_divergence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_experiments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[16], line 63\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(X_train, y_train, X_val, y_val, X_test, y_test, layers_size, n_classes, activation, loss_fn, class_names, epochs, batch_size, n_experiments)\u001b[0m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Evaluar\u001b[39;00m\n\u001b[0;32m     66\u001b[0m acc_total, acc_per_class, cm \u001b[38;5;241m=\u001b[39m evaluate_model(model, X_test, y_test, class_names)\n",
            "Cell \u001b[1;32mIn[16], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, y_train, X_val, y_val, loss_fn, epochs, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Detiene el proceso tras 5 epocas sin encontrar una mejora. Evalúa la pérdida en el\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# conjunto de validación.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m     13\u001b[0m   monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m   patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     15\u001b[0m   restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m final_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=0\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_model\n",
            "File \u001b[1;32mc:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\vicen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses\\__init__.py:207\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret loss identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: Could not interpret loss identifier: kullback_leibler_divergence"
          ]
        }
      ],
      "source": [
        "# Modelo 3 - QuickDraw-Animals\n",
        "# Detalles:\n",
        "# Activacion: sigmoid\n",
        "# Perdida: kullback_leibler_divergence\n",
        "# Capas: 2\n",
        "# Epocas: 10\n",
        "# Batch size: 1000\n",
        "\n",
        "acc_total_6, acc_clase_6, cm_6 = experiment(\n",
        "        train_images_animals,\n",
        "        train_labels_animals,\n",
        "        val_images_animals,\n",
        "        val_labels_animals,\n",
        "        test_images_animals,\n",
        "        test_labels_animals,\n",
        "        layers_size=[512, 256, 128, 64],\n",
        "        n_classes=12,\n",
        "        activation='sigmoid',\n",
        "        loss_fn='kullback_leibler_divergence',\n",
        "        class_names=clases,\n",
        "        epochs=10,\n",
        "        batch_size=20,\n",
        "        n_experiments=5\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
